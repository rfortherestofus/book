[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for the Rest of Us: A Statistics-Free Introduction",
    "section": "",
    "text": "About the Book\nThe R programming language is a remarkably powerful tool for data analysis and visualization, but its steep learning curve can be intimidating for some. If you just want to automate repetitive tasks or visualize your data, without the need for complex math, R for the Rest of Us is for you.\nInside you’ll find a crash course in R, a quick tour of the RStudio programming environment, and a collection of real-word applications that you can put to use right away. You’ll learn how to create informative visualizations, streamline report generation, and develop interactive websites — whether you’re a seasoned R user or have never written a line of R code.\nYou’ll also learn how to:\nUnlock a treasure trove of techniques to transform the way you work. With R for the Rest of Us, you’ll discover the power of R to get stuff done. No advanced statistics degree required.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "index.html#about-the-book",
    "href": "index.html#about-the-book",
    "title": "R for the Rest of Us: A Statistics-Free Introduction",
    "section": "",
    "text": "Manipulate, clean, and parse your data with tidyverse packages like dplyr and tidyr to make data science operations more user-friendly\nCreate stunning and customized plots, graphs, and charts with ggplot2 to effectively communicate your data insights\nImport geospatial data and write code to produce visually appealing maps automatically\nGenerate dynamic reports, presentations, and interactive websites with R Markdown and Quarto that seamlessly integrate code, text, and graphics\nDevelop custom functions and packages tailored to your specific needs, allowing you to extend R’s functionality and automate complex tasks",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "R for the Rest of Us: A Statistics-Free Introduction",
    "section": "About the Author",
    "text": "About the Author\n\nDavid Keyes is the founder and CEO of R for the Rest of Us, which offers online courses, workshops, and custom training sessions that help organizations take control of their data. He has a PhD in anthropology from UC San Diego, as well as a master’s degree in education from Ohio State, and has dedicated his professional life to teaching people to embrace R as the most powerful tool for data analysis and visualization.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "R for the Rest of Us: A Statistics-Free Introduction",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis book is a testament to the many members of the R community who share their knowledge freely and encourage others generously. I call myself self-taught, but really what I am is community-taught. Throughout this book, you will read about several R users from whom I have learned so much; still, many others go unmentioned. To everyone who has worked to develop R, share your knowledge about R, and make R a welcoming place, thank you.\nI’d also like to thank the team at R for the Rest of Us. Working directly with talented R users has taught me so much about what is possible with R.\nFinally, I’d like to thank people who have provided feedback as I’ve written this book. Technical reviewer Rita Giordano has helped me make sure everything works and suggested great ideas for improvement. My editor, Frances Saux, has provided fantastic input along the way. To Bill Pollock and the entire team at No Starch: thank you for taking a flyer on me and my strange idea to write a book about nonstatistical uses of a tool created for statistics.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "index.html#buy-the-book",
    "href": "index.html#buy-the-book",
    "title": "R for the Rest of Us: A Statistics-Free Introduction",
    "section": "Buy the Book",
    "text": "Buy the Book\nThere will always be a free online version of this book. However, if you’d like to purchase a physical or electronic copy, you can do so directly from No Starch Press, Powell’s, or Amazon.",
    "crumbs": [
      "About the Book"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Isn’t R Just for Statistical Analysis?\nMany people think of R as simply a tool for hardcore statistical analysis, but it can do much more than manipulate numerical values. After all, every R user must illuminate their findings and communicate their results somehow, whether that’s via data visualizations, reports, websites, or presentations. Also, the more you use R, the more you’ll find yourself wanting to automate tasks you currently do manually.\nAs a qualitatively trained anthropologist without a quantitative background, I used to feel ashamed about using R for my visualization and communication tasks. But the fact is, R is good at these jobs. The ggplot2 package is the tool of choice for many top information designers. Users around the world have taken advantage of R’s ability to automate reporting to make their work more efficient. Rather than simply replacing other tools, R can perform tasks that you’re probably already doing, like generating reports and tables, better than your existing workflow.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#who-this-book-is-for",
    "href": "introduction.html#who-this-book-is-for",
    "title": "Introduction",
    "section": "Who This Book Is For",
    "text": "Who This Book Is For\nNo matter your background, using R can transform your work. This book is for you if you’re either a current R user keen to explore its uses for visualization and communication or a non-R user wondering if R is right for you. I’ve written R for the Rest of Us so that it should make sense whether or not you’ve ever written a line of R code. But even if you’ve written entire R programs, the book should help you learn plenty of new techniques to up your game.\nR is a great tool for anyone who works with data. Maybe you’re a researcher looking for a new way to share your results. Perhaps you’re a journalist looking to analyze public data more efficiently. Or maybe you’re a data analyst tired of working in expensive, proprietary tools. If you have to work with data, you will get value from R.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#about-this-book",
    "href": "introduction.html#about-this-book",
    "title": "Introduction",
    "section": "About This Book",
    "text": "About This Book\nEach chapter focuses on one use of the R language and includes examples of real R projects that employ the techniques covered. I’ll dive into the project code, breaking the programs down to help you understand how they work, and suggest ways of going beyond the example. The book has three parts, outlined here.\nIn Part I, you’ll learn how to use R to visualize data.\n\nChapter 1: An R Programming Crash Course Introduces the RStudio programming environment and the foundational R syntax you’ll need to understand the rest of the book.\nChapter 2: Principles of Data Visualization Breaks down a visualization created for Scientific American on drought conditions in the United States. In doing so, this chapter introduces the ggplot2 package for data visualization and addresses important principles that can help you make high-quality graphics.\nChapter 3: Custom Data Visualization Themes Describes how journalists at the BBC made a custom theme for the ggplot2 data visualization package. As the chapter walks you through the package they created, you’ll learn how to make your own theme.\nChapter 4: Maps and Geospatial Data Explores the process of making maps in R using simple features data. You’ll learn how to write map-making code, find geospatial data, choose appropriate projections, and apply data visualization principles to make your map appealing.\nChapter 5: Designing Effective Tables Shows you how to use the gt package to make high-quality tables in R. With guidance from R table connoisseur Tom Mock, you’ll learn the design principles to present your table data effectively.\n\nPart II focuses on using R Markdown to communicate efficiently. You’ll learn how to incorporate visualizations like the ones discussed in Part I into reports, slideshow presentations, and static websites generated entirely using R code.\n\nChapter 6: R Markdown Reports Introduces R Markdown, a tool that allows you to generate a professional report in R. This chapter covers the structure of an R Markdown document, shows you how to use inline code to automatically update your report’s text when data values change, and discusses the tool’s many export options.\nChapter 7: Parameterized Reporting Covers one of the advantages of using R Markdown: the ability to produce multiple reports at the same time using a technique called parameterized reporting. You’ll see how staff members at the Urban Institute used R to generate fiscal briefs for all 50 US states. In the process, you’ll learn how parameterized reporting works and how you can use it.\nChapter 8: Slideshow Presentations Explains how to use R Markdown to make slides with the xaringan package. You’ll learn how to make your own presentations, adjust your content to fit on a slide, and add effects to your slideshow.\nChapter 9: Websites Shows you how to create your own website with R Markdown and the distill package. By examining a website about COVID-19 rates in Westchester County, New York, you’ll see how to create pages on your site, add interactivity through R packages, and deploy your website in multiple ways.\nChapter 10: Quarto Explains how to use Quarto, the next-generation version of R Markdown. You’ll learn how to use Quarto for all of the projects you previously used R Markdown for (reports, parameterized reporting, slideshow presentations, and websites).\n\nPart III focuses on ways you can use R to automate your work and share it with others.\n\nChapter 11: Automatically Accessing Online Data Explores two R packages that let you automatically import data from the internet: googlesheets4 for working with Google Sheets and tidycensus for working with US Census Bureau data. You’ll learn how the packages work and how to use them to automate the process of accessing data.\nChapter 12: Creating Functions and Packages Shows you how to create your own functions and packages and share them with others, which is one of R’s major benefits. Bundling your custom functions into a package can enable other R users to streamline their work, as you’ll read about with the packages that a group of R developers built for researchers working at the Moffitt Cancer Center.\n\nBy the end of this book, you should be able to use R for a wide range of nonstatistical tasks. You’ll know how to effectively visualize data and communicate your findings using maps and tables. You’ll be able to integrate your results into reports using R Markdown, as well as efficiently generate slideshow presentations and websites. And you’ll understand how to automate many tedious tasks using packages others have built or ones you develop yourself. Let’s dive in!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "crash-course.html",
    "href": "crash-course.html",
    "title": "1  An R Programming Crash Course",
    "section": "",
    "text": "Setting Up\nYou’ll need two pieces of software to use R effectively. The first is R itself, which provides the underlying computational tools that make the language work. The second is an integrated development environment (IDE) like RStudio. This coding platform simplifies working with R. The best way to understand the relationship between R and RStudio is with this analogy from Chester Ismay and Albert Kim’s book Statistical Inference via Data Science: A Modern Dive into R and the Tidyverse: R is the engine that powers your data, and RStudio is like the dashboard that provides a user-friendly interface.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#setting-up",
    "href": "crash-course.html#setting-up",
    "title": "1  An R Programming Crash Course",
    "section": "",
    "text": "Installing R and RStudio\nTo download R, go to https://cloud.r-project.org and choose the link for your operating system. Once you’ve installed it, open the file. This should open an interface, like the one shown in Figure 1.1, that lets you work with R on your operating system’s command line. For example, enter 2 + 2, and you should see 4.\n\n\n\n\n\nFigure 1.1: The R console\n\n\nA few brave souls work with R using only this command line, but most opt to use RStudio, which provides a way to see your files, the output of your code, and more. You can download RStudio at https://posit.co/download/rstudio-desktop/. Install RStudio as you would any other app and open it.\nExploring the RStudio Interface\nThe first time you open RStudio, you should see the three panes shown in Figure 1.2.\n\n\n\n\n\nFigure 1.2: The RStudio editor\n\n\nThe left pane should look familiar. It’s similar to the screen you saw when working in R on the command line. This is known as the console. You’ll use it to enter code and see the results. This pane has several tabs, such as Terminal and Background Jobs, for more advanced uses. For now, you’ll stick to the default tab.\nAt the bottom right, the files pane shows all of the files on your computer. You can click any file to open it within RStudio. Finally, at the top right is the environment pane, which shows the objects that are available to you when working in RStudio. Objects are discussed in “Saving Data as Objects” on page 11.\nThere is one more pane that you’ll typically use when working in RStudio, but to see it, first you need to create an R script file.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#r-script-files",
    "href": "crash-course.html#r-script-files",
    "title": "1  An R Programming Crash Course",
    "section": "R Script Files",
    "text": "R Script Files\nIf you write all of your code in the console, you won’t have any record of it. Say you sit down today and import your data, analyze it, and then make some graphs. If you run these operations in the console, you’ll have to re-create that code from scratch tomorrow. But if you write your code in files instead, you can run it multiple times.\nR script files, which use the .R extension, save your code so you can run it later. To create an R script file, go to File &gt; New File &gt; R Script, and the script file pane should appear in the top left of RStudio, as shown in Figure 1.3. Save this file in your Documents folder as sample-code.R.\n\n\n\n\n\nFigure 1.3: The script file pane (top left)\n\n\nNow you can enter R code into the new pane to add it to your script file. For example, try entering 2 + 2 in the script file pane to perform a simple addition operation.\nTo run a script file, click Run or use the keyboard shortcut command-enter on macOS or ctrl-enter on Windows. The result (4, in this case) should show up in the console pane.\nYou now have a working programming environment. Next you’ll use it to write some simple R code.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#basic-r-syntax",
    "href": "crash-course.html#basic-r-syntax",
    "title": "1  An R Programming Crash Course",
    "section": "Basic R Syntax",
    "text": "Basic R Syntax\nIf you’re trying to learn R, you probably want to perform more complex operations than 2 + 2, but understanding the fundamentals will prepare you to do more serious data analysis tasks later in this chapter. Let’s cover some of these basics.\nArithmetic Operators\nBesides +, R supports the common arithmetic operators - for subtraction, * for multiplication, and / for division. Try entering the following in the console:\n\n2 - 1\n\n[1] 1\n\n\n\n3 * 3\n\n[1] 9\n\n\n\n16 / 4\n\n[1] 4\n\n\nAs you can see, R returns the result of each calculation you enter. You don’t have to add the spaces around operators as shown here, but doing so makes your code much more readable.\nYou can also use parentheses to perform multiple operations at once and see their result. The parentheses specify the order in which R will evaluate the expression. Try running the following:\n\n2 * (2 + 1)\n\n[1] 6\n\n\nThis code first evaluates the expression within the parentheses, 2 + 1, before multiplying the result by 2 in order to get 6.\nR also has more advanced arithmetic operators, such as ** to calculate exponents:\n\n\n[1] 8\n\n\nThis is equivalent to 23, which returns 8.\nTo get the remainder of a division operation, you can use the %% operator:\n\n10 %% 3\n\n[1] 1\n\n\nDividing 10 by 3 produces a remainder of 1, the value R returns.\nYou won’t need to use these advanced arithmetic operators for the activities in this book, but they’re good to know nonetheless.\nComparison Operators\nR also uses comparison operators, which let you test how one value compares to another. R will return either TRUE or FALSE. For example, enter 2 &gt; 1 in the console:\n\n2 &gt; 1\n\n[1] TRUE\n\n\nR should return TRUE, because 2 is greater than 1.\nOther common comparison operators include less than (&lt;), greater than or equal to (&gt;=), less than or equal to (&lt;=), equal to (==), and not equal to (!=). Here are some examples:\n\n498 == 498\n\n[1] TRUE\n\n\n\n2 != 2\n\n[1] FALSE\n\n\nWhen you enter 498 == 498 in the console, R should return TRUE because the two values are equal. If you run 2 != 2 in the console, R should return FALSE because 2 does not not equal 2.\nYou’ll rarely use comparison operators to directly test how one value compares to another; instead, you’ll use them to perform tasks like keeping only data where a value is greater than a certain threshold. You’ll see comparison operators used in this way in “tidyverse Functions” (Section 1.6.1).\nFunctions\nYou can perform even more useful operations by making use of R’s many functions, predefined sections of code that let you efficiently do specific things. Functions have a name and a set of parentheses containing arguments, which are values that affect the function’s behavior.\nConsider the print() function, which displays information:\n\nprint(x = 1.1)\n\n[1] 1.1\n\n\nThe name of the print() function is print. Within the function’s parentheses, you specify the argument name – x, in this case — followed by the equal sign (=) and a value for the function to display. This code will print the number 1.1.\nTo separate multiple arguments, you use commas. For example, you can use the print() function’s digits argument to indicate how many digits of a number to display:\n\nprint(x = 1.1, digits = 1)\n\n[1] 1\n\n\nThis code will display only one digit (in other words, a whole number).\nUsing these two arguments allows you to do something specific (display results) while also giving you the flexibility to change the function’s behavior.\n\n\n\n\n\n\nNote\n\n\n\nFor a list of all functions built into R, see https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html.\n\n\nA common R pattern is using a function within a function. For example, if you wanted to calculate the mean, or average, of the values 10, 20, and 30, you could use the mean() function to operate on the result of the c() function like so:\n\nmean(x = c(10, 20, 30))\n\n[1] 20\n\n\nThe c() function combines multiple values into one, which is necessary because the mean() function accepts only one argument. This is why the code has two matching sets of open and close parentheses: one for mean() and a nested one for c().\nThe value after the equal sign in this example, c(10, 20, 30), tells R to use the values 10, 20, and 30 to calculate the mean. Running this code in the console returns the value 20.\nThe functions median() and mode() work with c() in the same way. To learn how to use a function and what arguments it accepts, enter ? followed by the function’s name in the console to see the function’s help file.\nNext, let’s look at how to import data for your R programs to work with.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#working-with-data",
    "href": "crash-course.html#working-with-data",
    "title": "1  An R Programming Crash Course",
    "section": "Working with Data",
    "text": "Working with Data\nR lets you do all of the same data manipulation tasks you might perform in a tool like Excel, such as calculating averages or totals. Conceptually, however, working with data in R is very different from working with Excel, where your data and analysis code live in the same place: a spreadsheet. While the data you work with in R might look similar to the data you work with in Excel, it typically comes from some external file, so you have to run code to import it.\nImporting Data\nYou’ll import data from a comma-separated values (CSV) file, a text file that holds a series of related values separated by commas. You can open CSV files using most spreadsheet applications, which use columns rather than commas as separators. For example, Figure 1.4 shows the population-by-state.csv file in Excel.\n\n\n\n\n\nFigure 1.4: The population-by-state.csv file in Excel\n\n\nTo work with this file in R, download it from https://data.rfortherestofus.com/population-by-state.csv. Save it to a location on your computer, such as your Documents folder.\nNext, to import the file into R, add a line like the following to the sample-code.R file you created earlier in this chapter, replacing my filepath with the path to the file’s location on your system:\n\nread.csv(file = \"/Users/davidkeyes/Documents/population-by-state.csv\")\n\nThe file argument in the read.csv() function specifies the path to the file to open.\nThe read.csv() function can accept additional optional arguments, separated by commas. For example, the following line uses the skip argument in addition to file to import the same file but skip the first row:\n\nread.csv(\n  file = \"/Users/davidkeyes/Documents/population-by-state.csv\",\n  skip = 1\n)\n\nTo learn about additional arguments for this function, enter ?read.csv() in the console to see its help file.\nAt this point, you can run the code to import your data (without the skip argument). Highlight the line you want to run in the script file pane in RStudio and click Run. You should see the following output in the console pane:\n\n\n   rank                State      Pop  Growth  Pop2018  Pop2010 growthSince2010\n1     1           California 39613493  0.0038 39461588 37319502          0.0615\n2     2                Texas 29730311  0.0385 28628666 25241971          0.1778\n3     3              Florida 21944577  0.0330 21244317 18845537          0.1644\n4     4             New York 19299981 -0.0118 19530351 19399878         -0.0051\n5     5         Pennsylvania 12804123  0.0003 12800922 12711160          0.0073\n6     6             Illinois 12569321 -0.0121 12723071 12840503         -0.0211\n7     7                 Ohio 11714618  0.0033 11676341 11539336          0.0152\n8     8              Georgia 10830007  0.0303 10511131  9711881          0.1151\n9     9       North Carolina 10701022  0.0308 10381615  9574323          0.1177\n10   10             Michigan  9992427  0.0008  9984072  9877510          0.0116\n11   11           New Jersey  8874520 -0.0013  8886025  8799446          0.0085\n12   12             Virginia  8603985  0.0121  8501286  8023699          0.0723\n13   13           Washington  7796941  0.0363  7523869  6742830          0.1563\n14   14              Arizona  7520103  0.0506  7158024  6407172          0.1737\n15   15            Tennessee  6944260  0.0255  6771631  6355311          0.0927\n16   16        Massachusetts  6912239  0.0043  6882635  6566307          0.0527\n17   17              Indiana  6805663  0.0165  6695497  6490432          0.0486\n18   18             Missouri  6169038  0.0077  6121623  5995974          0.0289\n19   19             Maryland  6065436  0.0049  6035802  5788645          0.0478\n20   20             Colorado  5893634  0.0356  5691287  5047349          0.1677\n21   21            Wisconsin  5852490  0.0078  5807406  5690475          0.0285\n22   22            Minnesota  5706398  0.0179  5606249  5310828          0.0745\n23   23       South Carolina  5277830  0.0381  5084156  4635649          0.1385\n24   24              Alabama  4934193  0.0095  4887681  4785437          0.0311\n25   25            Louisiana  4627002 -0.0070  4659690  4544532          0.0181\n26   26             Kentucky  4480713  0.0044  4461153  4348181          0.0305\n27   27               Oregon  4289439  0.0257  4181886  3837491          0.1178\n28   28             Oklahoma  3990443  0.0127  3940235  3759944          0.0613\n29   29          Connecticut  3552821 -0.0052  3571520  3579114         -0.0073\n30   30                 Utah  3310774  0.0499  3153550  2775332          0.1929\n31   31          Puerto Rico  3194374  0.0003  3193354  3721525         -0.1416\n32   32               Nevada  3185786  0.0523  3027341  2702405          0.1789\n33   33                 Iowa  3167974  0.0061  3148618  3050745          0.0384\n34   34             Arkansas  3033946  0.0080  3009733  2921964          0.0383\n35   35          Mississippi  2966407 -0.0049  2981020  2970548         -0.0014\n36   36               Kansas  2917224  0.0020  2911359  2858190          0.0207\n37   37           New Mexico  2105005  0.0059  2092741  2064552          0.0196\n38   38             Nebraska  1951996  0.0137  1925614  1829542          0.0669\n39   39                Idaho  1860123  0.0626  1750536  1570746          0.1842\n40   40        West Virginia  1767859 -0.0202  1804291  1854239         -0.0466\n41   41               Hawaii  1406430 -0.0100  1420593  1363963          0.0311\n42   42        New Hampshire  1372203  0.0138  1353465  1316762          0.0421\n43   43                Maine  1354522  0.0115  1339057  1327629          0.0203\n44   44              Montana  1085004  0.0229  1060665   990697          0.0952\n45   45         Rhode Island  1061509  0.0030  1058287  1053959          0.0072\n46   46             Delaware   990334  0.0257   965479   899593          0.1009\n47   47         South Dakota   896581  0.0204   878698   816166          0.0985\n48   48         North Dakota   770026  0.0158   758080   674715          0.1413\n49   49               Alaska   724357 -0.0147   735139   713910          0.0146\n50   50 District of Columbia   714153  0.0180   701547   605226          0.1800\n51   51              Vermont   623251 -0.0018   624358   625879         -0.0042\n52   52              Wyoming   581075  0.0060   577601   564487          0.0294\n   Percent    density\n1   0.1184   254.2929\n2   0.0889   113.8081\n3   0.0656   409.2229\n4   0.0577   409.5400\n5   0.0383   286.1704\n6   0.0376   226.3967\n7   0.0350   286.6944\n8   0.0324   188.3054\n9   0.0320   220.1041\n10  0.0299   176.7351\n11  0.0265  1206.7609\n12  0.0257   217.8776\n13  0.0233   117.3249\n14  0.0225    66.2016\n15  0.0208   168.4069\n16  0.0207   886.1845\n17  0.0203   189.9644\n18  0.0184    89.7419\n19  0.0181   624.8518\n20  0.0176    56.8653\n21  0.0175   108.0633\n22  0.0171    71.6641\n23  0.0158   175.5707\n24  0.0147    97.4271\n25  0.0138   107.0966\n26  0.0134   113.4760\n27  0.0128    44.6872\n28  0.0119    58.1740\n29  0.0106   733.7507\n30  0.0099    40.2918\n31  0.0095   923.4964\n32  0.0095    29.0195\n33  0.0095    56.7158\n34  0.0091    58.3059\n35  0.0089    63.2186\n36  0.0087    35.6808\n37  0.0063    17.3540\n38  0.0058    25.4087\n39  0.0056    22.5079\n40  0.0053    73.5443\n41  0.0042   218.9678\n42  0.0041   153.2674\n43  0.0040    43.9167\n44  0.0032     7.4547\n45  0.0032  1026.6044\n46  0.0030   508.1242\n47  0.0027    11.8265\n48  0.0023    11.1596\n49  0.0022     1.2694\n50  0.0021 11707.4262\n51  0.0019    67.6197\n52  0.0017     5.9847\n\n\nThis is R’s way of confirming that it imported the CSV file and understands the data within it. Four variables show each state’s rank (in terms of population size), name, current population, population growth between the Pop and Pop2018 variables (expressed as a percentage), and 2018 population. Several other variables are hidden in the output, but you’ll see them if you import this CSV file yourself.\nYou might think you’re ready to work with your data now, but all you’ve really done at this point is display the result of running the code that imports the data. To actually use the data, you need to save it to an object.\nSaving Data as Objects\nTo save your data for reuse, you need to create an object. For the purposes of this discussion, an object is a data structure that is stored for later use. To create an object, update your data-importing syntax so it looks like this:\nNow this line of code contains the &lt;- assignment operator, which takes what follows it and assigns it to the item on the left. To the left of the assignment operator is the population_data object. Put together, the whole line imports the CSV file and assigns it to an object called population_data.\nWhen you run this code, you should see population_data in your environment pane, as shown in Figure 1.5.\n\n\n\n\n\nFigure 1.5: The population_data object in the environment pane\n\n\nThis message confirms that your data import worked and that the population_data object is ready for future use. Now, instead of having to rerun the code to import the data, you can simply enter population_data in an R script file or in the console to output the data.\nData imported to an object in this way is known as a data frame. You can see that the population_data data frame has 52 observations and 9 variables. Variables are the data frame’s columns, each of which represents some value (for example, the population of each state). As you’ll see throughout the book, you can add new variables or modify existing ones using R code. The 52 observations come from the 50 states, as well as the District of Columbia and Puerto Rico.\n\npopulation_data &lt;- read.csv(file = \"/Users/davidkeyes/Documents/population-by-state.csv\")\n\nInstalling Packages\nThe read.csv() function you’ve been using, as well as the mean() and c() functions you saw earlier, comes from base R, the set of built-in R functions. To use base R functions, you simply enter their names. However, one of the benefits of R being an open source language is that anyone can create their own code and share it with others. R users around the world make R packages, which provide custom functions to accomplish specific goals.\nThe best analogy for understanding packages also comes from the book Statistical Inference via Data Science. The functionality in base R is like the features built into a smartphone. A smartphone can do a lot on its own, but you usually want to install additional apps for specific tasks. Packages are like apps, giving you functionality beyond what’s built into base R. In Chapter 12, you’ll create your own R package.\nYou can install packages using the install.packages() function. You’ll be working with the tidyverse package, which provides a range of functions for data import, cleaning, analysis, visualization, and more. To install it, enter install.packages(\"tidyverse\"). Typically, you’ll enter package installation code in the console rather than in a script file because you need to install a package only once on your computer to access its code in the future.\nTo confirm that the tidyverse package has been installed correctly, click the Packages tab on the bottom-right pane in RStudio. Search for tidyverse, and you should see it pop up.\nNow that you’ve installed the tidyverse, you’ll put it to use. Although you need to install packages only once per computer, you need to load them each time you restart RStudio. Return to the sample-code.R file and reimport your data using a function from the tidyverse package (your filepath will look slightly different):\n\nlibrary(tidyverse)\n\npopulation_data_2 &lt;- read_csv(file = \"/Users/davidkeyes/Documents/population-by-state.csv\")\n\nAt the top of the script, the line library(tidyverse) loads the tidyverse package. Then, the package’s read_csv() function imports the data. Note the underscore (_) in place of the period (.) in the function’s name; this differs from the base R function you used earlier. Using read_csv() to import CSV files achieves the same goal of creating an object, however — in this case, one called population_data_2. Enter population_data_2 in the console, and you should see this output:\n\npopulation_data_2 &lt;- read_csv(file = \"data/population-by-state.csv\")\n\npopulation_data_2\n\n# A tibble: 52 × 9\n    rank State       Pop  Growth Pop2018 Pop2010 growthSince2010 Percent density\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 Califor… 3.96e7  0.0038  3.95e7  3.73e7          0.0615  0.118     254.\n 2     2 Texas    2.97e7  0.0385  2.86e7  2.52e7          0.178   0.0889    114.\n 3     3 Florida  2.19e7  0.033   2.12e7  1.88e7          0.164   0.0656    409.\n 4     4 New York 1.93e7 -0.0118  1.95e7  1.94e7         -0.0051  0.0577    410.\n 5     5 Pennsyl… 1.28e7  0.0003  1.28e7  1.27e7          0.0073  0.0383    286.\n 6     6 Illinois 1.26e7 -0.0121  1.27e7  1.28e7         -0.0211  0.0376    226.\n 7     7 Ohio     1.17e7  0.0033  1.17e7  1.15e7          0.0152  0.035     287.\n 8     8 Georgia  1.08e7  0.0303  1.05e7  9.71e6          0.115   0.0324    188.\n 9     9 North C… 1.07e7  0.0308  1.04e7  9.57e6          0.118   0.032     220.\n10    10 Michigan 9.99e6  0.0008  9.98e6  9.88e6          0.0116  0.0299    177.\n# ℹ 42 more rows\n\n\nThis data looks slightly different from the data you generated using the read.csv() function. For example, R shows only the first 10 rows. This variation occurs because read_csv() imports the data not as a data frame but as a data type called a tibble. Both data frames and tibbles are used to describe rectangular data like what you would see in a spreadsheet. There are some minor differences between data frames and tibbles, the most important of which is that tibbles print only the first 10 rows by default, while data frames print all rows. For the purposes of this book, the two terms are used interchangeably.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#rstudio-projects",
    "href": "crash-course.html#rstudio-projects",
    "title": "1  An R Programming Crash Course",
    "section": "RStudio Projects",
    "text": "RStudio Projects\nSo far, you’ve imported a CSV file from your Documents folder. But because others won’t have this exact location on their computer, your code won’t work if they try to run it. One solution to this problem is an RStudio project.\nBy working in a project, you can use relative paths to your files instead of having to write the entire filepath when calling a function to import data. Then, if you place the CSV file in your project, anyone can open it by using the file’s name, as in read_csv(file = \"population-by-state.csv\"). This makes the path easier to write and enables others to use your code.\nTo create a new RStudio project, go to File New Project. Select either New Directory or Existing Directory and choose where to put your project. If you choose New Directory, you’ll need to specify that you want to create a new project. Next, choose a name for the new directory and where it should live. (Leave the checkboxes that ask about creating a Git repository and using renv unchecked; they’re for more advanced purposes.)\nOnce you’ve created this project, you should see two major differences in RStudio’s appearance. First, the files pane no longer shows every file on your computer. Instead, it shows only files in the example-project directory. Right now, that’s just the example-project.Rproj file, which indicates that the folder contains a project. Second, at the top right of RStudio, you can see the name example-project. This label previously read Project: (None). If you want to make sure you’re working in a project, check for its name here. Figure 1.6 shows these changes.\n\n\n\n\n\nFigure 1.6: RStudio with an active project\n\n\nNow that you’ve created a project, copy the population-by-state.csv file into the example-project directory. Once you’ve done so, you should see it in the RStudio files pane.\nWith this CSV file in your project, you can now import it more easily. As before, start by loading the tidyverse package. Then, remove the reference to the Documents folder and import your data by simply using the name of the file:\n\nlibrary(tidyverse)\n\npopulation_data_2 &lt;- read_csv(file = \"population-by-state.csv\")\n\nThe reason you can import the population-by-state.csv file this way is that the RStudio project sets the working directory to be the root of your project. With the working directory set like this, all references to files are relative to the .Rproj file at the root of the project. Now anyone can run this code because it imports the data from a location that is guaranteed to exist on their computer.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#data-analysis-with-the-tidyverse",
    "href": "crash-course.html#data-analysis-with-the-tidyverse",
    "title": "1  An R Programming Crash Course",
    "section": "Data Analysis with the tidyverse",
    "text": "Data Analysis with the tidyverse\nNow that you’ve imported the population data, you’re ready to do a bit of analysis on it. Although I’ve been referring to the tidyverse as a single package, it’s actually a collection of packages. We’ll explore several of its functions throughout this book, but this section introduces you to its basic workflow.\ntidyverse Functions\nBecause you’ve loaded the tidyverse package, you can now access its functions. For example, the package’s summarize() function takes a data frame or tibble and calculates some piece of information for one or more of the variables in that dataset. The following code uses summarize() to calculate the mean population of all states:\n\nsummarize(.data = population_data_2, mean_population = mean(Pop))\n\nFirst, the code passes population_data_2 to the summarize() function’s .data argument to tell R to use that data frame to perform the calculation. Next, it creates a new variable called mean_population and assigns it to the output of the mean() function introduced earlier. The mean() function runs on Pop, one of the variables in the population_data_2 data frame.\nYou might be wondering why you don’t need to use the c() function within mean(), as shown earlier in this chapter. The reason is that you’re passing the function only one argument here: Pop, which contains the set of population data for which you’re calculating the mean. In this case, there’s no need to use c() to combine multiple values into one.\nRunning this code should return a tibble with a single variable (mean_population), as shown here:\n\n\n# A tibble: 1 × 1\n  mean_population\n            &lt;dbl&gt;\n1        6433422.\n\n\nThe variable is of type double (dbl), which is used to hold general numeric data. Other common data types are integer (for whole numbers, such as 4, 82, and 915), character (for text values), and logical (for the TRUE/FALSE values returned from comparison operations). The mean_population variable has a value of 6433422, the mean population of all states.\nNotice also that the summarize() function creates a totally new tibble from the original population_data_2 data frame. This is why the variables from population_data_2 are no longer present in the output. This is a basic example of data analysis, but you can do a lot more with the tidyverse.\nThe tidyverse Pipe\nOne advantage of working with the tidyverse is that it uses the pipe for multi-step operations. The tidyverse pipe, which is written as %&gt;%, allows you to break steps into multiple lines. For example, you could rewrite your code using the pipe like so:\n\npopulation_data_2 %&gt;%\n  summarize(mean_population = mean(Pop))\n\nThis code says, “Start with the population_data_2 data frame, then run the summarize() function on it, creating a variable called mean_population by calculating the mean of the Pop variable.”\nNotice that the line following the pipe is indented. To make the code easier to read, RStudio automatically adds two spaces to the start of lines that follow pipes.\nThe pipe becomes even more useful when you use multiple steps in your data analysis. Say, for example, you want to calculate the mean population of the five largest states. The following code adds a line that uses the filter() function, also from the tidyverse package, to include only states where the rank variable is less than or equal to (&lt;=) 5. Then, it uses summarize() to calculate the mean of those states:\n\npopulation_data_2 %&gt;%\n  filter(rank &lt;= 5) %&gt;%\n  summarize(mean_population = mean(Pop))\n\nRunning this code returns the mean population of the five largest states:\n\n\n# A tibble: 1 × 1\n  mean_population\n            &lt;dbl&gt;\n1        24678497\n\n\nUsing the pipe to combine functions lets you refine your data in multiple ways while keeping it readable and easy to understand. Indentation can also make your code more readable. You’ve seen only a few functions for analysis at this point, but the tidyverse has many more functions that enable you to do nearly anything you could hope to do with your data. Because of how useful the tidyverse is, it will appear in every single piece of R code you write in this book.\n\n\n\n\n\n\nR for Data Science, 2nd edition, by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund is the bible of tidyverse programming and worth reading for more details on how the package’s many functions work.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#comments",
    "href": "crash-course.html#comments",
    "title": "1  An R Programming Crash Course",
    "section": "Comments",
    "text": "Comments\nIn addition to code, R script files often contain comments — lines that begin with hash marks (#) and aren’t treated as runnable code but instead as notes for anyone reading the script. For example, you could add a comment to the code from the previous section, like so:\n\n# Calculate the mean population of the five largest states\npopulation_data_2 %&gt;%\n  filter(rank &lt;= 5) %&gt;%\n  summarize(mean_population = mean(Pop))\n\nThis comment will help others understand what is happening in the code, and it can also serve as a useful reminder for you if you haven’t worked on the code in a while. R knows to ignore any lines that begin with the hash mark instead of trying to run them.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#how-to-get-help",
    "href": "crash-course.html#how-to-get-help",
    "title": "1  An R Programming Crash Course",
    "section": "How to Get Help",
    "text": "How to Get Help\nNow that you’ve learned the basics of how R works, you’re probably ready to dive in and write some code. When you do, though, you’re going to encounter errors. Being able to get help when you run into issues is a key part of learning to use R successfully. There are two main strategies you can use to get unstuck.\nThe first is to read the documentation for the functions you use. Remember, to access the documentation for any function, simply enter ? and then the name of the function in the console. In the bottom-right pane in Figure 1.7, for example, you can see the result of running ?read.csv.\n\n\n\n\n\nFigure 1.7: The documentation for the read.csv() function\n\n\nHelp files can be a bit hard to decipher, but essentially they describe what package the function comes from, what the function does, what arguments it accepts, and some examples of how to use it.\n\n\n\n\n\n\nFor additional guidance on reading documentation, I recommend the appendix of Kieran Healy’s book Data Visualization: A Practical Introduction. A free online version is available at https://socviz.co/appendix.html.\n\n\n\nThe second approach is to read the documentation websites associated with many R packages. These can be easier to read than RStudio’s help files. In addition, they often contain longer articles, known as vignettes, that provide an overview of how a given package works. Reading these can help you understand how to combine individual functions in the context of a larger project. Every package discussed in this book has a good documentation website.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#summary",
    "href": "crash-course.html#summary",
    "title": "1  An R Programming Crash Course",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you learned the basics of R programming. You saw how to download and set up R and RStudio, what the various RStudio panes are for, and how R script files work. You also learned how to import CSV files and explore them in R, how to save data as objects, and how to install packages to access additional functions. Then, to make the files used in your code more accessible, you created an RStudio project. Finally, you experimented with tidyverse functions and the tidyverse pipe, and you learned how to get help when those functions don’t work as expected.\nNow that you understand the basics, you’re ready to start using R to work with your data. See you in Chapter 2!",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "crash-course.html#additional-resources",
    "href": "crash-course.html#additional-resources",
    "title": "1  An R Programming Crash Course",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nKieran Healy, Data Visualization: A Practical Introduction (Princeton, NJ: Princeton University Press, 2018), https://socviz.co.\nChester Ismay and Albert Y. Kim, Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Boca Raton, FL: CRC Press, 2020), https://moderndive.com.\nDavid Keyes, “Getting Started with R,” online course, accessed November 10, 2023, https://rfortherestofus.com/courses/getting-started.\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund, R for Data Science, 2nd ed. (Sebastopol, CA: O’Reilly Media, 2023). https://r4ds.hadley.nz/",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An R Programming Crash Course</span>"
    ]
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "2  Principles of Data Visualization",
    "section": "",
    "text": "The Drought Visualization\nOther news organizations had relied on the same National Drought Center data in their stories, but Scherer and Karamanis visualized it so that it both grabs attention and communicates the scale of the phenomenon. Figure 2.1 shows a section of the final visualization (due to space constraints, I could include only four regions). The graph makes apparent the increase in drought conditions over the last two decades, especially in California and the Southwest.\nTo understand why this visualization is effective, let’s break it down.\nAt the broadest level, the data visualization is notable for its minimalist aesthetic. For example, there are no grid lines and few text labels, as well as minimal text along the axes. Scherer and Karamanis removed what statistician Edward Tufte, in his 1983 book The Visual Display of Quantitative Information (Graphics Press), calls chartjunk. Tufte wrote that extraneous elements often hinder, rather than help, our understanding of charts (and researchers and data visualization designers have generally agreed).\nNeed proof that Scherer and Karamanis’s decluttered graph is better than the alternative? Figure 2.2 shows a version with a few tweaks to the code to include grid lines and text labels on axes.\nFigure 2.1: A section of the final drought visualization, with a few tweaks made to fit this book\nFigure 2.2: The cluttered version of the drought visualization\nIt’s not just that this cluttered version looks worse; the clutter actively inhibits understanding. Rather than focusing on overall drought patterns (the point of the graph), our brains get stuck reading repetitive and unnecessary axis text.\nOne of the best ways to reduce clutter is to break a single chart into a set of component charts, as Scherer and Karamanis have done (this approach, known as faceting, will be discussed further in Section 2.4.3). Each rectangle represents one region in one year. Filtering the larger chart to show the Southwest region in 2003 produces the graph shown in Figure 2.3, where the x-axis indicates the week and the y-axis indicates the percentage of that region at different drought levels.\nFigure 2.3: A drought visualization for the Southwest in 2003\nZooming in on a single region in a single year also makes the color choices more obvious. The lightest orange bars show the percentage of the region that is abnormally dry, and the darkest purple bars show the percentage experiencing exceptional drought conditions. As you’ll see shortly, this range of colors was intentionally chosen to make differences in the drought levels visible to all readers.\nDespite the graph’s complexity, the R code that Scherer and Karamanis wrote to produce it is relatively simple, due largely to a theory called the grammar of graphics.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#the-grammar-of-graphics",
    "href": "data-viz.html#the-grammar-of-graphics",
    "title": "2  Principles of Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nWhen working in Excel, you begin by selecting the type of graph you want to make. Need a bar chart? Click the bar chart icon. Need a line chart? Click the line chart icon. If you’ve only ever made charts in Excel, this first step may seem so obvious that you’ve never even given the data visualization process much thought, but in fact there are many ways to think about graphs. For example, rather than thinking of graph types as distinct, we can recognize and use their commonalities as the starting point for making them.\nThis approach to thinking about graphs comes from the late statistician Leland Wilkinson. For years, Wilkinson thought deeply about what data visualization is and how we can describe it. In 1999 he published a book called The Grammar of Graphics (Springer) that sought to develop a consistent way of describing all graphs. In it, Wilkinson argued that we should think of plots not as distinct types, à la Excel, but as following a grammar that we can use to describe any plot. Just as English grammar tells us that a noun is typically followed by a verb (which is why “he goes” works, while the opposite, “goes he,” does not), the grammar of graphics helps us understand why certain graph types “work.”\nThinking about data visualization through the lens of the grammar of graphics helps highlight, for example, that graphs typically have some data that is plotted on the x-axis and other data that is plotted on the y-axis. This is the case whether the graph is a bar chart or a line chart, as Figure 2.4 shows.\n\n\n\n\n\n\n\nFigure 2.4: A bar chart and a line chart showing identical data\n\n\n\n\nWhile the graphs look different (and would, to the Excel user, be different types of graphs), Wilkinson’s grammar of graphics emphasizes their similarities. (Incidentally, Wilkinson’s feelings on graph-making tools like Excel became clear when he wrote that “most charting packages channel user requests into a rigid array of chart types.”)\nWhen Wilkinson wrote his book, no data visualization tool could implement his grammar of graphics. This would change in 2010, when Hadley Wickham announced the ggplot2 package for R in the article “A Layered Grammar of Graphics,” published in the Journal of Computational and Graphical Statistics. By providing the tools to implement Wilkinson’s ideas, ggplot2 would come to revolutionize the world of data visualization.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#working-with-ggplot2",
    "href": "data-viz.html#working-with-ggplot2",
    "title": "2  Principles of Data Visualization",
    "section": "Working With ggplot2",
    "text": "Working With ggplot2\nThe ggplot2 R package (which I, like nearly everyone in the data visualization world, will refer to simply as ggplot) relies on the idea of plots having multiple layers. This section will walk you through some of the most important ones. You’ll begin by selecting variables to map to aesthetic properties. Then you’ll choose a geometric object to use to represent your data. Next, you’ll change the aesthetic properties of your chart (its color scheme, for example) using a scale_ function. Finally, you’ll use a theme_ function to set the overall look and feel of your plot.\nMapping Data to Aesthetic Properties\nTo create a graph with ggplot, you begin by mapping data to aesthetic properties. All this really means is that you use elements like the x- or y-axis, color, and size (the so-called aesthetic properties) to represent variables. You’ll use the data on life expectancy in Afghanistan, introduced in Figure 2.4, to generate a plot. To access this data, enter the following code:\n\nlibrary(tidyverse)\n\ngapminder_10_rows &lt;- read_csv(\"https://data.rfortherestofus.com/gapminder_10_rows.csv\")\n\nThis code first loads the tidyverse package, introduced in Chapter 1, and then uses the read_csv() function to access data from the book’s website and assign it to the gapminder_10_rows object.\nThe resulting gapminder_10_rows tibble looks like this:\n\n\n# A tibble: 10 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n\n\nThis output is a shortened version of the full which includes over 1,700 rows of data.\nBefore making a chart with ggplot, you need to decice which variable to put on the x-axis and which to put on the y-axis. For data showing change over time, it’s common to put the date (in this case, year) on the x-axis and the changing value (in this case, lifeExp) on the y-axis. To do so, define the ggplot() function as follows:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n)\n\nThis function contains numerous arguments. Each argument goes on its own line, for the sake of readability, separated by commas. The data argument tells R to use the data frame gapminder_10_rows, and the mapping argument maps year to the x-axis and lifeExp to the y-axis.\nRunning this code produces the chart in Figure 2.5, which doesn’t look like much yet.\n\n\n\n\n\n\n\nFigure 2.5: A blank chart that maps year values to the x-axis and life expectancy values to the y-axis\n\n\n\n\nNotice that the x-axis corresponds to year and the y-axis corresponds to lifeExp, and the values on both axes match the scope of the data. In the gapminder_10_rows data frame, the first year is 1952 and the last year is 1997. The range of the x-axis has been created with this data in mind. Likewise, the values for lifeExp, which go from about 28 to about 42, will fit nicely on the y-axis.\nChoosing the Geometric Objects\nAxes are nice, but the graph is missing any type of visual representation of the data. To get this, you need to add the next ggplot layer: geoms. Short for geometric objects, geoms are functions that provide different ways of representing data. For example, to add points to the graph, you use geom_point():\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_point()\n\nNow the graph shows that people in 1952 had a life expectancy of about 28 and that this value rose every year in the dataset (see Figure 2.6).\n\n\n\n\n\n\n\nFigure 2.6: The life expectancy chart with points added\n\n\n\n\nSay you change your mind and want to make a line chart instead. All you have to do is replace geom_point() with geom_line() like so:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_line()\n\nFigure 2.7 shows the result.\n\n\n\n\n\n\n\nFigure 2.7: The data as a line chart\n\n\n\n\nTo really get fancy, you could add both geom_point() and geom_line() as follows:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_point() +\n  geom_line()\n\nThis code generates a line chart with points, as shown in Figure 2.8.\n\n\n\n\n\n\n\nFigure 2.8: The same data with both points and a line\n\n\n\n\nYou can swap in geom_col() to create a bar chart:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp\n  )\n) +\n  geom_col()\n\nNotice in Figure 2.9 that the y-axis range has been automatically updated, going from 0 to 40 to account for the different geom.\n\n\n\n\n\n\n\nFigure 2.9: The life expectancy data as a bar chart\n\n\n\n\nAs you can see, the difference between a line chart and a bar chart isn’t as great as the Excel chart-type picker might have you believe. Both can have the same underlying properties (namely, years on the x-axis and life expectancies on the y-axis). They simply use different geometric objects to visually represent the data.\nMany geoms are built into ggplot. In addition to geom_bar(), geom_point(), and geom_line(), the geoms geom_histogram(), geom_boxplot(), and geom_area() are among the most commonly used. To see all geoms, visit the ggplot documentation website at https://ggplot2.tidyverse.org/reference/index.html#geoms.\nAltering Aesthetic Properties\nBefore we return to the drought data visualization, let’s look at a few additional layers you can use to alter the bar chart. Say you want to change the color of the bars. In the grammar of graphics approach to chart-making, this means mapping some variable to the aesthetic property of fill. (For a bar chart, the aesthetic property of color would change only the outline of each bar.) In the same way that you mapped year to the x-axis and lifeExp to the y-axis, you can map fill to a variable, such as year:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp,\n    fill = year\n  )\n) +\n  geom_col()\n\nFigure 2.10 shows the result. Now the fill is darker for earlier years and lighter for later years (as also indicated by the legend, added to the right of the plot).\n\n\n\n\n\n\n\nFigure 2.10: The same chart, now with added colors\n\n\n\n\nTo change the fill colors, use a new scale layer with the scale_fill _viridis_c() function (the c at the end of the function name refers to the fact that the data is continuous, meaning it can take any numeric value):\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp,\n    fill = year\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_c()\n\nThis function changes the default palette to one that is colorblind-friendly and prints well in grayscale. The scale_fill_viridis_c() function is just one of many that start with scale_ and can alter the fill scale. Chapter 11 of ggplot2: Elegant Graphics for Data Analysis, 3rd edition, discusses various color and fill scales. You can read it online at https://ggplot2-book.org/scales-colour.html.\nThe Fourth Layer: Setting a Theme\nThe final layer we’ll look at is the theme layer, which allows you to change the overall look and feel of your plots (including their background and grid lines). As with the scale_ functions, a number of functions also start with theme_. Add theme_minimal() as follows:\n\nggplot(\n  data = gapminder_10_rows,\n  mapping = aes(\n    x = year,\n    y = lifeExp,\n    fill = year\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_c() +\n  theme_minimal()\n\nThis theme starts to declutter the plot, as you can see in Figure 2.11.\n\n\n\n\n\n\n\nFigure 2.11: The same chart with theme_minimal() added\n\n\n\n\nBy now, you should see why Hadley Wickham described the ggplot2 package as using a layered grammar of graphics. It implements Wilkinson’s theory by creating multiple layers: first, variables to map to aesthetic properties; second, geoms to represent the data; third, the scale_ function to adjust aesthetic properties; and finally, the theme_ function to set the plot’s overall look and feel.\nYou could still improve this plot in many ways, but instead let’s return to the drought data visualization by Scherer and Karamanis. By walking through their code, you’ll learn about making high-quality data visualization with ggplot and R.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#recreating-the-drought-visualization-with-ggplot",
    "href": "data-viz.html#recreating-the-drought-visualization-with-ggplot",
    "title": "2  Principles of Data Visualization",
    "section": "Recreating the Drought Visualization with ggplot",
    "text": "Recreating the Drought Visualization with ggplot\nThe drought visualization code relies on a combination of ggplot fundamentals and some lesser-known tweaks that make it really shine. To understand how Scherer and Karamanis made their data visualization, we’ll start with a simplified version of their code, then build it up layer by layer, adding elements as we go.\nFirst, you’ll import the data. Scherer and Karamanis did a bunch of data wrangling on the raw data, but I’ve saved the simplified output for you. Because it’s in JavaScript Object Notation (JSON) format, Scherer and Karamanis use the import() function from the rio package, which simplifies the process of importing JSON data:\n\nlibrary(rio)\n\nddm_perc_cat_hubs &lt;- import(\"https://data.rfortherestofus.com/dm_perc_cat_hubs.json\")\n\nJSON is a common format for data used in web applications, though it’s far less common in R, where it can be complicated to work with. Luckily, the rio package simplifies its import.\nPlotting One Region and Year\nScherer and Karamanis’s final plot consists of many years and regions. To see how they created it, we’ll start by looking at just the Southwest region in 2003.\nFirst, you need to create a data frame. You’ll use the filter() function twice: the first time to keep only data for the Southwest region, and the second time to keep only data from 2003. In both cases, you use the following syntax:\n\nfilter(variable_name == value)\n\nThis tells R to keep only observations where variable_name is equal to some value. The code starts with the dm_perc_cat_hubs_raw data frame before filtering it and then saving it as a new object called southwest_2003:\n\nsouthwest_2003 &lt;- dm_perc_cat_hubs %&gt;%\n  filter(hub == \"Southwest\") %&gt;%\n  filter(year == 2003)\n\nTo take a look at this object and see the variables you have to work with, enter southwest_2003 in the console, which should return this output:\n\n\n# A tibble: 255 × 7\n   date       hub       category percentage  year  week max_week\n   &lt;date&gt;     &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 2003-12-30 Southwest D0           0.0718  2003    52       52\n 2 2003-12-30 Southwest D1           0.0828  2003    52       52\n 3 2003-12-30 Southwest D2           0.269   2003    52       52\n 4 2003-12-30 Southwest D3           0.311   2003    52       52\n 5 2003-12-30 Southwest D4           0.0796  2003    52       52\n 6 2003-12-23 Southwest D0           0.0823  2003    51       52\n 7 2003-12-23 Southwest D1           0.131   2003    51       52\n 8 2003-12-23 Southwest D2           0.189   2003    51       52\n 9 2003-12-23 Southwest D3           0.382   2003    51       52\n10 2003-12-23 Southwest D4           0.0828  2003    51       52\n# ℹ 245 more rows\n\n\nThe date variable represents the start date of the week in which the observation took place. The hub variable is the region, and category is the level of drought: a value of D0 indicates the lowest level of drought, while D5 indicates the highest level. The percentage variable is the percentage of that region in that drought category, ranging from 0 to 1. The year and week variables are the observation year and week number (beginning with week 1). The max_week variable is the maximum number of weeks in a given year.\nNow you can use this southwest_2003 object for your plot:\n\nggplot(\n  data = southwest_2003,\n  aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )\n) +\n  geom_col()\n\nThe ggplot() function tells R to put week on the x-axis and percentage on the y-axis, as well as to use the category variable for the fill color. The geom_col() function creates a bar chart in which each bar’s fill color represents the percentage of the region at each drought level for that particular week, as shown in Figure 2.12.\n\n\n\n\n\n\n\nFigure 2.12: One year (2003) and region (Southwest) of the drought visualization\n\n\n\n\nThe colors, which include bright pinks, blues, greens, and reds, don’t match the final version of the plot, but you can start to see the outlines of Scherer and Karamanis’s data visualization.\nChanging Aesthetic Properties\nScherer and Karamanis next selected different fill colors for their bars. To do so, they used the scale_fill_viridis_d() function. The d here means that the data to which the fill scale is being applied has discrete categories (D0, D1, D2, D3, D4, and D5):\n\nggplot(\n  data = southwest_2003,\n  aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  )\n\nThey used the argument option = \"rocket\" to select the rocket palette, whose colors range from cream to nearly black. You could use several other palettes within the scale_fill_viridis_d() function; see them at https://sjmgarnier.github.io/viridisLite/reference/viridis.html.\nThen they used the direction = -1 argument to reverse the order of fill colors so that darker colors mean higher drought conditions.\nScherer and Karamanis also tweaked the appearance of the x- and y-axes:\n\nggplot(\n  data = southwest_2003,\n  aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )\n) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  ) +\n  scale_x_continuous(\n    name = NULL,\n    guide = \"none\"\n  ) +\n  scale_y_continuous(\n    name = NULL,\n    labels = NULL,\n    position = \"right\"\n  )\n\nOn the x axis, they removed both the axis title (“week”) using name = NULL and the 0–50 text with guide = \"none\". On the y axis, they removed the title and text showing percentages using labels = NULL, which functionally does the same thing as guide = \"none\". They also moved the axis lines themselves to the right side using position = \"right\". These axis lines are apparent only as tick marks at this point but will become more visible later. Figure 2.13 shows the result of these tweaks.\nOn the x-axis, they removed both the axis title (“week”) using name = NULL and the axis labels (the weeks numbered 0 to 50) with guide = \"none\". On the y-axis, they removed the title and text showing percentages using labels = NULL, which functionally does the same thing as guide = \"none\". They also moved the axis lines themselves to the right side using position = \"right\". These axis lines are apparent only as tick marks at this point but will become more visible later. Figure 2.13 shows the result of these tweaks.\n\n\n\n\n\n\n\nFigure 2.13: The 2003 drought data for the Southwest with adjustments to the x- and y-axes\n\n\n\n\nUp to this point, we’ve focused on one of the single plots that make up the larger data visualization. But the final product that Scherer and Karamanis made is actually 176 plots visualizing 22 years and 8 regions. Let’s discuss the ggplot feature they used to create all of these plots.\nFaceting the Plot\nOne of ggplot’s most useful capabilities is faceting (or, as it’s more commonly known in the data visualization world, small multiples). Faceting uses a variable to break down a single plot into multiple plots. For example, think of a line chart showing life expectancy by country over time; instead of multiple lines on one plot, faceting would create multiple plots with one line per plot. To specify which variable to put in the rows and which to put in the columns of your faceted plot, you use the facet_grid() function, as Scherer and Karamanis did in their code:\n\ndm_perc_cat_hubs %&gt;%\n  filter(hub %in% c(\n    \"Northwest\",\n    \"California\",\n    \"Southwest\",\n    \"Northern Plains\"\n  )) %&gt;%\n  ggplot(aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  ) +\n  scale_x_continuous(\n    name = NULL,\n    guide = \"none\"\n  ) +\n  scale_y_continuous(\n    name = NULL,\n    labels = NULL,\n    position = \"right\"\n  ) +\n  facet_grid(\n    rows = vars(year),\n    cols = vars(hub),\n    switch = \"y\"\n  )\n\nScherer and Karamanis put year in rows and hub (region) in columns. The switch = \"y\" argument moves the year label from the right side (where it appears by default) to the left. With this code in place, you can see the final plot coming together in Figure 2.14.\n\n\n\n\n\n\n\nFigure 2.14: The faceted version of the drought visualization\n\n\n\n\nIncredibly, the broad outlines of the plot took just 10 lines of code to create. The rest of the code falls into the category of small polishes. That’s not to minimize how important small polishes are (very) or the time it takes to create them (a lot). It does show, however, that a little bit of ggplot goes a long way.\nAdding Final Polishes\nNow let’s look at a few of the small polishes that Scherer and Karamanis made. The first is to apply a theme. They used theme_light(), which removes the default gray background and changes the font to Roboto using the base_family argument.\nThe theme_light() function is what’s known as a complete theme, one that changes the overall look and feel of a plot. The ggplot package has multiple complete themes that you can use (they’re listed at https://ggplot2.tidyverse.org/reference/index.html#themes). Individuals and organizations also make their own themes, as you’ll do in Chapter 3. For a discussion of which themes you might consider using, see my blog post at https://rfortherestofus.com/2019/08/themes-to-improve-your-ggplot-figures.\nScherer and Karamanis didn’t stop by simply applying theme_light(). They also used the theme() function to make additional tweaks to the plot’s design:\n\ndm_perc_cat_hubs %&gt;%\n  filter(hub %in% c(\n    \"Northwest\",\n    \"California\",\n    \"Southwest\",\n    \"Northern Plains\"\n  )) %&gt;%\n  ggplot(aes(\n    x = week,\n    y = percentage,\n    fill = category\n  )) +\n  geom_rect(\n    aes(\n      xmin = .5,\n      xmax = max_week + .5,\n      ymin = -0.005,\n      ymax = 1\n    ),\n    fill = \"#f4f4f9\",\n    color = NA,\n    size = 0.4\n  ) +\n  geom_col() +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1\n  ) +\n  scale_x_continuous(\n    name = NULL,\n    guide = \"none\"\n  ) +\n  scale_y_continuous(\n    name = NULL,\n    labels = NULL,\n    position = \"right\"\n  ) +\n  facet_grid(\n    rows = vars(year),\n    cols = vars(hub),\n    switch = \"y\"\n  ) +\n  theme_light(base_family = \"Roboto\") +\n  theme(\n    axis.title = element_text(\n      size = 14,\n      color = \"black\"\n    ),\n    axis.text = element_text(\n      family = \"Roboto Mono\",\n      size = 11\n    ),\n    axis.line.x = element_blank(),\n    axis.line.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.length.y = unit(2, \"mm\"),\n    legend.position = \"top\",\n    legend.title = element_text(\n      color = \"#2DAADA\",\n      face = \"bold\"\n    ),\n    legend.text = element_text(color = \"#2DAADA\"),\n    strip.text.x = element_text(\n      hjust = .5,\n      face = \"plain\",\n      color = \"black\",\n      margin = margin(t = 20, b = 5)\n    ),\n    strip.text.y.left = element_text(\n      angle = 0,\n      vjust = .5,\n      face = \"plain\",\n      color = \"black\"\n    ),\n    strip.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.spacing.x = unit(0.3, \"lines\"),\n    panel.spacing.y = unit(0.25, \"lines\"),\n    panel.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.border = element_rect(\n      color = \"transparent\",\n      size = 0\n    ),\n    plot.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\",\n      size = .4\n    ),\n    plot.margin = margin(rep(18, 4))\n  )\n\nThe code in the theme() function does many different things, but let’s look at a few of the most important. First, it moves the legend from the right side (the default) to the top of the plot. Then, the angle = 0 argument rotates the year text in the columns from vertical to horizontal. Without this argument, the years would be much less legible.\nThe theme() function also makes the distinctive axis lines and ticks that appear on the right side of the final plot. Calling element_blank() removes all grid lines. Finally, this code removes the borders and gives each individual plot a transparent background.\nYou might be thinking, Wait. Didn’t the individual plots have a gray background behind them? Yes, dear reader, they did. Scherer and Karamanis made these with a separate geom, geom_rect():\n\ngeom_rect(\n  aes(\n    xmin = .5,\n    xmax = max_week + .5,\n    ymin = -0.005,\n    ymax = 1\n  ),\n  fill = \"#f4f4f9\",\n  color = NA,\n  size = 0.4\n)\n\nThey also set some additional aesthetic properties specific to this geom — xmin, xmax, ymin, and ymax — which determine the boundaries of the rectangle it produces. The result is a gray background behind each small multiple, as shown in Figure 2.15.\n\n\n\n\n\n\n\nFigure 2.15: Faceted version of the drought visualization with gray backgrounds behind each small multiple\n\n\n\n\nFinally, Scherer and Karamanis made some tweaks to the legend. Previously you saw a simplified version of the scale_fill_viridis_d() function. Here’s a more complete version:\n\nscale_fill_viridis_d(\n  option = \"rocket\",\n  direction = -1,\n  name = \"Category:\",\n  labels = c(\n    \"Abnormally Dry\",\n    \"Moderate Drought\",\n    \"Severe Drought\",\n    \"Extreme Drought\",\n    \"Exceptional Drought\"\n  )\n)\n\nThe name argument sets the legend title, and the labels argument specifies the labels that show up in the legend. Figure 2.16 shows the result of these changes.\n\n\n\n\n\n\n\nFigure 2.16: The drought visualization with changes to the legend text\n\n\n\n\nRather than D0, D1, D2, D3, and D4, the legend text now reads Abnormally Dry, Moderate Drought, Severe Drought, Extreme Drought, and Exceptional Drought — much more user-friendly categories.\nThe Complete Visualization Code\nWhile I’ve shown you a nearly complete version of the code that Scherer and Karamanis wrote, I made some small changes to make it easier to understand. If you’re curious, the full code is here:\n\nggplot(dm_perc_cat_hubs, aes(week, percentage)) +\n  geom_rect(\n    aes(\n      xmin = .5,\n      xmax = max_week + .5,\n      ymin = -0.005,\n      ymax = 1\n    ),\n    fill = \"#f4f4f9\",\n    color = NA,\n    size = 0.4,\n    show.legend = FALSE\n  ) +\n  geom_col(\n    aes(\n      fill = category,\n      fill = after_scale(addmix(\n        darken(\n          fill,\n          .05,\n          space = \"HLS\"\n        ),\n        \"#d8005a\",\n        .15\n      )),\n      color = after_scale(darken(\n        fill,\n        .2,\n        space = \"HLS\"\n      ))\n    ),\n    width = .9,\n    size = 0.12\n  ) +\n  facet_grid(\n    rows = vars(year),\n    cols = vars(hub),\n    switch = \"y\"\n  ) +\n  coord_cartesian(clip = \"off\") +\n  scale_x_continuous(\n    expand = c(.02, .02),\n    guide = \"none\",\n    name = NULL\n  ) +\n  scale_y_continuous(\n    expand = c(0, 0),\n    position = \"right\",\n    labels = NULL,\n    name = NULL\n  ) +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    name = \"Category:\",\n    direction = -1,\n    begin = .17,\n    end = .97,\n    labels = c(\n      \"Abnormally Dry\",\n      \"Moderate Drought\",\n      \"Severe Drought\",\n      \"Extreme Drought\",\n      \"Exceptional Drought\"\n    )\n  ) +\n  guides(fill = guide_legend(\n    nrow = 2,\n    override.aes = list(size = 1)\n  )) +\n  theme_light(\n    base_size = 18,\n    base_family = \"Roboto\"\n  ) +\n  theme(\n    axis.title = element_text(\n      size = 14,\n      color = \"black\"\n    ),\n    axis.text = element_text(\n      family = \"Roboto Mono\",\n      size = 11\n    ),\n    axis.line.x = element_blank(),\n    axis.line.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.y = element_line(\n      color = \"black\",\n      size = .2\n    ),\n    axis.ticks.length.y = unit(2, \"mm\"),\n    legend.position = \"top\",\n    legend.title = element_text(\n      color = \"#2DAADA\",\n      size = 18,\n      face = \"bold\"\n    ),\n    legend.text = element_text(\n      color = \"#2DAADA\",\n      size = 16\n    ),\n    strip.text.x = element_text(\n      size = 16,\n      hjust = .5,\n      face = \"plain\",\n      color = \"black\",\n      margin = margin(t = 20, b = 5)\n    ),\n    strip.text.y.left = element_text(\n      size = 18,\n      angle = 0,\n      vjust = .5,\n      face = \"plain\",\n      color = \"black\"\n    ),\n    strip.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.spacing.x = unit(0.3, \"lines\"),\n    panel.spacing.y = unit(0.25, \"lines\"),\n    panel.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\"\n    ),\n    panel.border = element_rect(\n      color = \"transparent\",\n      size = 0\n    ),\n    plot.background = element_rect(\n      fill = \"transparent\",\n      color = \"transparent\",\n      size = .4\n    ),\n    plot.margin = margin(rep(18, 4))\n  )\n\nThere are a few additional tweaks to color and spacing, but most of the code reflects what you’ve seen so far.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#in-conclusion-ggplot-is-your-data-visualization-secret-weapon",
    "href": "data-viz.html#in-conclusion-ggplot-is-your-data-visualization-secret-weapon",
    "title": "2  Principles of Data Visualization",
    "section": "In Conclusion: ggplot is Your Data Visualization Secret Weapon",
    "text": "In Conclusion: ggplot is Your Data Visualization Secret Weapon\nYou may be thinking that ggplot is the solution to all of your data visualization problems. And yes, you have a new hammer, but not everything is a nail. If you look at the version of the data visualization that appeared in Scientific American in November 2021, you’ll see that some of its annotations aren’t visible in our re-creation. That’s because they were added in post-production. While you could have found ways to create them in ggplot, it’s often not the best use of your time. Get yourself 90 percent of the way there with ggplot and then use Illustrator, Figma, or a similar tool to finish your work.\nEven so, ggplot is a very powerful hammer, used to make plots that you’ve seen in the New York Times, FiveThirtyEight, the BBC, and other well-known news outlets. Although it’s not the only tool that can generate high-quality data visualizations, it makes the process straightforward. The graph by Scherer and Karamanis shows this in several ways:\n\nIt strips away extraneous elements, such as grid lines, to keep the focus on the data itself. Complete themes such as theme_light() and the theme() function allowed Scherer and Karamanis to create a decluttered visualization that communicates effectively.\nIt uses well-chosen colors. The scale_fill_viridis_d() function allowed them to create a color scheme that demonstrates differences between groups, is colorblind-friendly, and shows up well when printed in grayscale.\nIt uses faceting to break down data from two decades and eight regions into a set of graphs that come together to create a single plot. With a single call to the facet_grid() function, Scherer and Karamanis created over 100 small multiples that the tool automatically combined into a single plot.\n\nLearning to create data visualizations in ggplot involves a significant time investment. But the long-term payoff is even greater. Once you learn how ggplot works, you can look at others’ code and learn how to improve your own. By contrast, when you make a data visualization in Excel, the series of point-and-click steps disappears into the ether. To re-create a visualization you made last week, you’ll need to remember the exact steps you used, and to make someone else’s data visualization, you’ll need them to write up their process for you.\nBecause code-based data visualization tools allow you to keep a record of the steps you made, you don’t have to be the most talented designer to make high-quality data visualizations with ggplot. You can study others’ code, adapt it to your own needs, and create your own data visualization that not only is beautiful but also communicates effectively.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-viz.html#additional-resources",
    "href": "data-viz.html#additional-resources",
    "title": "2  Principles of Data Visualization",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWill Chase, “The Glamour of Graphics,” online course, accessed November 6, 2023, https://rfortherestofus.com/courses/glamour/.\nKieran Healy, Data Visualization: A Practical Introduction (Princeton, NJ: Princeton University Press, 2018), https://socviz.co.\nCédric Scherer, Graphic Design with ggplot2 (Boca Raton, FL: CRC Press, forthcoming).\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis, 3rd ed. (New York: Springer, forthcoming), https://ggplot2-book.org.\nClaus Wilke, Fundamentals of Data Visualization (Sebastopol, CA: O’Reilly Media, 2019), https://clauswilke.com/dataviz/.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Principles of Data Visualization</span>"
    ]
  },
  {
    "objectID": "themes.html",
    "href": "themes.html",
    "title": "3  Custom Data Visualization Themes",
    "section": "",
    "text": "Styling a Plot with a Custom Theme\nThe bbplot package has two functions: bbc_style() and finalise_plot(). The latter deals with tasks like adding the BBC logo and saving plots in the correct dimensions. For now, let’s look at the bbc_style() function, which applies a custom ggplot theme to make all the plots look consistent and follow BBC style guidelines.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "themes.html#styling-a-plot-with-a-custom-theme",
    "href": "themes.html#styling-a-plot-with-a-custom-theme",
    "title": "3  Custom Data Visualization Themes",
    "section": "",
    "text": "An Example Plot\nTo see how this function works, you’ll create a plot showing population data about several penguin species. You’ll be using the palmerpenguins package, which contains data about penguins living on three islands in Antarctica. For a sense of what this data looks like, load the palmerpenguins and tidyverse packages:\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\nNow you have data you can work with in an object called penguins. Here’s what the first 10 rows look like:\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nTo get the data in a more usable format, you’ll count how many penguins live on each island with the count() function from the dplyr package (one of several packages that are loaded with the tidyverse):\n\npenguins %&gt;%\n  count(island)\n\nThis gives you some simple data that you can use for plotting:\n\n\n# A tibble: 3 × 2\n  island        n\n  &lt;fct&gt;     &lt;int&gt;\n1 Biscoe      168\n2 Dream       124\n3 Torgersen    52\n\n\nYou’ll use this data multiple times in the chapter, so save it as an object called penguins_summary like so:\n\npenguins_summary &lt;- penguins %&gt;%\n  count(island)\n\nNow you’re ready to create a plot. Before you see what bbplot does, make a plot with the ggplot defaults:\n\npenguins_plot &lt;- ggplot(\n  data = penguins_summary,\n  aes(\n    x = island,\n    y = n,\n    fill = island\n  )\n) +\n  geom_col() +\n  labs(\n    title = \"Number of Penguins\",\n    subtitle = \"Islands are in Antarctica\",\n    caption = \"Data from palmerpenguins package\"\n  )\n\nThis code tells R to use the penguins_summary data frame, putting the island on the x-axis and the count of the number of penguins (n) on the y-axis, and making each bar a different color with the fill aesthetic property. Since you’ll modify this plot multiple times, saving it as an object called penguins_plot simplifies the process. Figure 3.1 shows the resulting plot.\n\n\n\n\n\n\n\nFigure 3.1: A chart with the default theme\n\n\n\n\nThis isn’t the most aesthetically pleasing chart. The gray background is ugly, the y-axis title is hard to read because it’s angled, and the text size overall is quite small. But don’t worry, you’ll be improving it soon.\nThe BBC’s Custom Theme\nNow that you have a basic plot to work with, you’ll start making it look like a BBC chart. To do this, you need to install the bbplot package. First, install the remotes package using install.packages(\"remotes\") so that you can access packages from remote sources. Then, run the following code to install bbplot from the GitHub repository at https://github.com/bbc/bbplot:\n\nlibrary(remotes)\ninstall_github(\"bbc/bbplot\")\n\nOnce you’ve installed the bbplot package, load it and apply the bbc_style() function to the penguins_plot as follows:\n\nlibrary(bbplot)\n\npenguins_plot +\n  bbc_style()\n\nFigure 3.2 shows the result.\n\n\n\n\n\n\n\nFigure 3.2: The same chart with BBC style\n\n\n\n\nVastly different, right? The font size is larger, the legend is on top, there are no axis titles, the grid lines are stripped down, and the background is white. Let’s look at these changes one by one.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "themes.html#the-bbc-theme-components",
    "href": "themes.html#the-bbc-theme-components",
    "title": "3  Custom Data Visualization Themes",
    "section": "The BBC Theme Components",
    "text": "The BBC Theme Components\nYou’ve just seen the difference that the bbc_style() function makes to a basic chart. This section walks you through the function’s code, with some minor tweaks for readability. Functions are discussed further in Chapter 12.\nFunction Definition\nThe first line gives the function a name and indicates that what follows is, in fact, a function definition:\n\nbbc_style &lt;- function() {\n  font &lt;- \"Helvetica\"\n  \n  ggplot2::theme(\n\nThe code then defines a variable called font and assigns it the value Helvetica. This allows later sections to simply use font rather than repeating Helvetica multiple times. If the BBC team ever wanted to use a different font, they could change Helvetica here to, say, Comic Sans and it would update the font for all of the BBC plots (though I suspect higher-ups at the BBC might not be on board with that choice).\nHistorically, working with custom fonts in R was notoriously tricky, but recent changes have made the process much simpler. To ensure that custom fonts such as Helvetica work in ggplot, first install the systemfonts and ragg packages by running this code in the console:\n\ninstall.packages(c(\"systemfonts\", \"ragg\"))\n\nThe systemfonts package allows R to directly access fonts you’ve installed on your computer, and ragg allows ggplot to use those fonts when generating plots.\nNext, select Tools &gt; Global Options from RStudio’s main menu bar. Click the Graphics menu at the top of the interface and, under the Backend option, select AGG. This change should ensure that RStudio renders the previews of any plots with the ragg package. With these changes in place, you should be able to use any fonts you’d like (assuming you have them installed) in the same way that the bbc_style() function uses Helvetica.\nAfter specifying the font to use, the code calls ggplot’s theme() function. Rather than first loading ggplot with library(ggplot2) and then calling its theme() function, the ggplot2::theme() syntax indicates in one step that the theme() function comes from the ggplot2 package. You’ll write code in this way when making an R package in Chapter 12.\nNearly all of the code in bbc_style() exists within this theme() function. Remember from Chapter 2 that theme() makes additional tweaks to an existing theme; it isn’t a complete theme like theme_light(), which will change the whole look and feel of your plot. In other words, by jumping straight into the theme() function, bbc_style() makes adjustments to the ggplot defaults. As you’ll see, the bbc_style() function does a lot of tweaking.\nText\nThe first code section within the theme() function formats the text:\n\nplot.title = ggplot2::element_text(\n  family = font,\n  size = 28,\n  face = \"bold\",\n  color = \"#222222\"\n),\nplot.subtitle = ggplot2::element_text(\n  family = font,\n  size = 22,\n  margin = ggplot2::margin(9, 0, 9, 0)\n),\nplot.caption = ggplot2::element_blank(),\n\nTo make changes to the title, subtitle, and caption, it follows this pattern:\n\nAREA_OF_CHART = ELEMENT_TYPE(\n  PROPERTY = VALUE\n)\n\nFor each area, this code specifies the element type: element_text(), element_line(), element_rect(), or element_blank(). Within the element type is where you assign values to properties—for example, setting the font family (the property) to Helvetica (the value). The bbc_style() function uses the various element_ functions to make tweaks, as you’ll see later in this chapter.\n\n\n\n\n\n\nFor additional ways to customize pieces of your plots, see the ggplot2 package documentation (https://ggplot2.tidyverse.org/reference/element.html), which provides a comprehensive list.\n\n\n\nOne of the main adjustments the bbc_style() function makes is bumping up the font size to help with legibility, especially when plots made with the bbplot package are viewed on smaller mobile devices. The code first formats the title (with plot.title) using Helvetica 28-point bold font in a nearly black color (the hex code #222222). The subtitle (plot.subtitle) is 22-point Helvetica.\nThe bbc_style() code also adds some spacing between the title and subtitle with the margin() function, specifying the value in points for the top (9), right (0), bottom (9), and left (0) sides. Finally, the element_blank() function removes the default caption (set through the caption argument in the labs() function), “Data from palmer penguins package.” (As mentioned earlier, the finalise_plot() function in the bbplot package adds elements, including an updated caption and the BBC logo, to the bottom of the plots.)\nFigure 3.3 shows these changes.\n\n\n\n\n\n\n\nFigure 3.3: The penguin chart with only the text formatting changed\n\n\n\n\nWith these changes in place, you’re on your way to the BBC look.\nLegend\nNext up is formatting the legend, positioning it above the plot and leftaligning its text:\n\nlegend.position = \"top\",\nlegend.text.align = 0,\nlegend.background = element_blank(),\nlegend.title = element_blank(),\nlegend.key = element_blank(),\nlegend.text = element_text(\n  family = font,\n  size = 18,\n  color = \"#222222\"\n),\n\nThis code removes the legend background (which would show up only if the background color of the entire plot weren’t white), the title, and the legend key (the borders on the boxes that show the island names, just barely visible in Figure 3.3). Finally, the code sets the legend’s text to 18-point Helvetica with the same nearly black color. Figure 3.4 shows the result.\n\n\n\n\n\n\n\nFigure 3.4: The penguin chart with changes to the legend\n\n\n\n\nThe legend is looking better, but now it’s time to format the rest of the chart so it matches.\nAxes\nThe code first removes the axis titles because they tend to take up a lot of chart real estate, and you can use the title and subtitle to clarify what the axes show:\n\naxis.title = ggplot2::element_blank(),\naxis.text = ggplot2::element_text(\n  family = font,\n  size = 18,\n  color = \"#222222\"\n),\naxis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)),\naxis.ticks = ggplot2::element_blank(),\naxis.line = ggplot2::element_blank(),\n\nAll text on the axes becomes 18-point Helvetica and nearly black. The text on the x-axis (Biscoe, Dream, and Torgersen) gets a bit of spacing around it. Finally, both axes’ ticks and lines are removed. Figure 3.5 shows these changes, although the removal of the axis lines doesn’t make a difference to the display here.\n\n\n\n\n\n\n\nFigure 3.5: The penguin chart with axis formatting changes\n\n\n\n\nThe axis text matches the legend text, and the axis tick marks and lines are gone.\nGrid Lines\nNow for the grid lines:\n\npanel.grid.minor = ggplot2::element_blank(),\npanel.grid.major.y = ggplot2::element_line(color = \"#cbcbcb\"),\npanel.grid.major.x = ggplot2::element_blank(),\n\nThe approach here is fairly straightforward: this code removes minor grid lines for both axes, removes major grid lines on the x-axis, and keeps major grid lines on the y-axis but makes them a light gray (the #cbcbcb hex code). Figure 3.6 shows the result.\n\n\n\n\n\n\n\nFigure 3.6: Our chart with tweaks to the grid lines\n\n\n\n\nBackground\nThe previous iteration of our plot still had a gray background. The bbc_style() function removes this with the following code.\n\npanel.background = ggplot2::element_blank(),\n\nThe plot without the gray background is seen in Figure @ref(fig:penguins-plot-no-bg).\n\n\n\n\n\n\n\nFigure 3.7: The chart with the gray background removed\n\n\n\n\nYou’ve nearly re-created the penguin plot using the bbc_style() function.\nSmall Multiples\nThe bbc_style() function contains a bit more code to modify strip.background and strip.text. In ggplot, the strip refers to the text above faceted charts like the ones discussed in Chapter 2. Next, you’ll turn your penguin chart into a faceted chart to see these components of the BBC’s theme. I’ve used the code from the bbc_style() function, minus the sections that deal with small multiples, to make Figure 3.8.\n\n\n\n\n\n\n\nFigure 3.8: The faceted chart with no changes to the strip text formatting\n\n\n\n\nUsing the facet_wrap() function to make a small multiples chart leaves you with one chart per island, but by default, the text above each small multiple is noticeably smaller than the rest of the chart. What’s more, the gray background behind the text stands out because you’ve already removed the gray background from the other parts of the chart. The consistency you’ve worked toward is now compromised, with small text that is out of proportion to the other chart text and a gray background that sticks out like a sore thumb.\nThe following code changes the strip text above each small multiple:\n\nstrip.background = ggplot2::element_rect(fill = \"white\"),\nstrip.text = ggplot2::element_text(size = 22, hjust = 0)\n\nThis code removes the background (or, more accurately, colors it white). Then it makes the text larger, bold, and left-aligned using hjust = 0. Note that I did have to make the text size slightly smaller than in the actual chart to fit the book, and I added code to make it bold. Figure 3.9 shows the result.\n\n\n\n\n\n\n\nFigure 3.9: The small multiples chart in the BBC style\n\n\n\n\nIf you look at any chart on the BBC website, you’ll see how similar it looks to your own. The tweaks in the bbc_style() function to the text formatting, legends, axes, grid lines, and backgrounds show up in charts viewed by millions of people worldwide.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "themes.html#color",
    "href": "themes.html#color",
    "title": "3  Custom Data Visualization Themes",
    "section": "Color",
    "text": "Color\nYou might be thinking, Wait, what about the color of the bars? Doesn’t the theme change those? This is a common point of confusion, but the answer is that it doesn’t. The documentation for the theme() function explains why this is the case: “Themes are a powerful way to customize the non-data components of your plots: i.e. titles, labels, fonts, background, gridlines, and legends.” In other words, ggplot themes change the elements of the chart that aren’t mapped to data.\nPlots, on the other hand, use color to communicate information about data. In the faceted chart, for instance, the fill property is mapped to the island (Biscoe is salmon, Dream is green, and Torgersen is blue). As you saw in Chapter 2, you can change the fill using the various scale_fill_ functions. In the world of ggplot, these scale_ functions control color, while the custom themes control the chart’s overall look and feel.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "themes.html#summary",
    "href": "themes.html#summary",
    "title": "3  Custom Data Visualization Themes",
    "section": "Summary",
    "text": "Summary\nWhen Stylianou and Guibourg started developing a custom theme for the BBC, they had one question: Would they be able to create graphs in R that could go directly onto the BBC website? Using ggplot, they succeeded. The bbplot package allowed them to make plots with a consistent look and feel that followed BBC standards and, most important, did not require a designer’s help.\nYou can see many of the principles of high-quality data visualization discussed in Chapter 2 in this custom theme. In particular, the removal of extraneous elements (axis titles and grid lines, for instance) helps keep the focus on the data itself. And because applying the theme requires users to add only a single line to their ggplot code, it was easy to get others on board. They had only to append bbc_style() to their code to produce a BBC-style plot.\nOver time, others at the BBC noticed the data journalism team’s production-ready graphs and wanted to make their own. The team members set up R trainings for their colleagues and developed a “cookbook” (https://bbc.github.io/rcookbook/) showing how to make various types of charts. Soon, the quality and quantity of BBC’s data visualization exploded. Stylianou told me, “I don’t think there’s been a day where someone at the BBC hasn’t used the package to produce a graphic.”\nNow that you’ve seen how custom ggplot themes work, try making one of your own. After all, once you’ve written the code, you can apply it with only one line of code.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "themes.html#additional-resources",
    "href": "themes.html#additional-resources",
    "title": "3  Custom Data Visualization Themes",
    "section": "Additional Resources",
    "text": "Additional Resources\nConsult the following resources to learn more about how the BBC created and used their custom theme:\n\nBBC Visual and Data Journalism cookbook for R graphics (2019), https://bbc.github.io/rcookbook/\n“How the BBC Visual and Data Journalism team works with graphics in R” by the BBC Visual and Data Journalism team (2019), https://medium.com/bbc-visual-and-data-journalism/how-the-bbc-visual-and-data-journalism-team-works-with-graphics-in-r-ed0b35693535",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Custom Data Visualization Themes</span>"
    ]
  },
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "4  Maps and Geospatial Data",
    "section": "",
    "text": "A Brief Primer on Geospatial Data\nYou don’t need to be a GIS expert to make maps, but you do need to understand a few things about how geospatial data works, starting with its two main types: vector and raster. Vector data uses points, lines, and polygons to represent the world. Raster data, which often comes from digital photographs, ties each pixel in an image to a specific geographic location. Vector data tends to be easier to work with, and you’ll be using it exclusively in this chapter.\nIn the past, working with geospatial data meant mastering competing standards, each of which required learning a different approach. Today, though, most people use the simple features model (often abbreviated as sf) for working with vector geospatial data, which is easier to understand. For example, to import simple features data about the state of Wyoming, enter the following:\nlibrary(sf)\n\nwyoming &lt;- read_sf(\"https://data.rfortherestofus.com/wyoming.geojson\")\nAnd then you can look at the data like so:\nwyoming\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -111.0546 ymin: 40.99477 xmax: -104.0522 ymax: 45.00582\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 2\n  NAME                                                                  geometry\n  &lt;chr&gt;                                                            &lt;POLYGON [°]&gt;\n1 Wyoming ((-106.3212 40.99912, -106.3262 40.99927, -106.3265 40.99927, -106.33…\nThe output has two columns, one for the state name (NAME) and another called geometry. This data looks like the data frames you’ve seen before, aside from two major differences.\nFirst, there are five lines of metadata above the data frame. At the top is a line stating that the data contains one feature and one field. A feature is a row of data, and a field is any column containing nonspatial data. Second, the simple features data contains geographical data in a variable called geometry. Because the geometry column must be present for a data frame to be geospatial data, it isn’t counted as a field. Let’s look at each part of this simple features data.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#a-brief-primer-on-geospatial-data",
    "href": "maps.html#a-brief-primer-on-geospatial-data",
    "title": "4  Maps and Geospatial Data",
    "section": "",
    "text": "The Geometry Type\nThe geometry type represents the shape of the geospatial data you’re working with and is typically shown in all caps. In this case, the relatively simple POLYGON type represents a single polygon. You can use ggplot to display this data by calling geom_sf(), a special geom designed to work with simple features data:\n\nlibrary(tidyverse)\n\nwyoming %&gt;%\n  ggplot() +\n  geom_sf()\n\nFigure 4.1 shows the resulting map of Wyoming. It may not look like much, but I wasn’t the one who made Wyoming a nearly perfect rectangle!\n\n\n\n\n\n\n\nFigure 4.1: A map of Wyoming generated using POLYGON simple features data\n\n\n\n\nOther geometry types used in simple features data include POINT, to display elements such as a pin on a map that represents a single location. For example, the map in Figure 4.2 uses POINT data to show a single electric vehicle charging station in Wyoming.\n\n\n\n\n\n\n\nFigure 4.2: A map of Wyoming containing POINT simple features data\n\n\n\n\nThe LINESTRING geometry type is for a set of points that can be connected with lines and is often used to represent roads. Figure 4.3 shows a map that uses LINESTRING data to represent a section of US Highway 30 that runs through Wyoming.\n\n\n\n\n\n\n\nFigure 4.3: A road represented using LINESTRING simple features data\n\n\n\n\nEach of these geometry types has a MULTI variation (MULTIPOINT, MULTILINESTRING, and MULTIPOLYGON) that combines multiple instances of the type in one row of data. For example, Figure 4.4 uses MULTIPOINT data to show all electric vehicle charging stations in Wyoming.\n\n\n\n\n\n\n\nFigure 4.4: Using MULTIPOINT data to represent multiple electric vehicle charging stations\n\n\n\n\nLikewise, you can use MULTILINESTRING data to show not just one road but all major roads in Wyoming (Figure 4.5).\n\n\n\n\n\n\n\nFigure 4.5: Using MULTILINESTRING data to represent several roads\n\n\n\n\nFinally, you could use MULTIPOLYGON data, for example, to depict a state made up of multiple polygons. The following data represents the 23 counties in the state of Wyoming:\n\n\nSimple feature collection with 23 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -111.0546 ymin: 40.99477 xmax: -104.0522 ymax: 45.00582\nGeodetic CRS:  WGS 84\n# A tibble: 23 × 2\n   NAME                                                                 geometry\n   &lt;chr&gt;                                                      &lt;MULTIPOLYGON [°]&gt;\n 1 Lincoln     (((-110.5417 42.28652, -110.5417 42.28638, -110.5417 42.28471, -…\n 2 Fremont     (((-109.3258 42.86878, -109.3258 42.86894, -109.3261 42.86956, -…\n 3 Uinta       (((-110.5849 41.57916, -110.5837 41.57916, -110.5796 41.57917, -…\n 4 Big Horn    (((-107.5034 44.64004, -107.5029 44.64047, -107.5023 44.64078, -…\n 5 Hot Springs (((-108.1563 43.47063, -108.1563 43.45961, -108.1564 43.45961, -…\n 6 Washakie    (((-107.6841 44.1664, -107.684 44.1664, -107.684 44.1664, -107.6…\n 7 Converse    (((-105.9232 43.49501, -105.9152 43.49503, -105.9072 43.49505, -…\n 8 Sweetwater  (((-109.5651 40.99839, -109.5652 40.99839, -109.5656 40.99839, -…\n 9 Crook       (((-104.4611 44.18075, -104.4612 44.18075, -104.4643 44.18075, -…\n10 Carbon      (((-106.3227 41.38265, -106.3227 41.38245, -106.3227 41.38242, -…\n# ℹ 13 more rows\n\n\nAs you can see on the second line, the geometry type of this data is MULTIPOLYGON. In addition, the repeated MULTIPOLYGON text in the geometry column indicates that each row contains a shape of type MULTIPOLYGON. Figure 4.6 shows a map made with this data.\n\n\n\n\n\n\n\nFigure 4.6: A map of Wyoming counties\n\n\n\n\nNotice that the map is made up entirely of polygons.\nThe Dimensions\nNext, the geospatial data frame contains the data’s dimensions, or the type of geospatial data you’re working with. In the Wyoming example, it looks like Dimension: XY, meaning the data is two-dimensional, as in the case of all the geospatial data used in this chapter. There are two other dimensions (Z and M) that you’ll see much more rarely. I’ll leave them for you to investigate further.\nBounding Box\nThe penultimate element in the metadata is the bounding box, which represents the smallest area in which you can fit all of your geospatial data. For the wyoming object, it looks like this:\nBounding box:  xmin: -111.0569 ymin: 40.99475 xmax: -104.0522 ymax: 45.0059\nThe ymin value of 40.99475 and ymax value of 45.0059 represent the lowest and highest latitudes, respectively, that the state’s polygon can fit into. The x-values do the same for the longitude. Bounding boxes are calculated automatically, and typically you don’t have to worry about altering them.\nThe Coordinate Reference System\nThe last piece of metadata specifies the coordinate reference system used to project the data when it’s plotted. The challenge with representing any geospatial data is that you’re displaying information about the three-dimensional Earth on a two-dimensional map. Doing so requires choosing a coordinate reference system that determines what type of correspondence, or projection, to use when making the map.\nThe data for the Wyoming counties map includes the line Geodetic CRS: WGS 84, indicating the use of a coordinate reference system known as WGS84. To see a different projection, check out the same map using the Albers equal-area conic convenience projection. While Wyoming looked perfectly horizontal in Figure 4.6, the version in Figure 4.7 appears to be tilted.\n\n\n\n\n\n\n\nFigure 4.7: A map of Wyoming counties using the Albers equal-area conic convenience projection\n\n\n\n\nIf you’re wondering how to change projections when making maps of your own, fear not: you’ll see how to do this when we look at Madjid’s map in the next section. And if you want to know how to choose appropriate projections for your maps, check out “Using Appropriate Projections” (Section 4.3.3).\nThe geometry Column\nIn addition to the metadata, simple features data differs from traditional data frames in another respect: its geometry column. As you might have guessed from the name, this column holds the data needed to draw the maps.\nTo understand how this works, consider the connect-the-dots drawings you probably completed as a kid. As you added lines to connect one point to the next, the subject of your drawing became clearer. The geometry column is similar. It has a set of numbers, each of which corresponds to a point. If you’re using LINESTRING/MULTILINESTRING or POLYGON/MULTIPOLYGON simple features data, ggplot uses the numbers in the geometry column to draw each point and then adds lines to connect the points. If you’re using POINT/MULTIPOINT data, it draws the points but doesn’t connect them.\nOnce again, thanks to R, you never have to worry about these details or look in any depth at the geometry column.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#re-creating-the-covid-map",
    "href": "maps.html#re-creating-the-covid-map",
    "title": "4  Maps and Geospatial Data",
    "section": "Re-creating the COVID Map",
    "text": "Re-creating the COVID Map\nNow that you understand the basics of geospatial data, let’s walk through the code Madjid used to make his COVID-19 map. Shown in Figure 4-8, it makes use of the geometry types, dimensions, bounding boxes, projections, and the geometry column just discussed.\n\n\n\n\n\n\n\nFigure 4.8: Abdoul Madjid’s map of COVID-19 in the United States in 2021\n\n\n\n\nI’ve made some small modifications to the code to make the final map fit on the page. You’ll begin by loading a few packages:\n\nlibrary(tidyverse)\nlibrary(albersusa)\nlibrary(sf)\nlibrary(zoo)\nlibrary(colorspace)\n\nThe albersusa package will give you access to geospatial data. Install it as follows:\n\nremotes::install_github(\"hrbrmstr/albersusa\")\n\nYou can install all of the other packages using the standard install.packages() code. You’ll use the tidyverse to import data, manipulate it, and plot it with ggplot. The sf package will enable you to change the coordinate reference system and use an appropriate projection for the data. The zoo package has functions for calculating rolling averages, and the colorspace package gives you a color scale that highlights the data well.\nImporting the Data\nNext, you’ll import the data you need: COVID-19 rates by state over time, state populations, and geospatial information. Madjid imported each of these pieces of data separately and then merged them, and you’ll do the same.\nThe COVID-19 data comes directly from the New York Times, which publishes daily case rates by state as a CSV file on its GitHub account. To import it, enter the following:\n\ncovid_data &lt;- \n  read_csv(\"https://data.rfortherestofus.com/covid-us-states.csv\") %&gt;%\n  select(-fips)\n\nFederal Information Processing Standards (FIPS) are numeric codes used to represent states, but you’ll reference states by their names instead, so the line select(-fips) drops the fips variable.\nLooking at this data, you can see the arrival of the first COVID-19 cases in the United States in January 2020:\n\n\n# A tibble: 61,102 × 4\n   date       state      cases deaths\n   &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 2020-01-21 Washington     1      0\n 2 2020-01-22 Washington     1      0\n 3 2020-01-23 Washington     1      0\n 4 2020-01-24 Illinois       1      0\n 5 2020-01-24 Washington     1      0\n 6 2020-01-25 California     1      0\n 7 2020-01-25 Illinois       1      0\n 8 2020-01-25 Washington     1      0\n 9 2020-01-26 Arizona        1      0\n10 2020-01-26 California     2      0\n# ℹ 61,092 more rows\n\n\nMadjid’s map shows per capita rates (the rates per 100,000 people) rather than absolute rates (the rates without consideration for a state’s population). So, to re-create his maps, you also need to obtain data on each state’s population. Download this data as a CSV as follows:\n\nusa_states &lt;- \n  read_csv(\"https://data.rfortherestofus.com/population-by-state.csv\") %&gt;%\n  select(State, Pop)\n\nThis code imports the data, keeps the State and Pop (population) variables, and saves the data as an object called usa_states. Here’s what usa_states looks like:\n\n\n# A tibble: 52 × 2\n   State               Pop\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 California     39613493\n 2 Texas          29730311\n 3 Florida        21944577\n 4 New York       19299981\n 5 Pennsylvania   12804123\n 6 Illinois       12569321\n 7 Ohio           11714618\n 8 Georgia        10830007\n 9 North Carolina 10701022\n10 Michigan        9992427\n# ℹ 42 more rows\n\n\nFinally, import the geospatial data and save it as an object called usa_states_geom like so:\n\nusa_states_geom &lt;- usa_sf() %&gt;%\n  select(name) %&gt;%\n  st_transform(us_laea_proj)\n\nThe usa_sf() function from the albersusa package gives you simple features data for all US states. Conveniently, it places Alaska and Hawaii at a position and scale that make them easy to see. This data includes multiple variables, but because you need only the state names, this code keeps just the name variable.\nThe st_transform() function from the sf package changes the coordinate reference system. The one used here comes from the us_laea_proj object in the albersusa package. This is the Albers equal-area conic convenience projection you used earlier to change the appearance of the Wyoming counties map.\nCalculating Daily COVID-19 Cases\nThe covid_data data frame lists cumulative COVID-19 cases by state, but not the number of cases per day, so the next step is to calculate that number:\n\ncovid_cases &lt;- covid_data %&gt;%\n  group_by(state) %&gt;%\n  mutate(\n    pd_cases = lag(cases)\n  ) %&gt;%\n  replace_na(list(pd_cases = 0)) %&gt;%\n  mutate(\n    daily_cases = case_when(\n      cases &gt; pd_cases ~ cases - pd_cases,\n      TRUE ~ 0\n    )\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(state, date)\n\nThe group_by() function calculates totals for each state, then creates a new variable called pd_cases, which represents the number of cases in the previous day (the lag() function is used to assign data to this variable). Some days don’t have case counts for the previous day, so set this value to 0 using the replace_na() function.\nNext, this code creates a new variable called daily_cases. To set the value of this variable, use the case_when() function to create a condition: if the cases variable (which holds the cases on that day) is greater than the pd_cases variable (which holds cases from one day prior), then daily_cases is equal to cases minus pd_cases. Otherwise, you set daily_cases to be equal to 0.\nFinally, because you grouped the data by state at the beginning of the code, now you need to remove this grouping using the ungroup() function before arranging the data by state and date.\nHere’s the resulting covid_cases data frame:\n\n\n# A tibble: 61,102 × 6\n   date       state   cases deaths pd_cases daily_cases\n   &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 2020-03-13 Alabama     6      0        0           6\n 2 2020-03-14 Alabama    12      0        6           6\n 3 2020-03-15 Alabama    23      0       12          11\n 4 2020-03-16 Alabama    29      0       23           6\n 5 2020-03-17 Alabama    39      0       29          10\n 6 2020-03-18 Alabama    51      0       39          12\n 7 2020-03-19 Alabama    78      0       51          27\n 8 2020-03-20 Alabama   106      0       78          28\n 9 2020-03-21 Alabama   131      0      106          25\n10 2020-03-22 Alabama   157      0      131          26\n# ℹ 61,092 more rows\n\n\nIn the next step, you’ll make use of the new daily_cases variable.\nCalculating Incidence Rates\nYou’re not quite done calculating values. The data that Madjid used to make his map didn’t include daily case counts. Instead, it contained a five-day rolling average of cases per 100,000 people. A rolling average is the average case rate in a certain time period. Quirks of reporting (for example, not reporting on weekends but instead rolling Saturday and Sunday cases into Monday) can make the value for any single day less reliable. Using a rolling average smooths out these quirks. Generate this data as follows:\n\ncovid_cases %&gt;%\n  mutate(roll_cases = rollmean(\n    daily_cases,\n    k = 5,\n    fill = NA\n  ))\n\nThis code creates a new data frame called covid_cases_rm (where rm stands for rolling mean). The first step in its creation is to use the rollmean() function from the zoo package to create a roll_cases variable, which holds the average number of cases in the five-day period surrounding a single date. The k argument is the number of days for which you want to calculate the rolling average (5, in this case), and the fill argument determines what happens in cases like the first day, where you can’t calculate a five-day rolling mean because there are no days prior to this day (Madjid set these values to NA).\nAfter calculating roll_cases, you need to calculate per capita case rates. To do this, you need population data, so join the population data from the usa_states data frame with the covid_cases data like so:\n\ncovid_cases_rm &lt;- covid_cases %&gt;%\n  mutate(roll_cases = rollmean(\n    daily_cases,\n    k = 5,\n    fill = NA\n  )) %&gt;%\n  left_join(\n    usa_states,\n    by = c(\"state\" = \"State\")\n  ) %&gt;%\n  drop_na(Pop)\n\nTo drop rows with missing population data, you call the drop_na() function with the Pop variable as an argument. In practice, this removes several US territories (American Samoa, Guam, the Northern Mariana Islands, and the Virgin Islands).\nNext, you create a per capita case rate variable called incidence_rate by multiplying the roll_cases variable by 100,000 and then dividing it by the population of each state:\n\ncovid_cases_rm &lt;- covid_cases_rm %&gt;%\n  mutate(incidence_rate = 10^5 * roll_cases / Pop) %&gt;%\n  mutate(\n    incidence_rate = cut(\n      incidence_rate,\n      breaks = c(seq(0, 50, 5), Inf),\n      include.lowest = TRUE\n    ) %&gt;%\n      factor(labels = paste0(\"&gt;\", seq(0, 50, 5)))\n  )\n\nRather than keeping raw values (for example, on June 29, 2021, Florida had a rate of 57.77737 cases per 100,000 people), you use the cut() function to convert the values into categories: values of &gt;0 (greater than zero), values of &gt;5 (greater than five), and values of &gt;50 (greater than 50).\nThe last step is to filter the data so it includes only 2021 data (the only year depicted in Madjid’s map) and then select just the variables (state, date, and incidence_rate) you’ll need to create the map:\nHere’s the final covid_cases_rm data frame:\n\n\n# A tibble: 18,980 × 3\n   state   date       incidence_rate\n   &lt;chr&gt;   &lt;date&gt;     &lt;fct&gt;         \n 1 Alabama 2021-01-01 &gt;50           \n 2 Alabama 2021-01-02 &gt;50           \n 3 Alabama 2021-01-03 &gt;50           \n 4 Alabama 2021-01-04 &gt;50           \n 5 Alabama 2021-01-05 &gt;50           \n 6 Alabama 2021-01-06 &gt;50           \n 7 Alabama 2021-01-07 &gt;50           \n 8 Alabama 2021-01-08 &gt;50           \n 9 Alabama 2021-01-09 &gt;50           \n10 Alabama 2021-01-10 &gt;50           \n# ℹ 18,970 more rows\n\n\nYou now have a data frame that you can combine with your geospatial data.\nAdding Geospatial Data\nYou’ve used two of the three data sources (COVID-19 case data and state population data) to create the covid_cases_rm data frame you’ll need to make the map. Now it’s time to use the third data source: the geospatial data you saved as usa_states_geom. Simple features data allows you to merge regular data frames and geospatial data (another point in its favor):\n\nusa_states_geom %&gt;%\n  left_join(covid_cases_rm, by = c(\"name\" = \"state\"))\n\nThis code merges the covid_cases_rm data frame into the geospatial data, matching the name variable from usa_states_geom to the state variable in covid_cases_rm.\nNext, you create a new variable called fancy_date to format the date nicely (for example, Jan. 01 instead of 2021-01-01):\n\nusa_states_geom_covid &lt;- usa_states_geom %&gt;%\n  left_join(covid_cases_rm, by = c(\"name\" = \"state\")) %&gt;%\n  mutate(fancy_date = fct_inorder(format(date, \"%b. %d\"))) %&gt;%\n  relocate(fancy_date, .before = incidence_rate)\n\nThe format() function does the formatting, while the fct_inorder() function makes the fancy_date variable sort data by date (rather than, say, alphabetically, which would put August before January). Last, the relocate() function puts the fancy_date column next to the date column.\nSave this data frame as usa_states_geom_covid and take a look at the result:\n\n\nSimple feature collection with 18615 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -2100000 ymin: -2500000 xmax: 2516374 ymax: 732103.3\nProjected CRS: +proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\nFirst 10 features:\n      name       date fancy_date incidence_rate                       geometry\n1  Arizona 2021-01-01    Jan. 01            &gt;50 MULTIPOLYGON (((-1111066 -8...\n2  Arizona 2021-01-02    Jan. 02            &gt;50 MULTIPOLYGON (((-1111066 -8...\n3  Arizona 2021-01-03    Jan. 03            &gt;50 MULTIPOLYGON (((-1111066 -8...\n4  Arizona 2021-01-04    Jan. 04            &gt;50 MULTIPOLYGON (((-1111066 -8...\n5  Arizona 2021-01-05    Jan. 05            &gt;50 MULTIPOLYGON (((-1111066 -8...\n6  Arizona 2021-01-06    Jan. 06            &gt;50 MULTIPOLYGON (((-1111066 -8...\n7  Arizona 2021-01-07    Jan. 07            &gt;50 MULTIPOLYGON (((-1111066 -8...\n8  Arizona 2021-01-08    Jan. 08            &gt;50 MULTIPOLYGON (((-1111066 -8...\n9  Arizona 2021-01-09    Jan. 09            &gt;50 MULTIPOLYGON (((-1111066 -8...\n10 Arizona 2021-01-10    Jan. 10            &gt;50 MULTIPOLYGON (((-1111066 -8...\n\n\nYou can see the metadata and geometry columns discussed earlier in the chapter.\nMaking the Map\nIt took a lot of work to end up with the surprisingly simple usa_states_geom_covid data frame. While the data may be simple, the code Madjid used to make his map is quite complex. This section walks you through it in pieces.\nThe final map is actually multiple maps, one for each day in 2021. Combining 365 days makes for a large final product, so instead of showing the code for every single day, filter the usa_states_geom_covid to show just the first six days in January:\n\nusa_states_geom_covid_six_days &lt;- usa_states_geom_covid %&gt;%\n  filter(date &lt;= as.Date(\"2021-01-06\"))\n\nSave the result as a data frame called usa_states_geom_covid_six_days. Here’s what this data looks like:\n\n\nSimple feature collection with 306 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -2100000 ymin: -2500000 xmax: 2516374 ymax: 732103.3\nProjected CRS: +proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\nFirst 10 features:\n       name       date fancy_date incidence_rate                       geometry\n1   Arizona 2021-01-01    Jan. 01            &gt;50 MULTIPOLYGON (((-1111066 -8...\n2   Arizona 2021-01-02    Jan. 02            &gt;50 MULTIPOLYGON (((-1111066 -8...\n3   Arizona 2021-01-03    Jan. 03            &gt;50 MULTIPOLYGON (((-1111066 -8...\n4   Arizona 2021-01-04    Jan. 04            &gt;50 MULTIPOLYGON (((-1111066 -8...\n5   Arizona 2021-01-05    Jan. 05            &gt;50 MULTIPOLYGON (((-1111066 -8...\n6   Arizona 2021-01-06    Jan. 06            &gt;50 MULTIPOLYGON (((-1111066 -8...\n7  Arkansas 2021-01-01    Jan. 01            &gt;50 MULTIPOLYGON (((557903.1 -1...\n8  Arkansas 2021-01-02    Jan. 02            &gt;50 MULTIPOLYGON (((557903.1 -1...\n9  Arkansas 2021-01-03    Jan. 03            &gt;50 MULTIPOLYGON (((557903.1 -1...\n10 Arkansas 2021-01-04    Jan. 04            &gt;50 MULTIPOLYGON (((557903.1 -1...\n\n\nMadjid’s map is giant, as it includes all 365 days. The size of a few elements have been changed so that they fit in this book.\nGenerating the Basic Map\nWith your six days of data, you’re ready to make some maps. Madjid’s map-making code has two main parts: generating the basic map, then tweaking its appearance. First, you’ll revisit the three lines of code used to make the Wyoming maps, with some adornments to improve the quality of the visualization:\n\nusa_states_geom_covid_six_days %&gt;%\n  ggplot() +\n  geom_sf(\n    aes(fill = incidence_rate),\n    size = .05,\n    color = \"grey55\"\n  ) +\n  facet_wrap(\n    vars(fancy_date),\n    strip.position = \"bottom\"\n  )\n\nThe geom_sf() function plots the geospatial data, modifying a couple of arguments: size = .05 makes the state borders less prominent and color = \"grey55\" sets them to a medium-gray color. Then, the facet_wrap() function is used for the faceting (that is, to make one map for each day). The vars(fancy _date) code specifies that the fancy_date variable should be used for the faceted maps, and strip.position = \"bottom\" moves the labels Jan. 01, Jan. 02, and so on to the bottom of the maps. Figure Figure 4.9 shows the result.\n\n\n\n\n\n\n\nFigure 4.9: A map showing the incidence rate of COVID-19 for the first six days of 2021\n\n\n\n\nHaving generated the basic map, let’s now make it look good.\nApplying Data Visualization Principles\nFrom now on, all of the code that Madjid uses is to improve the appearance of the maps. Many of the tweaks shown here should be familiar if you’ve read Chapter 2, highlighting a benefit of making maps with ggplot: you can apply the same data visualization principles you learned about when making charts.\n\nusa_states_geom_covid_six_days %&gt;%\n  ggplot() +\n  geom_sf(\n    aes(fill = incidence_rate),\n    size = .05,\n    color = \"transparent\"\n  ) +\n  facet_wrap(\n    vars(fancy_date),\n    strip.position = \"bottom\"\n  ) +\n  scale_fill_discrete_sequential(\n    palette = \"Rocket\",\n    name = \"COVID-19 INCIDENCE RATE\",\n    guide = guide_legend(\n      title.position = \"top\",\n      title.hjust = .5,\n      title.theme = element_text(\n        family = \"Times New Roman\",\n        size = rel(9),\n        margin = margin(\n          b = .1,\n          unit = \"cm\"\n        )\n      ),\n      nrow = 1,\n      keyheight = unit(.3, \"cm\"),\n      keywidth = unit(.3, \"cm\"),\n      label.theme = element_text(\n        family = \"Times New Roman\",\n        size = rel(6),\n        margin = margin(\n          r = 5,\n          unit = \"pt\"\n        )\n      )\n    )\n  ) +\n  labs(\n    title = \"2021 · A pandemic year\",\n    caption = \"Incidence rates are calculated for 100,000 people in each state.\n                  Inspired from a graphic in the DIE ZEIT newspaper of November 18, 2021.\n                  Data from NY Times · Tidytuesday Week-1 2022 · Abdoul ISSA BIDA.\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(\n      family = \"Times New Roman\",\n      color = \"#111111\"\n    ),\n    plot.title = element_text(\n      size = rel(2.5),\n      face = \"bold\",\n      hjust = 0.5,\n      margin = margin(\n        t = .25,\n        b = .25,\n        unit = \"cm\"\n      )\n    ),\n    plot.caption = element_text(\n      hjust = .5,\n      face = \"bold\",\n      margin = margin(\n        t = .25,\n        b = .25,\n        unit = \"cm\"\n      )\n    ),\n    strip.text = element_text(\n      size = rel(0.75),\n      face = \"bold\"\n    ),\n    legend.position = \"top\",\n    legend.box.spacing = unit(.25, \"cm\"),\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    plot.margin = margin(\n      t = .25,\n      r = .25,\n      b = .25,\n      l = .25,\n      unit = \"cm\"\n    ),\n    plot.background = element_rect(\n      fill = \"#e5e4e2\",\n      color = NA\n    )\n  )\n\nThe scale_fill_discrete_sequential() function, from the colorspace package, sets the color scale. This code uses the rocket palette (the same palette that Cédric Scherer and Georgios Karamanis used in Chapter 2) and changes the legend title to “COVID-19 INCIDENCE RATE.” The guide_legend() function adjusts the position, alignment, and text properties of the title. The code then puts the colored squares in one row, adjusts their height and width, and tweaks the text properties of the labels (&gt;0, &gt;5, and so on).\nNext, the labs() function adds a title and caption. Following theme_minimal(), the theme() function makes some design tweaks, including setting the font and text color; making the title and caption bold; and adjusting their size, alignment, and margins. The code then adjusts the size of the strip text (Jan. 01, Jan. 02, and so on) and makes it bold, puts the legend at the top of the maps, and adds a bit of spacing around it. Grid lines, as well as the longitude and latitude lines, are removed, and then the entire visualization gets a bit of padding and a light gray background.\nThere you have it! Figure 4.10 shows the re-creation of his COVID-19 map.\n\n\n\n\n\n\n\nFigure 4.10: The re-creation of Abdoul Madjid’s map\n\n\n\n\nFrom data import and data cleaning to analysis and visualization, you’ve seen how Madjid made a beautiful map in R.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#making-your-own-maps",
    "href": "maps.html#making-your-own-maps",
    "title": "4  Maps and Geospatial Data",
    "section": "Making Your Own Maps",
    "text": "Making Your Own Maps\nYou may now be wondering, Okay, great, but how do I actually make my own maps? In this section you’ll learn where you can find geospatial data, how to choose a projection, and how to prepare the data for mapping.\nThere are two ways to access simple features geospatial data. The first is to import raw data, and the second is to access it with R functions.\nImporting Raw Data\nGeospatial data can come in various formats. While ESRI shapefiles (with the .shp extension) are the most common, you might also encounter GeoJSON files (.geojson) like the ones we used in the Wyoming example at the beginning of this chapter, KML files (.kml), and others. Chapter 8 of Geocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow discusses this range of formats.\nThe good news is that a single function can read pretty much any type of geospatial data: read_sf() from the sf package. Say you’ve downloaded geospatial data about US state boundaries from the website &lt;geojson.xyz&gt; in GeoJSON format, then saved it in the data folder as states.geojson. To import this data, use the read_sf() function like so:\n\nus_states &lt;- read_sf(dsn = \"https://data.rfortherestofus.com/states.geojson\")\n\nThe dsn argument (which stands for data source name) tells read_sf() where to find the file. You save the data as the object us_states.\nAccessing Geospatial Data Using R Functions\nSometimes you’ll have to work with raw data in this way, but not always. That’s because certain R packages provide functions for accessing geospatial data. Madjid used the usa_sf() function from the albersusa package to acquire his data. Another package for accessing geospatial data related to the United States, tigris, has a number of well-named functions for different types of data. For example, load the tigris package and run the states() function like so:\n\nlibrary(tigris)\n\nstates_tigris &lt;- states(\n  cb = TRUE,\n  resolution = \"20m\",\n  progress_bar = FALSE\n)\n\nThe cb = TRUE argument opts out of using the most detailed shapefile and sets the resolution to a more manageable 20m (1:20 million). Without these changes, the resulting shapefile would be large and slow to work with. Setting progress_bar = FALSE hides the messages that tigris generates as it loads data. The result is saved as states_tigris. The tigris package has functions to get geospatial data about counties, census tracts, roads, and more.\nIf you’re looking for data outside the United States, the rnaturalearth package provides functions for importing geospatial data from across the world. For example, use ne_countries() to retrieve geospatial data about various countries:\n\nlibrary(rnaturalearth)\n\nafrica_countries &lt;- ne_countries(\n  returnclass = \"sf\",\n  continent = \"Africa\"\n)\n\nThis code uses two arguments: returnclass = \"sf\" to get data in simple features format, and continent = \"Africa\" to get only countries on the African continent. If you save the result to an object called africa_countries, you can plot the data on a map as follows:\n\nafrica_countries %&gt;%\n  ggplot() +\n  geom_sf()\n\nFigure 4.11 shows the resulting map.\n\n\n\n\n\n\n\nFigure 4.11: A map of Africa made with data from the rnaturalearth package\n\n\n\n\nIf you can’t find an appropriate package, you can always fall back on using read_sf() from the sf package.\nUsing Appropriate Projections\nOnce you have access to geospatial data, you need to decide which projection to use. If you’re looking for a simple answer to this question, you’ll be disappointed. As Geocomputation with R puts it, “The question of which CRS [to use] is tricky, and there is rarely a ‘right’ answer.”\nIf you’re overwhelmed by the task of choosing a projection, the crsuggest package from Kyle Walker can give you ideas. Its suggest_top_crs() function returns a coordinate reference system that is well suited for your data. Load crsuggest and try it out on your africa_countries data:\n\nlibrary(crsuggest)\n\nafrica_countries %&gt;%\n  suggest_top_crs()\n\nThe suggest_top_crs() function should return projection number 28232. Pass this value to the st_transform() function to change the projection before you plot:\n\nafrica_countries %&gt;%\n  st_transform(28232) %&gt;%\n  ggplot() +\n  geom_sf()\n\nWhen run, this code generates the map in Figure 4.12.\n\n\n\n\n\n\n\nFigure 4.12: A map of Africa made with projection number 28232\n\n\n\n\nAs you can see, you’ve successfully mapped Africa with a different projection.\nWrangling Your Geospatial Data\nThe ability to merge traditional data frames with geospatial data is a huge benefit of working with simple features data. Remember that for his COVID-19 map, Madjid analyzed traditional data frames before merging them with geospatial data. But because simple features data acts just like traditional data frames, you can just as easily apply the data-wrangling and analysis functions from the tidyverse directly to a simple features object. To see how this works, revisit the africa_countries simple features data and select two variables (name and pop_est) to see the name and population of the countries:\n\nafrica_countries %&gt;%\n  select(name, pop_est)\n\nThe output looks like the following:\n\n\nSimple feature collection with 51 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -17.62504 ymin: -34.81917 xmax: 51.13387 ymax: 37.34999\nGeodetic CRS:  WGS 84\nFirst 10 features:\n              name  pop_est                       geometry\n2         Tanzania 58005463 MULTIPOLYGON (((33.90371 -0...\n3        W. Sahara   603253 MULTIPOLYGON (((-8.66559 27...\n12 Dem. Rep. Congo 86790567 MULTIPOLYGON (((29.34 -4.49...\n13         Somalia 10192317 MULTIPOLYGON (((41.58513 -1...\n14           Kenya 52573973 MULTIPOLYGON (((39.20222 -4...\n15           Sudan 42813238 MULTIPOLYGON (((24.56737 8....\n16            Chad 15946876 MULTIPOLYGON (((23.83766 19...\n26    South Africa 58558270 MULTIPOLYGON (((16.34498 -2...\n27         Lesotho  2125268 MULTIPOLYGON (((28.97826 -2...\n49        Zimbabwe 14645468 MULTIPOLYGON (((31.19141 -2...\n\n\nSay you want to make a map showing which African countries have populations larger than 20 million. First, you’ll need to calculate this value for each country. To do so, use the mutate() and if_else() functions, which will return TRUE if a country’s population is over 20 million and FALSE otherwise, and then store the result in a variable called population_above_20_million:\n\nafrica_countries %&gt;%\n  select(name, pop_est) %&gt;%\n  mutate(population_above_20_million = if_else(pop_est &gt; 20000000, TRUE, FALSE))\n\nYou can then take this code and pipe it into ggplot, setting the fill aesthetic property to be equal to population_above_20_million:\n\nafrica_countries %&gt;%\n  select(name, pop_est) %&gt;%\n  mutate(population_above_20_million = if_else(pop_est &gt; 20000000, TRUE, FALSE)) %&gt;%\n  ggplot(aes(fill = population_above_20_million)) +\n  geom_sf()\n\nThis code generates the map shown in Figure 4-13.\n\n\n\n\n\n\n\nFigure 4.13: A map of Africa that highlights countries with populations above 20 million people\n\n\n\n\nThis is a basic example of the data wrangling and analysis you can perform on simple features data. The larger lesson is this: any skill you’ve developed for working with data in R will serve you well when working with geospatial data.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#summary",
    "href": "maps.html#summary",
    "title": "4  Maps and Geospatial Data",
    "section": "Summary",
    "text": "Summary\nIn this short romp through the world of mapmaking in R, you learned the basics of simple features geospatial data, reviewed how Abdoul Madjid applied this knowledge to make his map, explored how to get your own geospatial data, and saw how to project it appropriately to make your own maps.\nR may very well be the best tool for making maps. It also lets you use the skills you’ve developed for working with traditional data frames and the ggplot code to make your visualizations look great. After all, Madjid isn’t a GIS expert, but he combined a basic understanding of geospatial data, fundamental R skills, and knowledge of data visualization principles to make a beautiful map. Now it’s your turn to do the same.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "maps.html#additional-resources",
    "href": "maps.html#additional-resources",
    "title": "4  Maps and Geospatial Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nKieran Healy, “Draw Maps,” in Data Visualization: A Practical Introduction (Princeton, NJ: Princeton University Press, 2018), https://socviz.co.\nAndrew Heiss, “Lessons on Space from Data Visualization: Use R, ggplot2, and the Principles of Graphic Design to Create Beautiful and Truthful Visualizations of Data,” online course, last updated July 11, 2022, https://datavizs22.classes.andrewheiss.com/content/12-content/.\nRobin Lovelace, Jakub Nowosad, and Jannes Muenchow, Geocomputation with R (Boca Raton, FL: CRC Press, 2019), https://r.geocompx.org.\nKyle Walker, Analyzing US Census Data: Methods, Maps, and Models in R (Boca Raton, FL: CRC Press, 2023). https://walker-data.com/census-r/",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Maps and Geospatial Data</span>"
    ]
  },
  {
    "objectID": "tables.html",
    "href": "tables.html",
    "title": "5  Designing Effective Tables",
    "section": "",
    "text": "Creating a Data Frame\nYou will begin by creating a data frame that you can use to make tables throughout this chapter. First, load the packages you need (the tidyverse for general data manipulation functions, gapminder for the data you’ll use, gt to make the tables, and gtExtras to do some table formatting):\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(gt)\nlibrary(gtExtras)\nAs you saw in Chapter 2, the gapminder package provides country-level demographic statistics. To make a data frame for your table, you’ll use just a few countries (the first four, in alphabetical order: Afghanistan, Albania, Algeria, and Angola) and three years (1952, 1972, and 1992). The gapminder data has many years, but these will suffice to demonstrate table-making principles. The following code creates a data frame called gdp:\ngdp &lt;- gapminder %&gt;%\n  filter(country %in% c(\"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\")) %&gt;%\n  select(country, year, gdpPercap) %&gt;%\n  mutate(country = as.character(country)) %&gt;%\n  pivot_wider(\n    id_cols = country,\n    names_from = year,\n    values_from = gdpPercap\n  ) %&gt;%\n  select(country, `1952`, `1972`, `1992`) %&gt;%\n  rename(Country = country)\nHere’s what gdp looks like:\n# A tibble: 4 × 4\n  Country     `1952` `1972` `1992`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan   779.   740.   649.\n2 Albania      1601.  3313.  2497.\n3 Algeria      2449.  4183.  5023.\n4 Angola       3521.  5473.  2628.\nNow that you have some data, you’ll use it to make a table.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designing Effective Tables</span>"
    ]
  },
  {
    "objectID": "tables.html#table-design-principles",
    "href": "tables.html#table-design-principles",
    "title": "5  Designing Effective Tables",
    "section": "Table Design Principles",
    "text": "Table Design Principles\nUnsurprisingly, the principles of good table design are similar to those for data visualization more generally. This section covers six of the most important ones.\nMinimize Clutter\nYou can minimize clutter in your tables by removing unnecessary elements. For example, one common source of table clutter is grid lines, as shown in Figure 5.1.\n\n\n\n\n\n\n\nFigure 5.1: A table with grid lines everywhere can be distracting.\n\n\n\n\nHaving grid lines around every single cell in your table is unnecessary and distracts from the goal of communicating clearly. A table with minimal or even no grid lines (Figure 5.2) is a much more effective communication tool.\n\n\n\n\n\n\n\nFigure 5.2: A table with only horizontal grid lines is more effective.\n\n\n\n\nI mentioned that gt uses good table design principles by default, and this is a great example. The second table, with minimal grid lines, requires just two lines of code—piping the gdp data into the gt() function, which creates a table:\n\ngdp %&gt;%\n  gt()\n\nTo add grid lines to every part of the example, you’d have to add more code. Here, the code that follows the gt() function adds grid lines:\n\ngdp %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = cell_borders(\n      side = \"all\",\n      color = \"black\",\n      weight = px(1),\n      style = \"solid\"\n    ),\n    locations = list(\n      cells_body(\n        everything()\n      ),\n      cells_column_labels(\n        everything()\n      )\n    )\n  ) %&gt;%\n  opt_table_lines(extent = \"none\")\n\nSince I don’t recommend taking this approach, I won’t walk you through this code. However, if you wanted to remove additional grid lines, you could do so like this:\n\ngdp %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = cell_borders(color = \"transparent\"),\n    locations = cells_body()\n  )\n\nThe tab_style() function uses a two-step approach. First, it identifies the style to modify (in this case, the borders), then it specifies where to apply these modifications. Here, tab_style() tells R to modify the borders using the cell_borders() function, making the borders transparent, and to apply this transformation to the cells_body() location (versus, say, the cells_column_labels() for only the first row).\n\n\n\n\n\n\nTo see all options, check out the list of so-called helper functions on the gt package documentation website at https://gt.rstudio.com/reference/index.html#helper-functions.\n\n\n\nRunning this code outputs a table with no grid lines at all in the body (Figure 5.3).\n\n\n\n\n\n\n\nFigure 5.3: A clean-looking table with grid lines only on the header row and the bottom\n\n\n\n\nSave this table as an object called table_no_gridlines so that you can add to it later.\nDifferentiate the Header from the Body\nWhile reducing clutter is an important goal, going too far can have negative consequences. A table with no grid lines at all can make it hard to differentiate between the header row and the table body. Consider Figure 5.4, for example.\n\n\n\n\n\n\n\nFigure 5.4: An unclear table with all grid lines removed\n\n\n\n\nBy making the header row bold, you can make it stand out better:\n\ntable_no_gridlines %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )\n\nStarting with the table_no_gridlines object, this code applies formatting with the tab_style() function in two steps. First, it specifies that it wants to alter the text style by using the cell_text() function to set the weight to bold. Second, it sets the location for this transformation to the header row using the cells_column_labels() function. Figure 5.5 shows what the table looks like with its header row bolded.\n\n\n\n\n\n\n\nFigure 5.5: Making the header row more obvious using bold\n\n\n\n\nSave this table as table_bold_header in order to add further formatting.\n\ntable_bold_header &lt;- table_no_gridlines %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )\n\nAlign Appropriately\nA third principle of high-quality table design is appropriate alignment. Specifically, numbers in tables should be right-aligned. Tom Mock explains that left-aligning or center-aligning numbers “impairs the ability to clearly compare numbers and decimal places. Right alignment lets you align decimal places and numbers for easy parsing.”\nLet’s look at this principle in action. In Figure 5.6, the 1952 column is left-aligned, the 1972 column is center-aligned, and the 1992 column is right-aligned.\n\n\n\n\n\n\n\nFigure 5.6: Comparing numerical data aligned to the left (1952), center (1972), and right (1992)\n\n\n\n\nYou can see how much easier it is to compare the values in the 1992 column than those in the other two columns. In both the 1952 and 1972 columns, it’s challenging to compare the values because the numbers in the same position (the tens place, for example) aren’t aligned vertically. In the 1992 column, however, the number in the tens place in Afghanistan (4) aligns with the number in the tens place in Albania (9) and all other countries, making it much easier to scan the table.\nAs with other tables, you actually have to override the defaults to get the gt package to misalign the columns, as demonstrated in the following code:\n\ntable_bold_header %&gt;%\n  cols_align(\n    align = \"left\",\n    columns = 2\n  ) %&gt;%\n  cols_align(\n    align = \"center\",\n    columns = 3\n  ) %&gt;%\n  cols_align(\n    align = \"right\",\n    columns = 4\n  )\n\nBy default, gt will right-align numeric values. Don’t change anything, and you’ll be golden.\nRight alignment is best practice for numeric columns, but for text columns, use left alignment. As Jon Schwabish points out in his article “Ten Guidelines for Better Tables” in the Journal of Benefit-Cost Analysis, it’s much easier to read longer text cells when they are left-aligned. To see the benefit of left-aligning text, add a country with a long name to your table. I’ve added Bosnia and Herzegovina and saved this as a data frame called gdp_with_bosnia. You’ll see that I’m using nearly the same code I used previously to create the gdp data frame:\nHere’s what the gdp_with_bosnia data frame looks like:\n\ngdp_with_bosnia\n\n# A tibble: 5 × 4\n  Country                `1952` `1972` `1992`\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan              779.   740.   649.\n2 Albania                 1601.  3313.  2497.\n3 Algeria                 2449.  4183.  5023.\n4 Angola                  3521.  5473.  2628.\n5 Bosnia and Herzegovina   974.  2860.  2547.\n\n\nNow take the gdp_with_bosnia data frame and create a table with the Country column center-aligned. In the table in Figure 5.7, it’s hard to scan the country names, and that center-aligned column just looks a bit weird.\n\n\n\n\n\n\n\nFigure 5.7: Center-aligned text can be hard to read, especially when it includes longer values.\n\n\n\n\nThis is another example where you have to change the gt defaults to mess things up. In addition to right-aligning numeric columns by default, gt left-aligns character columns. As long as you don’t touch anything, you’ll get the alignment you’re looking for.\nIf you ever do want to override the default alignments, you can use the cols_align() function. For example, here’s how to make the table with center-aligned country names:\n\ngdp_with_bosnia %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = cell_borders(color = \"transparent\"),\n    locations = cells_body()\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) %&gt;%\n  cols_align(\n    columns = \"Country\",\n    align = \"center\"\n  )\n\nThe columns argument tells gt which columns to align, and the align argument selects the alignment (left, right, or center).\nUse the Correct Level of Precision\nIn all of the tables you’ve made so far, you’ve used the data exactly as it came to you. The data in the numeric columns, for example, extends to four decimal places—almost certainly too many. Having more decimal places makes a table harder to read, so you should always strike a balance between what Jon Schwabish describes as “necessary precision and a clean, spare table.”\nHere’s a good rule of thumb: if adding more decimal places would change some action, keep them; otherwise, take them out. In my experience, people tend to leave too many decimal places in, putting too much importance on a very high degree of accuracy (and, in the process, reducing the legibility of their tables).\nIn the GDP table, you can use the fmt_currency() function to format the numeric values:\n\ntable_bold_header %&gt;%\n  fmt_currency(\n    columns = c(`1952`, `1972`, `1992`),\n    decimals = 0\n  )\n\nThe gt package has a whole series of functions for formatting values in tables, all of which start with fmt_. This code applies fmt_currency() to the 1952, 1972, and 1992 columns, then uses the decimals argument to tell fmt_currency() to format the values with zero decimal places. After all, the difference between a GDP of $779.4453 and $779 is unlikely to lead to different decisions.\nThis produces values formatted as dollars. The fmt_currency() function automatically adds a thousands-place comma to make the values even easier to read (Figure 5.8).\n\n\n\n\n\n\n\nFigure 5.8: Rounding dollar amounts to whole numbers and adding dollar signs can simplify data.\n\n\n\n\nSave your table for reuse as table_whole_numbers.\nUse Color Intentionally\nSo far, our table hasn’t used any color. We’ll add some now to highlight outlier values. Especially for readers who want to scan your table, highlighting outliers with color can help significantly. Let’s make the highest value in the year 1952 a different color. To do this, we again use the tab_style() function:\n\ntable_whole_numbers %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"orange\",\n      weight = \"bold\"\n    ),\n    locations = cells_body(\n      columns = `1952`,\n      rows = `1952` == max(`1952`)\n    )\n  )\n\nThis function uses cell_text() to change the color of the text to orange and make it bold. Within the cells_body() function, the locations() function specifies the columns and rows to which the changes will apply. The columns argument is simply set to the year whose values are being changed, but setting the rows requires a more complicated formula. The code rows =1952== max(1952) applies the text transformation to rows whose value is equal to the maximum value in that year.\nRepeating this code for the 1972 and 1992 columns generates the result shown in Figure 5.9 (which represents the orange values in grayscale for print purposes).\n\n\n\n\n\n\n\nFigure 5.9: Using color to highlight important values, such as the largest number in each year\n\n\n\n\nThe gt package makes it straightforward to add color to highlight outlier values.\nAdd a Data Visualization Where Appropriate\nAdding color to highlight outliers is one way to help guide the reader’s attention. Another way is to incorporate graphs into tables. Tom Mock developed an add-on package for gt called gtExtras that makes it possible to do just this. For example, say you want to show how the GDP of each country changes over time. To do that, you can add a new column that visualizes this trend using a sparkline (essentially, a simple line chart):\n\ngdp_with_trend &lt;- gdp %&gt;%\n  group_by(Country) %&gt;%\n  mutate(Trend = list(c(`1952`, `1972`, `1992`))) %&gt;%\n  ungroup()\n\nThe gt_plt_sparkline() function requires you to provide the values needed to make the sparkline in a single column. To accomplish this, the code creates a variable called Trend, using group_by() and mutate(), to hold a list of the values for each country. For Afghanistan, for example, Trend would contain 779.4453145, 739.9811058, and 649.3413952. Save this data as an object called gdp_with_trend.\nNow you create your table as before but add the gt_plt_sparkline() function to the end of the code. Within this function, specify which column to use to create the sparkline (Trend) as follows:\n\ngdp_with_trend %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = cell_borders(color = \"transparent\"),\n    locations = cells_body()\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) %&gt;%\n  fmt_currency(\n    columns = c(`1952`, `1972`, `1992`),\n    decimals = 0\n  ) %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"orange\",\n      weight = \"bold\"\n    ),\n    locations = cells_body(\n      columns = `1952`,\n      rows = `1952` == max(`1952`)\n    )\n  ) %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"orange\",\n      weight = \"bold\"\n    ),\n    locations = cells_body(\n      columns = `1972`,\n      rows = `1972` == max(`1972`)\n    )\n  ) %&gt;%\n  tab_style(\n    style = cell_text(\n      color = \"orange\",\n      weight = \"bold\"\n    ),\n    locations = cells_body(\n      columns = `1992`,\n      rows = `1992` == max(`1992`)\n    )\n  ) %&gt;%\n  gt_plt_sparkline(\n    column = Trend,\n    label = FALSE,\n    palette = c(\"black\", \"transparent\", \"transparent\", \"transparent\", \"transparent\")\n  )\n\nSetting label = FALSE removes text labels that gt_plt_sparkline() adds by default, then adds a palette argument to make the sparkline black and all other elements of it transparent. (By default, the function will make different parts of the sparkline different colors.) The stripped-down sparkline in Figure 5.10 allows the reader to see the trend for each country at a glance.\n\n\n\n\n\n\n\nFigure 5.10: A table with sparklines can show changes in data over time.\n\n\n\n\nThe gtExtras package can do much more than merely create sparklines. Its set of theme functions allows you to make your tables look like those published by FiveThirtyEight, the New York Times, the Guardian, and other news outlets.\nAs an example, try removing the formatting you’ve applied so far and instead use the gt_theme_538() function to style the table. Then take a look at tables on the FiveThirtyEight website. You should see similarities to the one in Figure 5.11.\n\n\n\n\n\n\n\nFigure 5.11: A table redone in the FiveThirtyEight style\n\n\n\n\nAdd-on packages like gtExtras are common in the table-making landscape. If you’re working with the reactable package to make interactive tables, for example, you can also use the reactablefmtr to add interactive sparklines, themes, and more. You’ll learn more about making interactive tables in Chapter 9.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designing Effective Tables</span>"
    ]
  },
  {
    "objectID": "tables.html#summary",
    "href": "tables.html#summary",
    "title": "5  Designing Effective Tables",
    "section": "Summary",
    "text": "Summary\nMany of the tweaks you made to your table in this chapter are quite subtle. Changes like removing excess grid lines, bolding header text, right-aligning numeric values, and adjusting the level of precision can often go unnoticed, but if you skip them, your table will be far less effective. The final product isn’t flashy, but it does communicate clearly.\nYou used the gt package to make your high-quality table, and as you’ve repeatedly seen, this package has good defaults built in. Often, you don’t need to change much in your code to make effective tables. But no matter which package you use, it’s essential to treat tables as worthy of just as much thought as other kinds of data visualization.\nIn Chapter 6, you’ll learn how to create reports using R Markdown, which can integrate your tables directly into the final document. What’s better than using just a few lines of code to make publication-ready tables?",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designing Effective Tables</span>"
    ]
  },
  {
    "objectID": "tables.html#additional-resources",
    "href": "tables.html#additional-resources",
    "title": "5  Designing Effective Tables",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThomas Mock, “10+ Guidelines for Better Tables in R,” The MockUp, September 4, 2020, https://themockup.blog/posts/2020-09-04-10-table-rules-in-r/.\nAlbert Rapp, “Creating Beautiful Tables in R with {gt},” November 27, 2022, https://gt.albert-rapp.de.\nJon Schwabish, “Ten Guidelines for Better Tables,” Journal of Benefit-Cost Analysis 11, no. 2 (2020), https://doi.org/10.1017/bca.2020.11.",
    "crumbs": [
      "Part I: Visualizations",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Designing Effective Tables</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html",
    "href": "rmarkdown.html",
    "title": "6  R Markdown Reports",
    "section": "",
    "text": "Creating an R Markdown Document\nTo create an R Markdown document in RStudio, go to File &gt; New File &gt; R Markdown. Choose a title, author, and date, as well as your default output format (HTML, PDF, or Word). These values can be changed later. Click OK, and RStudio will create an R Markdown document with some placeholder content, as shown in Figure 6.1.\nFigure 6.1: The placeholder content in a new R Markdown document\nThe Knit menu at the top of RStudio converts an R Markdown document to the format you selected when creating it. In this example, the output format is set to be Word, so RStudio will create a Word document when you knit.\nDelete the document’s placeholder content. In the next section, you’ll replace it with your own.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#document-structure",
    "href": "rmarkdown.html#document-structure",
    "title": "6  R Markdown Reports",
    "section": "Document Structure",
    "text": "Document Structure\nTo explore the structure of an R Markdown document, you’ll create a report about penguins using data from the palmerpenguins package introduced in Chapter 3. I’ve separated the data by year, and you’ll use just the 2007 data. Figure 6.2 shows the complete R Markdown document, with boxes surrounding each section.\n\n\n\n\n\n\n\nFigure 6.2: Components of an R Markdown document\n\n\n\n\nThe YAML Metadata\nThe YAML section is the very beginning of an R Markdown document. The name YAML comes from the recursive acronym YAML ain’t markup language, whose meaning isn’t important for our purposes. Three dashes indicate its beginning and end, and the text inside of it contains metadata about the R Markdown document:\n\n---\ntitle: \"Penguins Report\"\nauthor: \"David Keyes\"\ndate: \"2024-01-12\"\noutput: word_document\n---\n\nAs you can see, the YAML provides the title, author, date, and output format. All elements of the YAML are given in key: value syntax, where each key is a label for a piece of metadata (for example, the title) followed by the value.\nThe R Code Chunks\nR Markdown documents have a different structure from the R script files you might be familiar with (those with the .R extension). R script files treat all content as code unless you comment out a line by putting a hash mark (#) in front of it. In the following listing, the first line is a comment, and the second line is code:\n\n```{r}\n# Import our data\ndata &lt;- read_csv(\"data.csv\")\n```\n\nIn R Markdown, the situation is reversed. Everything after the YAML is treated as text unless you specify otherwise by creating code chunks. These start with three backticks (```), followed by the lowercase letter r surrounded by curly brackets ({}). Another three backticks indicate the end of the code chunk:\n\n```{r}\nlibrary(tidyverse)\n```\n\nIf you’re working in RStudio, code chunks should have a light gray background. R Markdown treats anything in the code chunk as R code when you knit. For example, this code chunk will produce a histogram in the final Word document:\n\n```{r}\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\nFigure 6.3 shows the resulting histogram.\n\n\n\n\n\n\n\nFigure 6.3: A simple histogram generated by an R Markdown code chunk\n\n\n\n\nA code chunk at the top of each R Markdown document, known as the setup code chunk, gives instructions for what should happen when knitting a document. It contains the following options:\n\necho Do you want to show the code itself in the knitted document?\ninclude Do you want to show the output of the code chunk?\nmessage Do you want to include any messages that code might generate? For example, this message shows up when you run library(tidyverse):\n\n── Attaching core tidyverse packages ───── tidyverse 1.3.2.9000 ──\n✔ dplyr     1.0.10     ✔ readr     2.1.3 \n✔ forcats   0.5.2      ✔ stringr   1.5.0 \n✔ ggplot2   3.4.0      ✔ tibble    3.1.8 \n✔ lubridate 1.9.0      ✔ tidyr     1.2.1 \n✔ purrr     1.0.1      \n── Conflicts───── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nwarning Do you want to include any messages that the code might generate? For example, here’s the message you get when creating a histogram using geom_histogram():\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nTo see the full list of code chunk options, visit https://yihui.org/knitr/options/.\n\n\n\nIn cases where you’re using R Markdown to generate a report for a non-R user, you likely would want to hide the code, messages, and warnings but show the output (which would include any visualizations you generate). The following setup code chunk does this:\n\n```{r setup, include = FALSE}\nknitr::opts_chunk$set(\n  include = TRUE,\n  echo = FALSE,\n  message = FALSE,\n  warning = FALSE\n)\n```\n\nThe include = FALSE option on the first line applies to the setup code chunk itself. It tells R Markdown not to include the output of the setup code chunk when knitting. The options within knitr::opts_chunk$set() apply to all future code chunks. However, you can also override these global code chunk options on individual chunks. If you wanted your Word document to show both the plot itself and the code used to make it, for example, you could set echo = TRUE for that code chunk only:\n\n```{r echo = TRUE}\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\nBecause include is already set to TRUE within knitr::opts_chunk$set() in the setup code chunk, you don’t need to specify it again.\nMarkdown Text\nMarkdown is a way to style text. If you were writing directly in Word, you could just press the B button to make text bold, for example, but R doesn’t have such a button. If you want your knitted Word document to include bold text, you need to use Markdown to indicate this style in the document.\nMarkdown text sections (which have a white background in RStudio) will be converted into formatted text in the Word document after knitting. Figure 6.4 highlights the equivalent sections in the R Markdown and Word documents.\n\n\n\n\n\n\n\nFigure 6.4: Markdown text in R Markdown and its equivalent in a knitted Word document\n\n\n\n\nThe text # Introduction in R Markdown gets converted to a first-level heading, while ## Bill Length becomes a second-level heading. By adding hashes, you can create up to six levels of headings. In RStudio, headings are easy to find because they show up in blue.\nText without anything before it becomes body text in Word. To create italic text, add single asterisks around it (*like this*). To make text bold, use double asterisks (**as shown here**).\nYou can make bulleted lists by placing a dash at the beginning of a line and adding your text after it:\n\n- Adelie\n- Gentoo\n- Chinstrap\n\nTo make ordered lists, replace the dashes with numbers. You can either number each line consecutively or, as done below, repeat 1. In the knitted document, the proper numbers will automatically generate.\n\n1. Adelie\n1. Gentoo\n1. Chinstrap\n\nFormatting text in Markdown might seem more complicated than doing so in Word. But if you want to switch from a multi-tool workflow to a reproducible R Markdown–based workflow, you need to remove all manual actions from the process so that you can easily repeat it in the future.\nInline R Code\nR Markdown documents can also include little bits of code within Markdown text. To see how this inline code works, take a look at the following sentence in the R Markdown document:\n\nThe average bill length is `r average_bill_length` millimeters.\n\nInline R code begins with a backtick and the lowercase letter r and ends with another backtick. In this example, the code tells R to print the value of the variable average_bill_length, which is defined as follows in the code chunk before the inline code:\n\n```{r}\naverage_bill_length &lt;- penguins %&gt;%\n  summarize(avg_bill_length = mean(\n    bill_length_mm,\n    na.rm = TRUE\n  )) %&gt;%\n  pull(avg_bill_length)\n```\n\nThis code calculates the average bill length and saves it as average_bill_length. Having created this variable, you can now use it in the inline code. As a result, the Word document includes the sentence “The average bill length is 43.9219298.”\nOne benefit of using inline R code is that you avoid having to copy and paste values, which is error-prone. Inline R code also makes it possible to automatically calculate values on the fly whenever you reknit the R Markdown document with new data. To see how this works, you’ll make a new report using data from 2008. To do this, you need to change only one line, the one that reads the data:\n\npenguins &lt;- read_csv(\"https://data.rfortherestofus.com/penguins-2008.csv\")\n\nNow that you’ve switched penguins-2007.csv to penguins-2008.csv, you can reknit the report and produce a new Word document, complete with updated results. Figure 6.5 shows the new document.\n\n\n\n\n\n\n\nFigure 6.5: The knitted Word document with 2008 data\n\n\n\n\nThe new histogram is based on the 2008 data, as is the average bill length of 43.5412281. These values update automatically because every time you press Knit, the code is rerun, regenerating plots and recalculating values. As long as the data you use has a consistent structure, updating a report requires just a click of the Knit button.\nRunning Code Chunks Interactively\nYou can run the code in an R Markdown document in two ways. The first is by knitting the entire document. The second is to run code chunks manually (also known as interactively) by pressing the green play button at the top right of a code chunk. The down arrow next to the green play button will run all code until that point. You can see these buttons in Figure 6.6.\n\n\n\n\n\n\n\nFigure 6.6: The buttons on code chunks in RStudio\n\n\n\n\nYou can also use command-enter on macOS or ctrl-enter on Windows to run sections of code, as in an R script file. Running code interactively is a good way to test that portions of it work before you knit the entire document.\nThe one downside to running code interactively is that you can sometimes make mistakes that cause your R Markdown document to fail to knit. That is because, in order to knit, an R Markdown document must contain all the code it uses. If you’re working interactively and, say, load data from a separate file, you won’t be able to knit your document. When working in R Markdown, always keep all your code within a single document.\nThe code must also appear in the right order. An R Markdown document that looks like this, for example, will give you an error if you try to knit it:\n\n---\ntitle: \"Penguins Report\"\nauthor: \"David Keyes\"\ndate: \"2024-01-12\"\noutput: word_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  include = TRUE,\n  echo = FALSE,\n  message = FALSE,\n  warning = FALSE\n)\n```\n\n```{r}\npenguins &lt;- read_csv(\"https://data.rfortherestofus.com/penguins-2008.csv\")\n```\n\n```{r}\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\n```{r}\nlibrary(tidyverse)\n```\n\nThis error happens because you are attempting to use tidyverse functions like read_csv(), as well as various ggplot functions, before you load the tidyverse package.\nAlison Hill, a research scientist and one of the most prolific R Markdown educators, tells her students to knit early and often. This practice makes it easier to isolate issues that make knitting fail. Hill describes her typical R Markdown workflow as spending 75 percent of her time working on a new document and 25 percent of her time knitting to check that the R Markdown document works.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#quarto",
    "href": "rmarkdown.html#quarto",
    "title": "6  R Markdown Reports",
    "section": "Quarto",
    "text": "Quarto\nIn 2022, Posit released a publishing tool similar to R Markdown. Known as Quarto, this tool takes what R Markdown has done for R and extends it to other languages, including Python, Julia, and Observable JS. As I write this book, Quarto is gaining traction. Luckily, the concepts you’ve learned in this chapter apply to Quarto as well. Quarto documents have a YAML section, code chunks, and Markdown text. You can export Quarto documents to HTML, PDF, and Word. However, R Markdown and Quarto documents have some syntactic differences, which are explored further in Chapter 10.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#summary",
    "href": "rmarkdown.html#summary",
    "title": "6  R Markdown Reports",
    "section": "Summary",
    "text": "Summary\nYou started this chapter by considering the scenario of a report that needs to be regenerated monthly. You learned how you can use R Markdown to reproduce this report every month without changing your code. Even if you lost the final Word document, you could quickly re-create it.\nBest of all, working with R Markdown makes it possible to do in seconds what would have previously taken hours. When making a single report requires three tools and five steps, you may not want to work on it. But, as Alison Hill has pointed out, with R Markdown you can even work on reports before you receive all of the data. You could simply write code that works with partial data and rerun it with the final data at any time.\nThis chapter has just scratched the surface of what R Markdown can do. The next chapter will show you how to use it to instantly generate hundreds of reports. Magic indeed!",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#additional-resources",
    "href": "rmarkdown.html#additional-resources",
    "title": "6  R Markdown Reports",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nYihui Xie, J. J. Allaire, and Garrett Grolemund, R Markdown: The Definitive Guide (Boca Raton, FL: CRC Press, 2019), https://bookdown.org/yihui/rmarkdown/.\nYihui Xie, Christophe Dervieux, and Emily Riederer, R Markdown Cookbook (Boca Raton, FL: CRC Press, 2021), https://bookdown.org/yihui/rmarkdown-cookbook/.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R Markdown Reports</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html",
    "href": "parameterized-reporting.html",
    "title": "7  Parameterized Reporting",
    "section": "",
    "text": "Report Templates in R Markdown\nIf you’ve ever had to create multiple reports at the same time, you know how frustrating it can be, especially if you’re using the multi-tool workflow described in Chapter 6. Making just one report can take a long time. Multiply that work by 10, 20, or, in the case of the Urban Institute team, 51, and it can quickly feel overwhelming. Fortunately, with parameterized reporting, you can generate thousands of reports at once using the following workflow:\nYou’ll begin by creating a report template for one state. I’ve taken the code that the Urban Institute staff used to make their state fiscal briefs and simplified it significantly. All of the packages used are ones you’ve seen in previous chapters, with the exception of the urbnthemes package. This package contains a custom ggplot theme. It can be installed by running remotes::install_github(\"UrbanInstitute/urbnthemes\") in the console. Instead of focusing on fiscal data, I’ve used data you may be more familiar with: COVID-19 rates from mid-2022. Here’s the R Markdown document:\n---\ntitle: \"Urban Institute COVID Report\"\noutput: html_document\nparams:\nstate: \"Alabama\"\n---\n  \n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo = FALSE,\n  warning = FALSE,\n  message = FALSE\n)\n```\n\n```{r}\nlibrary(tidyverse)\nlibrary(urbnthemes)\nlibrary(scales)\n```\n\n# `r params$state`\n\n```{r}\ncases &lt;- tibble(state.name) %&gt;%\n  rbind(state.name = \"District of Columbia\") %&gt;%\n  left_join(\n    read_csv(\n      \"united_states_covid19_cases_deaths_and_testing_by_state.csv\",\n      skip = 2\n    ),\n    by = c(\"state.name\" = \"State/Territory\")\n  ) %&gt;%\n  select(\n    total_cases = `Total Cases`,\n    state.name,\n    cases_per_100000 = `Case Rate per 100000`\n  ) %&gt;%\n  mutate(cases_per_100000 = parse_number(cases_per_100000)) %&gt;%\n  mutate(case_rank = rank(-cases_per_100000, ties.method = \"min\"))\n```\n\n```{r}\nstate_text &lt;- if_else(params$state == \"District of Columbia\", str_glue(\"the District of Columbia\"), str_glue(\"state of {params$state}\"))\n\nstate_cases_per_100000 &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(cases_per_100000) %&gt;%\n  comma()\n\nstate_cases_rank &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(case_rank)\n```\n\nIn `r state_text`, there were `r state_cases_per_100000` cases per 100,000 people in the last seven days. This puts `r params$state` at number `r state_cases_rank` of 50 states and the District of Columbia. \n\n```{r fig.height = 8}\nset_urbn_defaults(style = \"print\")\n\ncases %&gt;%\n  mutate(highlight_state = if_else(state.name == params$state, \"Y\", \"N\")) %&gt;%\n  mutate(state.name = fct_reorder(state.name, cases_per_100000)) %&gt;%\n  ggplot(aes(\n    x = cases_per_100000,\n    y = state.name,\n    fill = highlight_state\n  )) +\n  geom_col() +\n  scale_x_continuous(labels = comma_format()) +\n  theme(legend.position = \"none\") +\n  labs(\n    y = NULL,\n    x = \"Cases per 100,000\"\n  )\n```\nThe text and charts in the report come from the cases data frame, shown here:\n# A tibble: 51 × 4\n   total_cases state.name  cases_per_100000 case_rank\n   &lt;chr&gt;       &lt;chr&gt;                  &lt;dbl&gt;     &lt;int&gt;\n 1 1302945     Alabama                26573        18\n 2 246345      Alaska                 33675         2\n 3 2025435     Arizona                27827        10\n 4 837154      Arkansas               27740        12\n 5 9274208     California             23472        36\n 6 1388702     Colorado               24115        34\n 7 766172      Connecticut            21490        43\n 8 264376      Delaware               27150        13\n 9 5965411     Florida                27775        11\n10 2521664     Georgia                23750        35\n# ℹ 41 more rows\nWhen you knit the document, you end up with the simple HTML file shown in Figure 7.1.\nFigure 7.1: The Alabama report generated via R Markdown\nYou should recognize the R Markdown document’s YAML, R code chunks, inline code, and Markdown text from Chapter 6.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#report-templates-in-r-markdown",
    "href": "parameterized-reporting.html#report-templates-in-r-markdown",
    "title": "7  Parameterized Reporting",
    "section": "",
    "text": "Make a report template in R Markdown.\nAdd a parameter (for example, one representing US states) in the YAML of your R Markdown document to represent the values that will change between reports.\nUse that parameter to generate a report for one state, to make sure you can knit your document.\nCreate a separate R script file that sets the value of the parameter and then knits a report.\nRun this script for all states.\n\n\n\n\n\n\n\n\nDefining Parameters\nIn R Markdown, parameters are variables that you set in the YAML to allow you to create multiple reports. Take a look at these two lines in the YAML:\n\nparams:\n  state: \"Alabama\"\n\nThis code defines a variable called state. You can use the state variable throughout the rest of the R Markdown document with the params$variable_name syntax, replacing variable_name with state or any other name you set in the YAML. For example, consider this inline R code:\n\n# `r params$state`\n\nAny instance of the params$state parameter will be converted to “Alabama” when you knit it. This parameter and several others appear in the following code, which sets the first-level heading visible in Figure 7.1:\n\nIn `r state_text`, there were `r state_cases_per_100000` cases per 100,000 people in the last seven days. This puts `r params$state` at number `r state_cases_rank` of 50 states and the District of Columbia. \n\nAfter knitting the document, you should see the following text:\n\nIn the state of Alabama, there were 26,573 cases per 100,000 people in the last seven days. This puts Alabama at number 18 of 50 states and the District of Columbia.\n\nThis text is automatically generated. The inline R code `r state_text` prints the value of the variable state_text, which is determined by a previous call to if_else(), shown in this code chunk:\n\nstate_text &lt;- if_else(params$state == \"District of Columbia\", str_glue(\"the District of Columbia\"), str_glue(\"state of {params$state}\"))\n\nIf the value of params$states is “District of Columbia”, this code sets state_text equal to “the District of Columbia”. If params$state isn’t “District of Columbia”, then state_text gets the value “state of”, followed by the state name. This allows you to put state_text in a sentence and have it work no matter whether the state parameter is a state or the District of Columbia.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#generating-numbers-with-parameters",
    "href": "parameterized-reporting.html#generating-numbers-with-parameters",
    "title": "7  Parameterized Reporting",
    "section": "Generating Numbers with Parameters",
    "text": "Generating Numbers with Parameters\nYou can also use parameters to generate numeric values to include in the text. For example, to calculate the values of the state_cases_per_100000 and state_cases_rank variables dynamically, use the state parameter, as shown here:\n\nstate_cases_per_100000 &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(cases_per_100000) %&gt;%\n  comma()\n\nstate_cases_rank &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(case_rank)\n\nFirst, this code filters the cases data frame (which contains data for all states) to keep only the data for the state in params$state. Then, the pull() function gets a single value from that data, and the comma() function from the scales package applies formatting to make state_cases_per_100000 display as 26,573 (rather than 26573). Finally, the state_cases_per_100000 and state_case_rank variables are integrated into the inline R code.\nIncluding Parameters in Visualization Code\nThe params$state parameter is used in other places as well, such as to highlight a state in the report’s bar chart. To see how to accomplish this, look at the following section from the last code chunk:\n\ncases %&gt;%\n  mutate(highlight_state = if_else(state.name == params$state, \"Y\", \"N\"))\n\nThis code creates a variable called highlight_state. Within the cases data frame, the code checks whether state.name is equal to params$state. If it is, highlight_state gets the value Y. If not, it gets N. Here’s what the relevant columns look like after you run these two lines:\n\n\n# A tibble: 51 × 2\n   state.name  highlight_state\n   &lt;chr&gt;       &lt;chr&gt;          \n 1 Alabama     Y              \n 2 Alaska      N              \n 3 Arizona     N              \n 4 Arkansas    N              \n 5 California  N              \n 6 Colorado    N              \n 7 Connecticut N              \n 8 Delaware    N              \n 9 Florida     N              \n10 Georgia     N              \n# ℹ 41 more rows\n\n\nLater, the ggplot code uses the highlight_state variable for the bar chart’s fill aesthetic property, highlighting the state in params$state in yellow and coloring the other states blue. Figure 7.2 shows the chart with Alabama highlighted.\n\n\n\n\n\n\n\nFigure 7.2: Highlighting data in a bar chart using parameters\n\n\n\n\nAs you’ve seen, setting a parameter in the YAML allows you to dynamically generate text and charts in the knitted report. But you’ve generated only one report so far. How can you create all 51 reports? Your first thought might be to manually update the YAML by changing the parameter’s value from “Alabama” to, say, “Alaska” and then knitting the document again. While you could follow this process for all states, it would be tedious, which is what you’re trying to avoid. Instead, you can automate the report generation.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#creating-an-r-script",
    "href": "parameterized-reporting.html#creating-an-r-script",
    "title": "7  Parameterized Reporting",
    "section": "Creating an R Script",
    "text": "Creating an R Script\nTo automatically generate multiple reports based on the template you’ve created, you’ll use an R script that changes the value of the parameters in the R Markdown document and then knits it. You’ll begin by creating an R script file named render.R.\nKnitting the Document with Code\nYour script needs to be able to knit an R Markdown document. While you’ve seen how to do this using the Knit button, you can do the same thing with code. Load the rmarkdown package and then use its render() function as shown here:\n\nlibrary(rmarkdown)\n\nrender(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = \"Alaska.html\",\n  params = list(state = \"Alaska\")\n)\n\nThis function generates an HTML document called urban-covid-budget-report.html. By default, the generated file has the same name as the R Markdown (.Rmd) document, with a different extension. The output_file argument assigns the file a new name, and the params argument specifies parameters that will override those in the R Markdown document itself. For example, this code tells R to use Alaska for the state parameter and save the resulting HTML file as Alaska.html.\nThis approach to generating reports works, but to create all 51 reports, you’d have to manually change the state name in the YAML and update the render() function before running it for each report. In the next section, you’ll update your code to make it more efficient.\nCreating a Tibble with Parameter Data\nTo write code that generates all your reports automatically, first you must create a vector (in colloquial terms, a list of items) of all the state names and the District of Columbia. To do this, you’ll use the built-in dataset state.name, which has all 50 state names in a vector:\n\nstate &lt;- tibble(state.name) %&gt;%\n  rbind(\"District of Columbia\") %&gt;%\n  pull(state.name)\n\nThis code turns state.name into a tibble and then uses the rbind() function to add the District of Columbia to the list. The pull() function gets one single column and saves it as state. Here’s what the state vector looks like:\n\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"Florida\"             \n[10] \"Georgia\"              \"Hawaii\"               \"Idaho\"               \n[13] \"Illinois\"             \"Indiana\"              \"Iowa\"                \n[16] \"Kansas\"               \"Kentucky\"             \"Louisiana\"           \n[19] \"Maine\"                \"Maryland\"             \"Massachusetts\"       \n[22] \"Michigan\"             \"Minnesota\"            \"Mississippi\"         \n[25] \"Missouri\"             \"Montana\"              \"Nebraska\"            \n[28] \"Nevada\"               \"New Hampshire\"        \"New Jersey\"          \n[31] \"New Mexico\"           \"New York\"             \"North Carolina\"      \n[34] \"North Dakota\"         \"Ohio\"                 \"Oklahoma\"            \n[37] \"Oregon\"               \"Pennsylvania\"         \"Rhode Island\"        \n[40] \"South Carolina\"       \"South Dakota\"         \"Tennessee\"           \n[43] \"Texas\"                \"Utah\"                 \"Vermont\"             \n[46] \"Virginia\"             \"Washington\"           \"West Virginia\"       \n[49] \"Wisconsin\"            \"Wyoming\"              \"District of Columbia\"\n\n\nRather than use render() with the input and output_file arguments, as you did earlier, you can pass it the params argument to give it parameters to use when knitting. To do so, create a tibble with the information needed to render all 51 reports and save it as an object called reports, which you’ll pass to the render() function, as follows:\n\nreports &lt;- tibble(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = str_glue(\"{state}.html\"),\n  params = map(state, ~ list(state = .))\n)\n\nThis code generates a tibble with 51 rows and 3 variables. In all rows, the input variable is set to the name of the R Markdown document. The value of output_file is set with str_glue() to be equal to the name of the state, followed by .html (for example, Alabama.html).\nThe params variable is the most complicated of the three. It is what’s known as a named list. This data structure puts the data in the state: state _name format needed for the R Markdown document’s YAML. The map() function from the purrr package creates the named list, telling R to set the value of each row as state = \"Alabama\", then state = \"Alaska\", and so on, for all of the states. You can see these variables in the reports tibble:\n\n\n# A tibble: 51 × 3\n   input                         output_file      params          \n   &lt;chr&gt;                         &lt;glue&gt;           &lt;list&gt;          \n 1 urban-covid-budget-report.Rmd Alabama.html     &lt;named list [1]&gt;\n 2 urban-covid-budget-report.Rmd Alaska.html      &lt;named list [1]&gt;\n 3 urban-covid-budget-report.Rmd Arizona.html     &lt;named list [1]&gt;\n 4 urban-covid-budget-report.Rmd Arkansas.html    &lt;named list [1]&gt;\n 5 urban-covid-budget-report.Rmd California.html  &lt;named list [1]&gt;\n 6 urban-covid-budget-report.Rmd Colorado.html    &lt;named list [1]&gt;\n 7 urban-covid-budget-report.Rmd Connecticut.html &lt;named list [1]&gt;\n 8 urban-covid-budget-report.Rmd Delaware.html    &lt;named list [1]&gt;\n 9 urban-covid-budget-report.Rmd Florida.html     &lt;named list [1]&gt;\n10 urban-covid-budget-report.Rmd Georgia.html     &lt;named list [1]&gt;\n# ℹ 41 more rows\n\n\nThe params variable shows up as &lt;named list&gt;, but if you open the tibble in the RStudio viewer (click reports in the Environment tab), you can see the output more clearly, as shown in Figure 7.3.\n\n\n\n\n\n\n\nFigure 7.3: The named list column in the RStudio viewer\n\n\n\n\nThis view allows you to see the named list in the params variable, with the state variable equal to the name of each state.\nOnce you’ve created the reports tibble, you’re ready to render the reports. The code to do so is only one line long:\n\npwalk(reports, render)\n\nThis pwalk() function (from the purrr package) has two arguments: a data frame or tibble (reports, in this case) and a function that runs for each row of this tibble, render().\n\n\n\n\n\n\nYou don’t include the open and closing parentheses when passing the render() function to pwalk().\n\n\n\nRunning this code runs the render() function for each row in reports, passing in the values for input, output_file, and params. This is equivalent to entering code like the following to run the render() function 51 times (for 50 states plus the District of Columbia):\n\nrender(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = \"Alabama.html\",\n  params = list(state = \"Alabama\")\n)\n\nrender(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = \"Alaska.html\",\n  params = list(state = \"Alaska\")\n)\n\nrender(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = \"Arizona.html\",\n  params = list(state = \"Arizona\")\n)\n\nHere’s the full R script file:\n\n# Load packages\nlibrary(tidyverse)\nlibrary(rmarkdown)\n\n# Create a vector of all states and the District of Columbia\nstate &lt;- tibble(state.name) %&gt;%\n  rbind(\"District of Columbia\") %&gt;%\n  pull(state.name)\n\n# Create a tibble with information on the:\n# input R Markdown document\n# output HTML file\n# parameters needed to knit the document\nreports &lt;- tibble(\n  input = \"urban-covid-budget-report.Rmd\",\n  output_file = str_glue(\"{state}.html\"),\n  params = map(state, ~ list(state = .))\n)\n\n# Generate all of our reports\npwalk(reports, render)\n\nAfter running the pwalk(reports, render) code, you should see 51 HTML documents appear in the files pane in RStudio. Each document consists of a report for that state, complete with a customized graph and accompanying text.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#best-practices",
    "href": "parameterized-reporting.html#best-practices",
    "title": "7  Parameterized Reporting",
    "section": "Best Practices",
    "text": "Best Practices\nWhile powerful, parameterized reporting can present some challenges. For example, make sure to consider outliers in your data. In the case of the state reports, Washington, DC, is an outlier because it isn’t technically a state. The Urban Institute team altered the language in the report text so that it didn’t refer to Washington, DC, as a state by using an if_else() statement, as you saw at the beginning of this chapter.\nAnother best practice is to manually generate and review the reports whose parameter values have the shortest (Iowa, Ohio, and Utah in the state fiscal briefs) and longest (District of Columbia) text lengths. This way, you can identify places where the text length may have unexpected results, such as cut-off chart titles or page breaks disrupted by text running onto multiple lines. A few minutes of manual review can make the process of autogenerating multiple reports much smoother.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#summary",
    "href": "parameterized-reporting.html#summary",
    "title": "7  Parameterized Reporting",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you re-created the Urban Institute’s state fiscal briefs using parameterized reporting. You learned how to add a parameter to your R Markdown document, then use an R script to set the value of that parameter and knit the report.\nAutomating report production can be a huge time-saver, especially as the number of reports you need to generate grows. Consider another project at the Urban Institute: making county-level reports. With over 3,000 counties in the United States, creating these reports by hand isn’t realistic. Not only that, but if the Urban Institute employees were to make their reports using SPSS, Excel, and Word, they would have to copy and paste values between programs. Humans are fallible, and mistakes occur, no matter how hard we try to avoid them. Computers, on the other hand, never make copy-and-paste errors. Letting computers handle the tedious work of generating multiple reports reduces the chance of error significantly.\nWhen you’re starting out, parameterized reporting might feel like a heavy lift, as you have to make sure that your code works for every version of your report. But once you have your R Markdown document and accompanying R script file, you should find it easy to produce multiple reports at once, saving you work in the end.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "parameterized-reporting.html#additional-resources",
    "href": "parameterized-reporting.html#additional-resources",
    "title": "7  Parameterized Reporting",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nData@Urban Team, “Iterated Fact Sheets with R Markdown,” Medium, July 24, 2018, https://urban-institute.medium.com/iterated-fact-sheets-with-r-markdown-d685eb4eafce.\nData@Urban Team, “Using R Markdown to Track and Publish State Data,” Medium, April 21, 2021, https://urban-institute.medium.com/using-r-markdown-to-track-and-publish-state-data-d1291bfa1ec0.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parameterized Reporting</span>"
    ]
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "8  Slideshow Presentations",
    "section": "",
    "text": "Why Use xaringan?\nYou might have noticed the Presentation option while creating a new R Markdown document in RStudio. This option offers several ways to make slides, such as knitting an R Markdown document to PowerPoint. However, using the xaringan package provides advantages over these options.\nFor example, because xaringan creates slides as HTML documents, you can post them online versus having to email them or print them out for viewers. You can send someone the presentation simply by sharing a link. Chapter 9 will discuss ways to publish your presentations online.\nA second benefit of using xaringan is accessibility. HTML documents are easy to manipulate, giving viewers control over their appearance. For example, people with limited vision can access HTML documents in ways that allow them to view the content, such as by increasing the text size or using screen readers. Making presentations with xaringan lets more people engage with your slides.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slideshow Presentations</span>"
    ]
  },
  {
    "objectID": "presentations.html#how-xaringan-works",
    "href": "presentations.html#how-xaringan-works",
    "title": "8  Slideshow Presentations",
    "section": "How xaringan Works",
    "text": "How xaringan Works\nTo get started with xaringan, run install.packages(\"xaringan\") in RStudio to install the package. Next, navigate to File &gt; New File &gt; R Markdown to create a new project. Choose the From Template tab and select the template called Ninja Presentation, then click OK.\nYou should get an R Markdown document containing some default content. Delete this and add the penguin R report you created in Chapter 6. Then, change the output format in the YAML to xaringan::moon_reader like so:\n\n---\ntitle: \"Penguins Report\"\nauthor: \"David Keyes\"\ndate: \"2024-01-12\"\noutput: xaringan::moon_reader\n---\n\nThe moon_reader output format takes R Markdown documents and knits them as slides. Try clicking Knit to see what this looks like. You should get an HTML file with the same name as the R Markdown document (such as xaringan-example.html), as shown in Figure 8.1.\n\n\n\n\n\n\n\nFigure 8.1: The xaringan package automatically generates a title slide.\n\n\n\n\nIf you scroll to the next slide with the right arrow key, you should see familiar content. Figure 8.2 shows the second slide, which has the same text as the report from Chapter 6 and a cut-off version of its histogram.\n\n\n\n\n\n\n\nFigure 8.2: The second slide needs adjustment, as the histogram is cut off.\n\n\n\n\nAlthough the syntax for making slides with xaringan is nearly identical to that used to make reports with R Markdown, you need to make a few tweaks so that the content can fit on the slides. When you’re working in a document that will be knitted to Word, its length doesn’t matter, because reports can have 1 page or 100 pages. Working with xaringan, however, requires you to consider how much content can fit on a single slide. The cut-off histogram demonstrates what happens if you don’t. You’ll fix it next.\nCreating a New Slide\nYou’ll make this histogram fully visible by putting it in its own slide. To make a new slide, add three dashes (---) where you’d like it to begin. I’ve added them before the histogram code:\n\n---\n\n## Bill Length\n\nWe can make a histogram to see the distribution of bill lengths.\n\n```{r}\npenguins %&gt;% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\nWhen you knit the document again, what was one slide should now be broken into two: an Introduction slide and a Bill Length slide. However, if you look closely, you’ll notice that the bottom of the histogram is still slightly cut off. To correct this, you’ll change its size.\nAdjusting the Size of Figures\nAdjust the size of the histogram using the code chunk option fig.height:\n\n---\n\n## Bill Length\n\nWe can make a histogram to see the distribution of bill lengths.\n\n```{r fig.height = 4}\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\nDoing this fits the histogram fully on the slide and also reveals the text that was hidden below it. Keep in mind that fig.height adjusts only the figure’s output height; sometimes you may need to adjust the output width using fig.width in addition or instead.\nRevealing Content Incrementally\nWhen presenting a slideshow, you might want to show only a portion of the content on each slide at a time. Say, for example, that when you’re presenting the first slide, you want to talk a bit about each penguin species. Rather than show all three species when you open this slide, you might prefer to have the names come up one at a time.\nYou can do this using a feature xaringan calls incremental reveal. Place two dashes (–) between any content you want to display incrementally, like so:\n\n# Introduction\n\nWe are writing a report about the **Palmer Penguins**. These penguins are *really* amazing. There are three species:\n\n- Adelie\n\n--\n\n- Gentoo\n\n--\n\n- Chinstrap\n\nThis code lets you show Adelie onscreen first; then Adelie and Gentoo; and then Adelie, Gentoo, and Chinstrap. When presenting your slides, use the right arrow to incrementally reveal the species.\nAligning Content with Content Classes\nYou’ll also likely want to control how your content is aligned. To do so, you add the content classes .left[], right[], and center[] to specify the desired alignment for a piece of content. For example, to center-align the histogram, use .center[] as follows:\n\n.center[\n```{r fig.height = 4}\npenguins %&gt;% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n]\n\nThis code centers the chart on the slide.\nOther built-in options can make two-column layouts. Adding .pull-left[] and .pull-right[] will make two equally spaced columns. Use the following code to display the histogram on the left side of the slide and the accompanying text on the right:\n\n.pull-left[\n```{r fig.height = 4}\npenguins %&gt;% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n]\n\n.pull-right[\n```{r}\naverage_bill_length &lt;- penguins %&gt;% \n  summarize(avg_bill_length = mean(bill_length_mm,\n                                   na.rm = TRUE)) %&gt;% \n  pull(avg_bill_length)\n```\n\nThe chart shows the distribution of bill lengths. The average bill length is `r average_bill_length` millimeters.\n]\n\nFigure 8.3 shows the result.\n\n\n\n\n\n\n\nFigure 8.3: A slide with two columns of equal size\n\n\n\n\nTo make a narrow left column and wide right column, use the content classes .left-column[] and .right-column[]. Figure 8.4 shows what the slide looks like with the text on the left and the histogram on the right.\n\n\n\n\n\n\n\nFigure 8.4: A slide with a smaller left column and a larger right column\n\n\n\n\nIn addition to aligning particular pieces of content on slides, you can also horizontally align the entire content using the left, right, and center classes. To do so, specify the class right after the three dashes that indicate a new slide, but before any content:\n\n---\nclass: center\n\n## Bill Length\n\nWe can make a histogram to see the distribution of bill lengths.\n\n```{r fig.height = 4}\npenguins %&gt;% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\n```{r}\naverage_bill_length &lt;- penguins %&gt;% \n  summarize(avg_bill_length = mean(bill_length_mm,\n                                   na.rm = TRUE)) %&gt;% \n  pull(avg_bill_length)\n```\n\nThe chart shows the distribution of bill lengths. The average bill length is `r average_bill_length` millimeters.\n\nThis code produces a horizontally centered slide. To adjust the vertical position, you can use the classes top, middle, and bottom.\nAdding Background Images to Slides\nUsing the same syntax you just used to center the entire slide, you can also add a background image. Create a new slide, use the classes center and middle to horizontally and vertically align the content, and add a background image by specifying the path to the image within the parentheses of url():\n\nclass: center, middle\nbackground-image: url(\"penguins.jpg\")\n\n## Penguins\n\nTo run this code, you’ll need a file called penguins.jpg in your project (you can download it at https://data.rfortherestofus.com/penguins.jpg). Knitting the document should produce a slide that uses this image as a background with the text Penguins in front of it, as shown in Figure 8.5.\n\n\n\n\n\n\n\nFigure 8.5: A slide that uses a background image\n\n\n\n\nNow you’ll add custom CSS to further improve this slide.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slideshow Presentations</span>"
    ]
  },
  {
    "objectID": "presentations.html#applying-css-to-slides",
    "href": "presentations.html#applying-css-to-slides",
    "title": "8  Slideshow Presentations",
    "section": "Applying CSS to Slides",
    "text": "Applying CSS to Slides\nOne issue with the slide you just made is that the word Penguins is hard to read. It would be better if you could make the text bigger and a different color. To do this, you’ll need to use Cascading Style Sheets (CSS), the language used to style HTML documents. If you’re thinking, I’m reading this book to learn R, not CSS, don’t worry: you’ll need only a bit of CSS to make tweaks to your slides. To apply them, you can write your own custom code, use a CSS theme, or combine the two approaches using the xaringanthemer package.\nCustom CSS\nTo add custom CSS, create a new code chunk and place css between the curly brackets:\n\n```{css}\n.remark-slide-content h2 {\n  font-size: 150px;\n  color: white;\n}\n```\n\nThis code chunk tells R Markdown to make the second-level header (h2) 150 pixels large and white. Adding .remark-slide-content before the header targets specific elements in the presentation. The term remark comes from remark.js, a JavaScript library for making presentations that xaringan uses under the hood.\nTo change the font in addition to the text’s size and color, add this CSS:\n\n```{css}\n@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap');\n\n.remark-slide-content h2 {\n  font-size: 150px;\n  color: white;\n    font-family: Inter;\n  font-weight: bold;\n}\n```\n\nThe first new line makes a font called Inter available to the slides, because some people might not have the font installed on their computers. Next, this code applies Inter to the header and makes it bold. You can see the slide with bold Inter font in Figure 8.6.\n\n\n\n\n\n\n\nFigure 8.6: The title slide with CSS changes to the font\n\n\n\n\nBecause xaringan slides are built as HTML documents, you can customize them with CSS however you’d like. The sky’s the limit!\nThemes\nYou may not care to know the ins and outs of CSS. Fortunately, you can customize your slides in two ways without writing any CSS yourself. The first way is to apply xaringan themes created by other R users. Run this code to get a list of all available themes:\n\nnames(xaringan:::list_css())\n\nThe output should look something like this:\n\n\n [1] \"chocolate-fonts\"  \"chocolate\"        \"default-fonts\"    \"default\"         \n [5] \"duke-blue\"        \"fc-fonts\"         \"fc\"               \"glasgow_template\"\n [9] \"hygge-duke\"       \"hygge\"            \"ki-fonts\"         \"ki\"              \n[13] \"kunoichi\"         \"lucy-fonts\"       \"lucy\"             \"metropolis-fonts\"\n[17] \"metropolis\"       \"middlebury-fonts\" \"middlebury\"       \"nhsr-fonts\"      \n[21] \"nhsr\"             \"ninjutsu\"         \"rladies-fonts\"    \"rladies\"         \n[25] \"robot-fonts\"      \"robot\"            \"rutgers-fonts\"    \"rutgers\"         \n[29] \"shinobi\"          \"tamu-fonts\"       \"tamu\"             \"uio-fonts\"       \n[33] \"uio\"              \"uo-fonts\"         \"uo\"               \"uol-fonts\"       \n[37] \"uol\"              \"useR-fonts\"       \"useR\"             \"uwm-fonts\"       \n[41] \"uwm\"              \"wic-fonts\"        \"wic\"             \n\n\nSome CSS files change fonts only, while others change general elements, such as text size, colors, and whether slide numbers are displayed. Using prebuilt themes usually requires you to use both a general theme and a fonts theme, as follows:\n\n---\ntitle: \"Penguins Report\"\nauthor: \"David Keyes\"\ndate: \"2024-01-12\"\noutput:\n  xaringan::moon_reader:\n    css: [default, metropolis, metropolis-fonts]\n---\n\nThis code tells xaringan to use the default CSS, as well as customizations made in the metropolis and metropolis-fonts CSS themes. These come bundled with xaringan, so you don’t need to install any additional packages to access them. Figure 8.7 shows how the theme changes the look and feel of the slides.\n\n\n\n\n\n\n\nFigure 8.7: A slide using the metropolis theme\n\n\n\n\nIf writing custom CSS is the totally flexible but more challenging option for tweaking your xaringan slides, then using a custom theme is simpler but a lot less flexible. Custom themes allow you to easily use others’ prebuilt CSS but not to tweak it further.\nThe xaringanthemer Package\nA nice middle ground between writing custom CSS and applying someone else’s theme is to use the xaringanthemer package by Garrick Aden-Buie. This package includes several built-in themes but also allows you to easily create your own custom theme. After installing the package, adjust the css line in your YAML to use the xaringan-themer.css file like so:\n\n---\ntitle: \"Penguins Report\"\nauthor: \"David Keyes\"\ndate: \"2024-01-12\"\noutput:\n  xaringan::moon_reader:\n    css: xaringan-themer.css\n---\n\nNow you can customize your slides by using the style_xaringan() function. This function has over 60 arguments, enabling you to tweak nearly any part of your xaringan slides. To replicate the custom CSS you wrote earlier in this chapter using xaringanthemer, you’ll use just a few of the arguments:\n\n```{r}\nlibrary(xaringanthemer)\n\nstyle_xaringan(\n  header_h2_font_size = \"150px\",\n  header_color = \"white\",\n  header_font_weight = \"bold\",\n  header_font_family = \"Inter\"\n)\n```\n\nThis code sets the header size to 150 pixels and makes all the headers use the bold, white Inter font.\nOne particularly nice thing about the xaringanthemer package is that you can use any font available on Google Fonts by simply adding its name to header_font_family or another argument that sets font families (text_font_family and code_font_family are the other two, for styling body text and code, respectively). This means you won’t have to include the line that makes the Inter font available.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slideshow Presentations</span>"
    ]
  },
  {
    "objectID": "presentations.html#summary",
    "href": "presentations.html#summary",
    "title": "8  Slideshow Presentations",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you learned how to create presentations using the xaringan package. You saw how to incrementally reveal content on slides, create multi-column layouts, and add background images to slides. You also changed your slides’ appearance by applying custom themes, writing your own CSS, and using the xaringanthemer package.\nWith xaringan, you can create any type of presentation you want and then customize it to match your desired look and feel. Creating presentations with xaringan also allows you to share your HTML slides easily and enables greater accessibility.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slideshow Presentations</span>"
    ]
  },
  {
    "objectID": "presentations.html#additional-resources",
    "href": "presentations.html#additional-resources",
    "title": "8  Slideshow Presentations",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGarrick Aden-Buie, Silvia Canelón, and Shannon Pileggi, “Professional, Polished, Presentable: Making Great Slides with xaringan,” workshop materials, n.d., https://presentable-user2021.netlify.app.\nSilvia Canelón, “Sharing Your Work with xaringan: An Introduction to xaringan for Presentations: The Basics and Beyond,” workshop for the NHS-R Community 2020 Virtual Conference, November 2, 2020, https://spcanelon.github.io/xaringan-basics-and-beyond/index.html.\nAlison Hill, “Meet xaringan: Making Slides in R Markdown,” slideshow presentation, January 16, 2019, https://arm.rbind.io/slides/xaringan.html.\nYihui Xie, J. J. Allaire, and Garrett Grolemund, “xaringan Presentations,” in R Markdown: The Definitive Guide (Boca Raton, FL: CRC Press, 2019), https://bookdown.org/yihui/rmarkdown/.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Slideshow Presentations</span>"
    ]
  },
  {
    "objectID": "websites.html",
    "href": "websites.html",
    "title": "9  Websites",
    "section": "",
    "text": "Creating a New distill Project\nA website is merely a collection of HTML files like the one you produced in Chapter 8 when you created a slideshow presentation. The distill package uses multiple R Markdown documents to create several HTML files, then connects them with a navigation menu and more.\nTo create a distill website, install the package using install.packages(\"distill\"). Then start a project in RStudio by navigating to File &gt; New Project &gt; New Directory and selecting Distill Website as the project type.\nSpecify the directory and subdirectory where your project will live on your computer, then give your website a title. Check the Configure for GitHub Pages option, which provides an easy way to post your website online (you’ll learn how it works in Section 9.6.2). Select it if you’d like to use this deployment option.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Websites</span>"
    ]
  },
  {
    "objectID": "websites.html#the-project-files",
    "href": "websites.html#the-project-files",
    "title": "9  Websites",
    "section": "The Project Files",
    "text": "The Project Files\nYou should now have a project with several files. In addition to the covid-website.Rproj file indicating that you’re working in an RStudio project, you should have two R Markdown documents, a *_site.yml* file, and a docs folder, where the rendered HTML files will go. Let’s take a look at these website files.\nR Markdown Documents\nEach R Markdown file represents a page of the website. By default, distill creates a home page (index.Rmd) and an About page (about.Rmd) containing placeholder content. If you wanted to generate additional pages, you would simply add new R Markdown files, then list them in the *_site.yml* file discussed in the next section.\nIf you open the index.Rmd file, you’ll notice that the YAML contains two arguments, description and site, that didn’t appear in the R Markdown documents from previous chapters:\n\n---\ntitle: \"COVID Website\"\ndescription: |\n  Welcome to the website. I hope you enjoy it!\nsite: distill::distill_website\n---\n\nThe description argument specifies the text that should go below the title of each page, as shown in Figure 9.1.\n\n\n\n\n\n\n\nFigure 9.1: The default website description\n\n\n\n\nThe site: distill::distill_website line identifies the root page of a distill website. This means that when you knit the document, R Markdown knows to create a website rather than an individual HTML file and that the website should display this page first. The other pages of the website don’t require this line. As long as they’re listed in the *_site.yml* file, they’ll be added to the site.\nYou’ll also notice the absence of an argument you’ve seen in other R Markdown documents: output, which specifies the output format R should use while knitting. The reason output is missing here is that you’ll specify the output for the entire website in the *_site.yml* file.\nThe _site.yml File\nThe *_site.yml* file tells R which R Markdown documents make up the website, what the knitted files should look like, what the website should be called, and more. When you open it, you should see the following code:\n\nname: \"covid-website\"\ntitle: \"COVID Website\"\ndescription: |\n  COVID Website\noutput_dir: \"docs\"\nnavbar:\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\noutput: distill::distill_article\n\nThe name argument determines the URL for your website. By default, this should be the name of the directory where your distill project lives; in my case, that’s the covid-website directory. The title argument creates the title for the entire website and shows up in the top left of the navigation bar by default. The description argument provides what’s known as a meta description, which will show up as a couple of lines in Google search results to give users an overview of the website content.\nThe output_dir argument determines where the rendered HTML files live when you generate the website. You should see the docs directory listed here. However, you can change the output directory to any folder you choose.\nNext, the navbar section defines the website’s navigation. Here it appears on the right side of the header, but swapping the right parameter for left would switch its position. The navigation bar includes links to the site’s two pages, Home and About, as shown in Figure 9.2.\n\n\n\n\n\n\n\nFigure 9.2: The website navigation bar\n\n\n\n\nWithin the navbar code, the text argument specifies what text shows up in the menu. (Try, for example, changing About to About This Website, and then change it back.) The href argument determines which HTML file the text in the navigation bar links to. If you want to include additional pages on your menu, you’ll need to add both the text and href parameters.\nFinally, the output argument specifies that all R Markdown documents should be rendered using the distill_article format. This format allows for layouts of different widths, asides (parenthetical items that live in a sidebar next to the main content), easily customizable CSS, and more.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Websites</span>"
    ]
  },
  {
    "objectID": "websites.html#building-the-site",
    "href": "websites.html#building-the-site",
    "title": "9  Websites",
    "section": "Building the Site",
    "text": "Building the Site\nWe’ve explored the project’s files but haven’t yet used them to create the website. To do this, click Build Website in the Build tab of RStudio’s topright pane. (You could also run rmarkdown::render_site() in the console or in an R script file.)\nThis should render all R Markdown documents and add the top navigation bar to them with the options specified in the *_site.yml* file. To find the rendered files, look in docs (or whatever output directory you specified). Open the index.html file and you’ll find your website, which should look like Figure 9.3.\n\n\n\n\n\n\n\nFigure 9.3: The COVID website with default content\n\n\n\n\nYou can open any other HTML file as well to see its rendered version.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Websites</span>"
    ]
  },
  {
    "objectID": "websites.html#applying-custom-css-with-create_theme",
    "href": "websites.html#applying-custom-css-with-create_theme",
    "title": "9  Websites",
    "section": "Applying Custom CSS with create_theme()\n",
    "text": "Applying Custom CSS with create_theme()\n\nWebsites made with distill tend to look similar, but you can change their design using custom CSS. The distill package even provides a function to simplify this process. Run distill::create_theme() in the console to create a file called theme.css, shown here:\n\n/* base variables */\n\n/* Edit the CSS properties in this file to create a custom\n   Distill theme. Only edit values in the right column\n   for each row; values shown are the CSS defaults.\n   To return any property to the default,\n   you may set its value to: unset\n   All rows must end with a semi-colon.                      */\n\n/* Optional: embed custom fonts here with `@import`          */\n/* This must remain at the top of this file.                 */\n\nhtml {\n  /*-- Main font sizes --*/\n  --title-size:      50px;\n  --body-size:       1.06rem;\n  --code-size:       14px;\n  --aside-size:      12px;\n  --fig-cap-size:    13px;\n  /*-- Main font colors --*/\n  --title-color:     #000000;\n  --header-color:    rgba(0, 0, 0, 0.8);\n  --body-color:      rgba(0, 0, 0, 0.8);\n  --aside-color:     rgba(0, 0, 0, 0.6);\n  --fig-cap-color:   rgba(0, 0, 0, 0.6);\n  /*-- Specify custom fonts ~~~ must be imported above   --*/\n  --heading-font:    sans-serif;\n  --mono-font:       monospace;\n  --body-font:       sans-serif;\n  --navbar-font:     sans-serif;  /* websites + blogs only */\n}\n\n/*-- ARTICLE METADATA --*/\nd-byline {\n  --heading-size:    0.6rem;\n  --heading-color:   rgba(0, 0, 0, 0.5);\n  --body-size:       0.8rem;\n  --body-color:      rgba(0, 0, 0, 0.8);\n}\n\n/*-- ARTICLE TABLE OF CONTENTS --*/\n.d-contents {\n  --heading-size:    18px;\n  --contents-size:   13px;\n}\n\n/*-- ARTICLE APPENDIX --*/\nd-appendix {\n  --heading-size:    15px;\n  --heading-color:   rgba(0, 0, 0, 0.65);\n  --text-size:       0.8em;\n  --text-color:      rgba(0, 0, 0, 0.5);\n}\n\n/*-- WEBSITE HEADER + FOOTER --*/\n/* These properties only apply to Distill sites and blogs  */\n\n.distill-site-header {\n  --title-size:       18px;\n  --text-color:       rgba(255, 255, 255, 0.8);\n  --text-size:        15px;\n  --hover-color:      white;\n  --bkgd-color:       #0F2E3D;\n}\n\n.distill-site-footer {\n  --text-color:       rgba(255, 255, 255, 0.8);\n  --text-size:        15px;\n  --hover-color:      white;\n  --bkgd-color:       #0F2E3D;\n}\n\n/*-- Additional custom styles --*/\n/* Add any additional CSS rules below                      */\n\nWithin this file is a set of CSS variables that allow you to customize the design of your website. Most of them have names that clearly show their purpose, and you can alter their default values to whatever you’d like. For example, the following edits to the site’s header make the title and text size larger and change the background color to a light blue:\n\n.distill-site-header {\n  --title-size:       28px;\n  --text-color:       rgba(255, 255, 255, 0.8);\n  --text-size:        20px;\n  --hover-color:      white;\n  --bkgd-color:       #6cabdd;\n}\n\nBefore you can see these changes, however, you need to add a line to the *_site.yml* file to tell distill to use this custom CSS when rendering:\n\nname: \"covid-website\"\ntitle: \"COVID Website\"\ndescription: |\n  COVID Website\ntheme: theme.css\noutput_dir: \"docs\"\nnavbar:\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\noutput: distill::distill_article\n\nNow you can generate the site again, and you should see your changes reflected.\nThere are a lot of other CSS variables in theme.css that you can change to tweak the appearance of your website. Playing around with them and regenerating your site is a great way to figure out what each one does.\n\n\n\n\n\n\nTo learn more about customizing the look and feel of your website, check out the distill websites made by others at https://distillery.rbind.io.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Websites</span>"
    ]
  },
  {
    "objectID": "websites.html#working-with-website-content",
    "href": "websites.html#working-with-website-content",
    "title": "9  Websites",
    "section": "Working with Website Content",
    "text": "Working with Website Content\nYou can add content to a page on your website by creating Markdown text and code chunks in the page’s R Markdown document. For example, to highlight rates of COVID cases over time, you’ll replace the contents of index.Rmd with code that displays a table, a map, and a chart on the website’s home page. Here’s the start of the file:\n\n---\ntitle: \"COVID Website\"\ndescription: \"Information about COVID rates in the United States over time\"\nsite: distill::distill_website\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo = FALSE,\n  warning = FALSE,\n  message = FALSE\n)\n```\n\n```{r}\n# Load packages\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(tigris)\nlibrary(gt)\nlibrary(lubridate)\n```\n\nAfter the YAML and setup code chunk, this code loads several packages, most of which you’ve seen in previous chapters: the tidyverse for data import, manipulation, and plotting (with ggplot); janitor for its clean_names() function, which makes the variable names easier to work with; tigris to import geospatial data about states; gt for making nice tables; and lubridate to work with dates.\nNext, to import and clean the data, add this new code chunk:\n\n```{r}\n# Import data\n\nus_states &lt;- states(\n  cb = TRUE,\n  resolution = \"20m\",\n  progress_bar = FALSE\n) %&gt;%\n  shift_geometry() %&gt;%\n  clean_names() %&gt;%\n  select(geoid, name) %&gt;%\n  rename(state = name) %&gt;%\n  filter(state %in% state.name)\n\ncovid_data &lt;- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/rolling-averages/us-states.csv\") %&gt;%\n  filter(state %in% state.name) %&gt;%\n  mutate(geoid = str_remove(geoid, \"USA-\"))\n\nlast_day &lt;- covid_data %&gt;%\n  slice_max(\n    order_by = date,\n    n = 1\n  ) %&gt;%\n  distinct(date) %&gt;%\n  mutate(date_nice_format = str_glue(\"{month(date, label = TRUE, abbr = FALSE)} {day(date)}, {year(date)}\")) %&gt;%\n  pull(date_nice_format)\n```\n\n# COVID Death Rates as of `r last_day`\n\nThis code uses the slice_max() function to get the latest date in the covid_data data frame. (Data was added until March 23, 2023, so that date is the most recent one.) From there, it uses distinct() to get a single observation of the most recent date (each date shows up multiple times in the covid_data data frame). The code then creates a date_nice_format variable using the str_glue() function to combine easy-to-read versions of the month, day, and year. Finally, the pull() function turns the data frame into a single variable called last_day, which is referenced later in a text section. Using inline R code, this header now displays the current date.\nInclude the following code to make a table showing the death rates per 100,000 people in four states (using all states would create too large a table):\n\n```{r}\ncovid_data %&gt;%\n  filter(state %in% c(\n    \"Alabama\",\n    \"Alaska\",\n    \"Arizona\",\n    \"Arkansas\"\n  )) %&gt;%\n  slice_max(\n    order_by = date,\n    n = 1\n  ) %&gt;%\n  select(state, deaths_avg_per_100k) %&gt;%\n  arrange(state) %&gt;%\n  set_names(\"State\", \"Death rate\") %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  )\n```\n\nThis table resembles the code you saw in Chapter 5. First, the filter() function filters the data down to four states, and the slice_max() function gets the latest date. The code then selects the relevant variables (state and deaths_avg_per_100k), arranges the data in alphabetical order by state, sets the variable names, and pipes this output into a table made with the gt package.\nAdd the following code, which uses techniques covered in Chapter 4, to make a map of this data for all states:\n\nWe can see this same death rate data for all states on a map.\n\n```{r}\nmost_recent &lt;- us_states %&gt;%\n  left_join(covid_data, by = \"state\") %&gt;%\n  slice_max(\n    order_by = date,\n    n = 1\n  )\n\nmost_recent %&gt;%\n  ggplot(aes(fill = deaths_avg_per_100k)) +\n  geom_sf() +\n  scale_fill_viridis_c(option = \"rocket\") +\n  labs(fill = \"Deaths per\\n100,000 people\") +\n  theme_void()\n```\n\nThis code creates a most_recent data frame by joining the us_states geospatial data with the covid_data data frame before filtering to include only the most recent date. Then, it uses most_recent to create a map that shows deaths per 100,000 people.\nFinally, to make a chart that shows COVID death rates over time in the four states from the table, add the following:\n\n# COVID Death Rates Over Time\n\nThe following chart shows COVID death rates from the start of COVID in early 2020 until `r last_day`.\n\n```{r}\ncovid_data %&gt;%\n  filter(state %in% c(\n    \"Alabama\",\n    \"Alaska\",\n    \"Arizona\",\n    \"Arkansas\"\n  )) %&gt;%\n  ggplot(aes(\n    x = date,\n    y = deaths_avg_per_100k,\n    group = state,\n    fill = deaths_avg_per_100k\n  )) +\n  geom_col() +\n  scale_fill_viridis_c(option = \"rocket\") +\n  theme_minimal() +\n  labs(title = \"Deaths per 100,000 people over time\") +\n  theme(\n    legend.position = \"none\",\n    plot.title.position = \"plot\",\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    axis.title = element_blank()\n  ) +\n  facet_wrap(\n    ~state,\n    nrow = 2\n  )\n```\n\nUsing the geom_col() function, this code creates a faceted set of bar charts that show change over time by state (faceting was discussed in Chapter 2). Finally, it applies the rocket color palette, applies theme_minimal(), and makes a few tweaks to that theme. Figure 9.4 shows what the website’s home page looks like three years after the start of the pandemic.\n\n\n\n\n\n\n\nFigure 9.4: The COVID website with a table, map, and chart\n\n\n\n\nNow that you have some content in place, you can tweak it. For example, because many states are quite small, especially in the Northeast, it’s a bit challenging to see them. Let’s look at how to make the entire map bigger.\nApplying distill Layouts\nOne nice feature of distill is that it includes four layouts you can apply to a code chunk to widen its output: l-body-outset (creates output that is a bit wider than the default), l-page (creates output that is wider still), l-screen (creates full-screen output), and l-screen-inset (creates full-screen output with a bit of a buffer).\nApply l-screen-inset to the map by modifying the first line of its code chunk as follows:\n\n```{r layout = \"l-screen-inset\"}\n\nThis makes the map wider and taller and, as a result, much easier to read.\nMaking the Content Interactive\nThe content you’ve added to the website so far is all static; it has none of the interactivity typically seen in websites, which often use JavaScript to respond to user behavior. If you’re not proficient with HTML and JavaScript, you can use R packages like distill, plotly, and DT, which wrap JavaScript libraries, to add interactive elements like the graphics and maps Matt Herman uses on his Westchester County COVID website. Figure 9.5, for example, shows a tooltip that allows the user to see results for any single day.\n\n\n\n\n\n\n\nFigure 9.5: An interactive tooltip showing new COVID cases by day\n\n\n\n\nUsing the DT package, Herman also makes interactive tables that allow the user to scroll through the data and sort the values by clicking any variable in the header, as shown in Figure 9.6.\n\n\n\n\n\n\n\nFigure 9.6: An interactive table made with the DT package\n\n\n\n\nLet’s add some interactivity to our national COVID website. We’ll begin by making our table interactive.\nAdding Pagination to a Table with reactable\nRemember how you included only four states in the table to keep it from getting too long? By creating an interactive table, you can avoid this limitation. The reactable package is a great option for interactive tables. First, install it with install.packages(\"reactable\"). Then, swap out the gt package code you used to make your static table with the reactable() function to show all states:\n\nlibrary(reactable)\n\ncovid_data %&gt;%\n  slice_max(\n    order_by = date,\n    n = 1\n  ) %&gt;%\n  select(state, deaths_avg_per_100k) %&gt;%\n  arrange(state) %&gt;%\n  set_names(\"State\", \"Death rate\") %&gt;%\n  reactable()\n\nThe reactable package shows 10 rows by default and adds pagination, as shown in Figure 9.7.\n\n\n\n\n\n\n\nFigure 9.7: An interactive table built with reactable\n\n\n\n\nThe reactable() function also enables sorting by default. Although you used the arrange() function in your code to sort the data by state name, users can click the “Death rate” column to sort values using that variable instead.\nCreating a Hovering Tooltip with plotly\nNow you’ll add some interactivity to the website’s chart using the plotly package. First, install plotly with install.packages(\"plotly\"). Then, create a plot with ggplot and save it as an object. Pass the object to the ggplotly() function, which turns it into an interactive plot, and run the following code to apply plotly to the chart of COVID death rates over time:\n\nlibrary(plotly)\n\ncovid_chart &lt;- covid_data %&gt;%\n  filter(state %in% c(\n    \"Alabama\",\n    \"Alaska\",\n    \"Arizona\",\n    \"Arkansas\"\n  )) %&gt;%\n  ggplot(aes(\n    x = date,\n    y = deaths_avg_per_100k,\n    group = state,\n    fill = deaths_avg_per_100k\n  )) +\n  geom_col() +\n  scale_fill_viridis_c(option = \"rocket\") +\n  theme_minimal() +\n  labs(title = \"Deaths per 100,000 people over time\") +\n  theme(\n    legend.position = \"none\",\n    plot.title.position = \"plot\",\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    axis.title = element_blank()\n  ) +\n  facet_wrap(\n    ~state,\n    nrow = 2\n  )\n\nggplotly(covid_chart)\n\nThis is identical to the chart code shown earlier in this chapter, except that now you’re saving your chart as an object called covid_chart and then running ggplotly(covid_chart). This code produces an interactive chart that shows the data for a particular day when a user mouses over it. But the tooltip that pops up, shown in Figure 9.8, is cluttered and overwhelming because the ggplotly() function shows all data by default.\n\n\n\n\n\n\n\nFigure 9.8: The plotly default produces a messy tooltip.\n\n\n\n\nTo make the tooltip more informative, create a single variable containing the data you want to display and tell ggplotly() to use it:\n\ncovid_chart &lt;- covid_data %&gt;%\n  filter(state %in% c(\n    \"Alabama\",\n    \"Alaska\",\n    \"Arizona\",\n    \"Arkansas\"\n  )) %&gt;%\n  mutate(date_nice_format = str_glue(\"{month(date, label = TRUE, abbr = FALSE)} {day(date)}, {year(date)}\")) %&gt;%\n  mutate(tooltip_text = str_glue(\"{state}&lt;br&gt;{date_nice_format}&lt;br&gt;{deaths_avg_per_100k} per 100,000 people\")) %&gt;%\n  ggplot(aes(\n    x = date,\n    y = deaths_avg_per_100k,\n    group = state,\n    text = tooltip_text,\n    fill = deaths_avg_per_100k\n  )) +\n  geom_col() +\n  scale_fill_viridis_c(option = \"rocket\") +\n  theme_minimal() +\n  labs(title = \"Deaths per 100,000 people over time\") +\n  theme(\n    legend.position = \"none\",\n    plot.title.position = \"plot\",\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    axis.title = element_blank()\n  ) +\n  facet_wrap(\n    ~state,\n    nrow = 2\n  )\n\n\nggplotly(\n  covid_chart,\n  tooltip = \"tooltip_text\"\n)\n\nThis code begins by creating a date_nice_format variable that produces dates in the more readable format January 1, 2023, instead of 2023-01-01. This value is then combined with the state and death rate variables, and the result is saved as tooltip_text. Next, the code adds a new aesthetic property in the ggplot() function. This property doesn’t do anything until it’s passed to ggplotly().\nFigure 9.9 shows what the new tooltip looks like: it displays the name of the state, a nicely formatted date, and that day’s death rate.\n\n\n\n\n\n\n\nFigure 9.9: Easy-to-read interactive tooltips on the COVID-19 death rate chart\n\n\n\n\nAdding interactivity is a great way to take advantage of the website medium. Users who might feel overwhelmed looking at the static chart can explore the interactive version, mousing over areas to see a summary of the results on any single day.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Websites</span>"
    ]
  },
  {
    "objectID": "websites.html#hosting-the-website",
    "href": "websites.html#hosting-the-website",
    "title": "9  Websites",
    "section": "Hosting the Website",
    "text": "Hosting the Website\nNow that you’ve made a website, you need a way to share it. There are various ways to do this, ranging from simple to quite complex. The easiest solution is to compress the files in your docs folder (or whatever folder you put your rendered website in) and email your ZIP file to your recipients. They can unzip it and open the HTML files in their browser. This works fine if you know you won’t want to make changes to your website’s data or styles. But, as Chapter 6 discussed, most projects aren’t really one-time events.\nCloud Hosting\nA better approach is to put your entire docs folder in a place where others can see it. This could be an internal network, Dropbox, Google Drive, Box, or something similar. Hosting the files in the cloud this way is simple to implement and allows you to control who can see your website.\nYou can even automate the process of copying your docs folder to various online file-sharing sites using R packages: the rdrop2 package works with Dropbox, googledrive works with Google Drive, and boxr works with Box. For example, code like the following would automatically upload the project to Dropbox:\n\nlibrary(tidyverse)\nlibrary(rmarkdown)\nlibrary(fs)\nlibrary(rdrop2)\n\n# Render the website\nrender_site()\n\n# Upload to Dropbox\nwebsite_files &lt;- dir_ls(\n  path = \"docs\",\n  type = \"file\",\n  recurse = TRUE\n)\n\nwalk(website_files, drop_upload, path = \"COVID Website\")\n\nThis code, which I typically add to a separate file called render.R, renders the site, uses the dir_ls() function from the fs package to identify all files in the docs directory, and then uploads these files to Dropbox. Now you can run your entire file to generate and upload your website in one go.\nGitHub Hosting\nA more complicated yet powerful alternative to cloud hosting is to use a static hosting service like GitHub Pages. Each time you commit (take a snap-shot of) your code and push (sync) it to GitHub, this service deploys the website to a URL you’ve set up. Learning to use GitHub is an investment of time and effort (the self-published book Happy Git and GitHub for the useR by Jenny Bryan at https://happygitwithr.com is a great resource), but being able to host your website for free makes it worthwhile.\nHere’s how GitHub Pages works. Most of the time, when you look at a file on GitHub, you see its underlying source code, so if you looked at an HTML file, you’d see only the HTML code. GitHub Pages, on the other hand, shows you the rendered HTML files. To host your website on GitHub Pages, you’ll need to first push your code to GitHub. Once you have a repository set up there, go to it, then go to the Settings tab, which should look like Figure 9.10.\n\n\n\n\n\n\n\nFigure 9.10: Setting up GitHub Pages\n\n\n\n\nNow choose how you want GitHub to deploy the raw HTML. The easiest approach is to keep the default source. To do so, select Deploy from a branch and then select your default branch (usually main or master). Next, select the directory containing the HTML files you want to be rendered. If you configured your website for GitHub Pages at the beginning of this chapter, the files should be in docs. Click Save and wait a few minutes, and GitHub should show the URL where your website now lives.\nThe best part about hosting your website on GitHub Pages is that any time you update your code or data, the website will update as well. R Markdown, distill, and GitHub Pages make building and maintaining websites a snap.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Websites</span>"
    ]
  },
  {
    "objectID": "websites.html#summary",
    "href": "websites.html#summary",
    "title": "9  Websites",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you learned to use the distill package to make websites in R. This package provides a simple way to get a website up and running with the tool you’re already using for working with data. You’ve seen how to:\n\nCreate new pages and add them to your top navigation bar\nCustomize the look and feel of your website with tweaks to the CSS\nUse wider layouts to make content fit better on individual pages\nConvert static data visualization and tables into interactive versions\nUse GitHub Pages to host an always-up-to-date version of your website\n\nMatt Herman has continued building websites with R. He and his colleagues at the Council of State Governments Justice Center have made a great website using Quarto, the language-agnostic version of R Markdown. This website, found at https://projects.csgjusticecenter.org/tools-for-states-to-address-crime/, highlights crime trends throughout the United States using many of the same techniques you saw in this chapter.\nWhether you prefer distill or Quarto, using R is a quick way to develop complex websites without having to be a sophisticated frontend web developer. The websites look good and communicate well. They are one more example of how R can help you efficiently share your work with the world.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Websites</span>"
    ]
  },
  {
    "objectID": "websites.html#additional-resources",
    "href": "websites.html#additional-resources",
    "title": "9  Websites",
    "section": "Additional Resources",
    "text": "Additional Resources\nConsult the following resources to learn how to make websites with the distill package and to see examples of other websites made with distill:\n\nThe Distillery, “Welcome to the Distillery!,” accessed November 30, 2023, https://distillery.rbind.io.\nThomas Mock, “Building a Blog with distill,” The MockUp, August 1, 2020, https://themockup.blog/posts/2020-08-01-building-a-blog-with-distill/.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Websites</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "10  Quarto",
    "section": "",
    "text": "Creating a Quarto Document\nVersions of RStudio starting with 2022.07.1 come with Quarto installed. To check your RStudio version, click RStudio &gt; About RStudio in the top menu bar. If you have an older version of RStudio, update it now by reinstalling it, as outlined in Chapter 1. Quarto should then be installed for you.\nOnce you’ve installed Quarto, create a document by clicking File &gt; New File &gt; Quarto Document. You should see a menu, shown in Figure 10.1, that looks like the one used to create an R Markdown document.\nFigure 10.1: The RStudio menu for creating a new Quarto document\nGive your document a title and choose an output format. The Engine option allows you to select a different way to render documents. By default, it uses Knitr, the same rendering tool used by R Markdown. The Use Visual Markdown Editor option provides an interface that looks more like Microsoft Word, but it can be finicky, so I won’t cover it here.\nThe resulting Quarto document should contain default content, just as R Markdown documents do:\n---\ntitle: \"My Report\"\nformat: html\n---\n\n## Quarto\n\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see &lt;https://quarto.org&gt;.\n\n## Running Code\n\nWhen you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n```{r}\n1 + 1\n```\n\nYou can add options to executable code like this \n\n```{r}\n#| echo: false\n2 * 2\n```\n\nThe `echo: false` option disables the printing of code (only output is displayed).\nAlthough R Markdown and Quarto have many features in common, they also have some differences to be aware of.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#comparing-r-markdown-and-quarto",
    "href": "quarto.html#comparing-r-markdown-and-quarto",
    "title": "10  Quarto",
    "section": "Comparing R Markdown and Quarto",
    "text": "Comparing R Markdown and Quarto\nQuarto and R Markdown documents have the same basic structure — YAML metadata, followed by a combination of Markdown text and code chunks — but they have some variations in syntax.\nThe format and execute YAML Fields\nQuarto uses slightly different options in its YAML. It replaces the output field with the format field and uses the value html instead of html_document:\n\n---\ntitle: \"My Report\"\noutput: html_document\n---\n\nOther Quarto formats also use different names than their R Markdown counterparts: docx instead of word_document and pdf instead of pdf_document, for example. All of the possible formats can be found at https://quarto.org/docs/guide/.\nA second difference between R Markdown and Quarto syntax is that Quarto doesn’t use a setup code chunk to set default options for showing code, charts, and other elements in the rendered versions of the document. In Quarto, these options are set in the execute field of the YAML. For example, the following would hide code, as well as all warnings and messages, from the rendered document:\n\n---\ntitle: \"My Report\"\nformat: html\nexecute:\n  echo: false\n  warning: false\n  message: false\n---\n\nQuarto also allows you to write true and false in lowercase.\nIndividual Code Chunk Options\nIn R Markdown, you override options at the individual code chunk level by adding the new option within the curly brackets that start a code chunk. For example, the following would show both the code 2 * 2 and its output:\n\n```{r echo = TRUE}\n2 * 2\n```\n\nQuarto instead uses this syntax to set individual code chunk–level options:\n\n```{r}\n#| echo: false\n2 * 2\n```\n\nThe option is set within the code chunk itself. The characters #| (known as a hash pipe) at the start of a line indicate that you are setting options.\nDashes in Option Names\nAnother difference you’re likely to see if you switch from R Markdown to Quarto is that option names consisting of two words are separated by a dash rather than a period. R Markdown, for example, uses the code chunk option fig.height to specify the height of plots. In contrast, Quarto uses fig-height, as follows:\n\n```{r}\n#| fig-height: 10\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\nggplot(\n  penguins,\n  aes(\n    x = bill_length_mm,\n    y = bill_depth_mm\n  )\n) +\n  geom_point()\n```\n\nHelpfully for anyone coming from R Markdown, fig.height and similar options containing periods will continue to work if you forget to make the switch. A list of all code chunk options can be found on the Quarto website at https://quarto.org/docs/reference/cells/cells-knitr.html.\nThe Render Button\nYou can follow the same process to render your Quarto document as in R Markdown, but in Quarto the button is called Render rather than Knit. Clicking Render will turn the Quarto document into an HTML file, Word document, or any other output format you select.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#parameterized-reporting",
    "href": "quarto.html#parameterized-reporting",
    "title": "10  Quarto",
    "section": "Parameterized Reporting",
    "text": "Parameterized Reporting\nNow that you’ve learned a bit about how Quarto works, you’ll make a few different documents with it, starting with a parameterized report. The process of making parameterized reports with Quarto is nearly identical to doing so with R Markdown. In fact, you can adapt the R Markdown document you used to make the Urban Institute COVID report in Chapter 7 for Quarto simply by copying the .Rmd file, changing its extension to .qmd, and then making a few other changes:\n\n---\ntitle: \"Urban Institute COVID Report\"\nformat: html\nparams:\n  state: \"Alabama\"\nexecute:\n  echo: false\n  warning: false\n  message: false\n---\n\n```{r}\nlibrary(tidyverse)\nlibrary(urbnthemes)\nlibrary(here)\nlibrary(scales)\n```\n\n# `r params$state`\n\n```{r}\ncases &lt;- tibble(state.name) %&gt;%\n  rbind(state.name = \"District of Columbia\") %&gt;%\n  left_join(\n    read_csv(\"https://data.rfortherestofus.com/united_states_covid19_cases_deaths_and_testing_by_state.csv\", skip = 2),\n    by = c(\"state.name\" = \"State/Territory\")\n  ) %&gt;%\n  select(\n    total_cases = `Total Cases`,\n    state.name,\n    cases_per_100000 = `Case Rate per 100000`\n  ) %&gt;%\n  mutate(cases_per_100000 = parse_number(cases_per_100000)) %&gt;%\n  mutate(case_rank = rank(-cases_per_100000, ties.method = \"min\"))\n```\n\n```{r}\nstate_text &lt;- if_else(params$state == \"District of Columbia\", str_glue(\"the District of Columbia\"), str_glue(\"state of {params$state}\"))\n\nstate_cases_per_100000 &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(cases_per_100000) %&gt;%\n  comma()\n\nstate_cases_rank &lt;- cases %&gt;%\n  filter(state.name == params$state) %&gt;%\n  pull(case_rank)\n```\n\nIn `r state_text`, there were `r state_cases_per_100000` cases per 100,000 people in the last seven days. This puts `r params$state` at number `r state_cases_rank` of 50 states and the District of Columbia. \n\n```{r}\n#| fig-height: 8\n\nset_urbn_defaults(style = \"print\")\n\ncases %&gt;%\n  mutate(highlight_state = if_else(state.name == params$state, \"Y\", \"N\")) %&gt;%\n  mutate(state.name = fct_reorder(state.name, cases_per_100000)) %&gt;%\n  ggplot(aes(\n    x = cases_per_100000,\n    y = state.name,\n    fill = highlight_state\n  )) +\n  geom_col() +\n  scale_x_continuous(labels = comma_format()) +\n  theme(legend.position = \"none\") +\n  labs(\n    y = NULL,\n    x = \"Cases per 100,000\"\n  )\n```\n\nThis code switches output: html_document to format: html in the YAML, then removes the setup code chunk and sets those options in the YAML’s execute field. Finally, the fig.height option in the last code chunk is replaced with fig-height and labeled as an option with the hash pipe.\nNext, to create one report for each state, you must tweak the render.R script file you used to make parameterized reports in Chapter 7:\n\n# Load packages\nlibrary(tidyverse)\nlibrary(quarto)\n\n# Create a vector of all states and the District of Columbia\nstate &lt;- tibble(state.name) %&gt;%\n  rbind(\"District of Columbia\") %&gt;% \n  pull(state.name)\n\n# Create a tibble with information on the:\n# input R Markdown document\n# output HTML file\n# parameters needed to knit the document\nreports &lt;- tibble(\n  input = \"urban-covid-budget-report.qmd\",\n  output_file = str_glue(\"{state}.html\"),\n  execute_params = map(state, ~list(state = .))\n)\n\n# Generate all of our reports\nreports %&gt;%\n  pwalk(quarto_render)\n\nThis updated render.R file loads the quarto package instead of the rmarkdown package 1 and changes the input file to urban-covid-budget-report.qmd. The reports tibble uses execute_params instead of params because this is the argument that the quarto_render() function expects. To render the reports, the quarto_render() function replaces the render() function from the rmarkdown package. As in Chapter 7, running this code should produce a report for each state.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#making-presentations",
    "href": "quarto.html#making-presentations",
    "title": "10  Quarto",
    "section": "Making Presentations",
    "text": "Making Presentations\nQuarto can also produce slideshow presentations like those you made in Chapter 8 with the xaringan package. To make a presentation with Quarto, click File &gt; New File &gt; Quarto Presentation. Choose Reveal JS to make your slides and leave the Engine and Editor options untouched.\nThe slides you’ll make use the reveal.js JavaScript library under the hood, a technique similar to making slides with xaringan. The following code updates the presentation you made in Chapter 8 so that it works with Quarto:\n\n---\ntitle: \"Penguins Report\"\nauthor: \"David Keyes\"\nformat: revealjs\nexecute: \n  echo: false\n  warning: false\n  message: false\n---\n\n# Introduction\n\n```{r}\nlibrary(tidyverse)\n```\n\n```{r}\npenguins &lt;- read_csv(\"https://raw.githubusercontent.com/rfortherestofus/r-without-statistics/main/data/penguins-2008.csv\")\n```\n\nWe are writing a report about the **Palmer Penguins**. These penguins are *really* amazing. There are three species:\n\n- Adelie\n- Gentoo\n- Chinstrap\n\n## Bill Length\n\nWe can make a histogram to see the distribution of bill lengths.\n\n```{r}\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\n```{r}\naverage_bill_length &lt;- penguins %&gt;%\n  summarize(avg_bill_length = mean(\n    bill_length_mm,\n    na.rm = TRUE\n  )) %&gt;%\n  pull(avg_bill_length)\n```\n\nThe chart shows the distribution of bill lengths. The average bill length is `r average_bill_length` millimeters.\n\nThis code sets format: revealjs in the YAML to make a presentation and adds several global code chunk options in the execute section. It then removes the three dashes used to make slide breaks because firstor secondlevel headings make new slides in Quarto (though you could still use three dashes to manually add slide breaks). When you render this code, you should get an HTML file with your slides. The output should look similar to the default xaringan slides from Chapter 8.\nRevealing Content Incrementally\nQuarto slides can incrementally reveal content. To reveal bulleted and numbered lists one item at a time by default, add incremental: true to the document’s YAML like so:\n\n---\ntitle: \"Penguins Report\"\nauthor: \"David Keyes\"\nformat: \n  revealjs:\n      incremental: true\nexecute: \n  echo: false\n  warning: false\n  message: false\n---\n\nAs a result of this code, the content in all lists in the presentation should appear on the slide one item at a time.\nYou can also set just some lists to incrementally reveal using this format:\n\n::: {.incremental}\n- Adelie\n- Gentoo\n- Chinstrap\n:::\n\nUsing ::: to start and end a segment of the document creates a section in the resulting HTML file known as a div. The HTML &lt;div&gt; tag allows you to define properties within that section. In this code, adding {.incremental} sets a custom CSS class that displays the list incrementally.\nAligning Content and Adding Background Images\nYou can use a &lt;div&gt; tag to create columns in Quarto slides, too. Say you want to create a slide with content in two columns, as in Figure 10.2.\n\n\n\n\n\n\n\nFigure 10.2: Creating two columns with a &lt;div&gt; tag\n\n\n\n\nThe following code creates this two-column slide:\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n```{r}\npenguins %&gt;%\n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\n:::\n\n::: {.column width=\"50%\"}\n```{r}\npenguins %&gt;%\n  ggplot(aes(x = bill_depth_mm)) +\n  geom_histogram() +\n  theme_minimal()\n```\n\n:::\n\n::::\n\nNotice the :::, as well as ::::, which creates nested &lt;div&gt; sections. The columns class tells the HTML that all content within the :::: should be laid out as columns. Then, ::: {.column width=\"50%\"} starts a\n\nthat takes up half the width of the slide. The closing ::: and :::: indicate the end of the section.\nWhen using xaringan, you easily centered content on a slide by surrounding it with .center[]. Alignment in Quarto is slightly more complicated. Quarto has no built-in CSS class to center content, so you’ll need to create one yourself. Begin a CSS code chunk and a custom class called center-slide:\n\n```{css}\n.center-slide {\n    text-align: center;\n}\n```\n\nThis CSS center-aligns all content. (The text-align property aligns images, too, not just text.)\nTo apply the new center-slide class, put it next to the title of the slide, as follows:\n\n## Bill Length {.center-slide}\n\nWith the custom CSS applied, the slide should now center all content.\nFinally, when working in xaringan, you added a background image to a slide. To do the same thing in Quarto, apply the background-image attribute to a slide, like so:\n\n## Penguins {background-image=\"penguins.jpg\"}\n\nThis should add a slide with the text Penguins in front of the selected image.\n\nCustomizing Your Slides with Themes and CSS\nYou’ve started making some changes to the look and feel of the Quarto slides, but you can add even more customization to your design. As with xaringan, there are two main ways to further customize your slides in Quarto: using existing themes and changing the CSS.\nThemes are the easiest way to change your slide design. To apply a theme in Quarto, simply add its name to your YAML:\n\n---\ntitle: \"Penguins Report\"\nformat:\n  revealjs: \n    theme: dark\n---\n\nUsing this option should change the theme from light (the default) to dark. You can see the title slide with the dark theme applied in Figure 10.3. To see the full list of available themes, go to https://quarto.org/docs/presentations/revealjs/themes.html.\n\n\n\n\n\n\n\nFigure 10.3: A slide with the dark theme applied\n\n\n\n\nThe second option to change your slide design further is to write custom CSS. Quarto uses a type of CSS called Sass that lets you include variables in the CSS. These variables resemble those from the xaringanthemer package, which allowed you to set values for header formatting using header_h2_font_size and header_color.\nGo to File &gt; New File &gt; New Text File, create a Sass file called theme.scss, and add the following two mandatory sections:\n\n/*-- scss:defaults --*/\n\n/*-- scss:rules --*/\n\nThe scss:defaults section is where you use the Quarto Sass variables. For example, to change the color and size of first-level headers, add this code:\n\n/*-- scss:defaults --*/\n$presentation-heading-color: red;\n$presentation-h1-font-size: 150px;\n\n/*-- scss:rules --*/\n\nAll Quarto Sass variables start with a dollar sign, followed by a name. To apply these tweaks to your slides, adjust your YAML to tell Quarto to use the custom theme.scss file:\n\n---\ntitle: \"Penguins Reports\"\nformat:\n  revealjs: \n    theme: theme.scss\n---\n\nFigure 10.4 shows the changes applied to the rendered slides.\n\n\n\n\n\n\n\nFigure 10.4: A slide modified using custom CSS\n\n\n\n\nAll predefined variables should go in the scss:defaults section. You can find the full list of these variables at https://quarto.org/docs/presentations/revealjs/themes.html#sass-variables.\nThe scss:rules section is where you can add CSS tweaks for which there are no existing variables. For example, you could place the code you wrote to center the slide’s content in this section:\n\n/*-- scss:defaults --*/\n$presentation-heading-color: red;\n$presentation-h1-font-size: 150px;\n\n/*-- scss:rules --*/\n.center-slide {\n  text-align: center;\n}\n\nBecause rendered Quarto slides are HTML documents, you can tweak them however you’d like with custom CSS. What’s more, because the slides use reveal.js under the hood, any features built into that JavaScript library work in Quarto. This library includes easy ways to add transitions, animations, interactive content, and much more. The demo Quarto presentation available at https://quarto.org/docs/presentations/revealjs/demo/ shows many of these features in action.\nMaking Websites\nQuarto can make websites without requiring the use of an external package like distill. To create a Quarto website, go to File &gt; New Project. Select New Directory, then Quarto Website. You’ll be prompted to choose a directory in which to place your project. Keep the default engine (Knitr), check Create a Git Repository (which should show up only if you’ve already installed Git), and leave everything else unchecked.\nClick Create Project, which should create a series of files: index.qmd, about.qmd, _quarto.yml, and styles.css. These files resemble those created by the distill package. The .qmd files are where you’ll add content, the _quarto.yml file is where you’ll set options for the entire website, and the styles.css file is where you’ll add CSS to customize the website’s appearance.\nBuilding the Website\nYou’ll start by modifying the .qmd files. Open the home page file (index.qmd), delete the default content after the YAML, and replace it with the content from the website you made in Chapter 9. Remove the layout = \"l-page\" element, which you used to widen the layout. I’ll discuss how to change the page’s layout in Quarto later in this section.\nTo render a Quarto website, look for the Build tab in the top right of RStudio and click Render Website. The rendered website should now appear in the Viewer pane on the bottom-right pane of RStudio. If you navigate to the Files pane on the same panel, you should also see that a _site folder has been created to hold the content of the rendered site. Try opening the index.html file in your web browser. You should see the website in Figure 10.5.\n\n\n\n\n\n\n\nFigure 10.5: The Quarto website with warnings and messages\n\n\n\n\nAs you can see, the web page includes many warnings and messages that you don’t want to show. In R Markdown, you removed these in the setup code chunk; in Quarto, you can do so in the YAML. Add the following code to the index.qmd YAML to remove all code, warnings, and messages from the output:\n\nexecute: \n  echo: false\n  warning: false\n  message: false\n\nNote, however, that these options will make changes to only one file. Next, you’ll see how to set these options for the entire website.\nSetting Options\nWhen using distill, you modified the _site.yml file to make changes to all files in the website. In Quarto, you use the _quarto.yml file for the same purpose. If you open it, you should see three sections:\n\nproject:\n  type: website\n\nwebsite:\n  title: \"covid-website-quarto\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\n\nThe top section sets the project type (in this case, a website). The middle section defines the website’s title and determines the options for its navigation bar. The bottom section modifies the site’s appearance.\nYou’ll start from the bottom. To remove code, warnings, and messages for every page in the website, add the portion of the YAML you wrote earlier to the _quarto.yml file. The bottom section should now look like this:\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\nexecute: \n  echo: false\n  warning: false\n  message: false\n\nIf you build the website again, you should now see just the content, as in Figure 10.6.\n\n\n\n\n\n\n\nFigure 10.6: The website with warnings and messages removed\n\n\n\n\nIn this section of the _quarto.yml file, you can add any options you would otherwise place in a single .qmd file to apply them across all the pages of your website.\nChanging the Website’s Appearance\nThe format section of the _quarto.yml file determines the appearance of rendered files. By default, Quarto applies a theme called cosmo, but there are many themes available. (You can see the full list at https://quarto.org/docs/output-formats/html-themes.html.) To see how a different theme affects the output, make the following change:\n\nformat:\n  html:\n    theme: minty\n    css: styles.css\n    toc: true\n\nThe minty theme changes the website’s fonts and updates the color scheme to gray and light green.\nIn addition to using prebuilt themes, you can customize your website with CSS. The css: styles.css section in the _quarto.yml file indicates that Quarto will use any CSS in the styles.css file when rendering. Try adding the following CSS to styles.css to make first-level headers red and 50 pixels large:\n\nh1 {\n  color: red;\n  font-size: 50px;\n}\n\nThe re-rendered index.html now has large red headings (shown in Figure 10.7).\n\n\n\n\n\n\n\nFigure 10.7: The website with custom CSS applied\n\n\n\n\nAn alternative approach to customizing your website is to use Sass variables in a .scss file, as you did in your presentation. For example, create a file called styles.scss and add a line like this one to make the body background bright yellow:\n\n/*-- scss:defaults --*/\n$body-bg: yellow;\n\nTo get Quarto to use the styles.scss file, adjust the theme line as follows:\n\nformat:\n  html:\n    theme: [minty, styles.scss]\n    css: styles.css\n    toc: true\n\nThis syntax tells Quarto to use the minty theme, then make additional tweaks based on the styles.scss file. If you render the website again, you should see the bright yellow background throughout (Figure 10.8).\n\n\n\n\n\n\n\nFigure 10.8: The website with custom CSS applied through styles.scss\n\n\n\n\nNote that when you add a .scss file, the tweaks made in styles.css no longer apply. If you wanted to use those, you’d need to add them to the styles.scss file.\nThe line toc: true creates a table of contents on the right side of the web pages (which you can see in Figure 10.5 and Figure 10.7). You can remove the table of contents by changing true to false. Add any further options, such as figure height, to the bottom section of the _quarto.yml file.\nAdjusting the Title and Navigation Bar\nThe middle section of the _quarto.yml file sets the website’s title and navigation. Change the title and the text for the About page link as follows:\n\nwebsite:\n  title: \"Quarto COVID Website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: about.qmd\n        text: About this Website\n\nChanging the title requires adjusting the title line. The navbar section functions nearly identically to how it does with distill. The href line lists the files the navigation bar should link to. The optional text line specifies the text that should show up for that link. Figure 10.9 shows these changes applied to the website.\n\n\n\n\n\n\n\nFigure 10.9: The changes to the navigation bar\n\n\n\n\nThe title on the home page is still covid-website-quarto, but you could change this in the index.qmd file.\nCreating Wider Layouts\nWhen you created a website with distill, you used the line layout = \"l-page\" to widen the map on the web page. You can accomplish the same result with Quarto by using the ::: syntax to add HTML &lt;div&gt; tags:\n\n:::{.column-screen-inset}\n```{r}\n#| out-width: 100%\n# Make map\n\nmost_recent &lt;- us_states %&gt;%\n  left_join(covid_data, by = \"state\") %&gt;%\n  slice_max(\n    order_by = date,\n    n = 1\n  )\n\nmost_recent %&gt;%\n  ggplot(aes(fill = deaths_avg_per_100k)) +\n  geom_sf() +\n  scale_fill_viridis_c(option = \"rocket\") +\n  labs(fill = \"Deaths per\\n100,000 people\") +\n  theme_void()\n```\n:::\n\nThis code adds :::{.column-screen-inset} to the beginning of the mapmaking code chunk and ::: to the end of it. This code chunk now also includes the line #| out-width: 100% to specify that the map should take up all of the available width. Without this line, the map would take up only a portion of the window. There are a number of different output widths you can use; see the full list at https://quarto.org/docs/authoring/article-layout.html.\nHosting Your Website on GitHub Pages and Quarto Pub\nYou can host your Quarto website using GitHub Pages, just as you did with your distill website. Recall that GitHub Pages requires you to save the website’s files in the docs folder. Change the _quarto.yml file so that the site outputs to this folder:\n\nproject:\n  type: website\n  output-dir: docs\n\nNow, when you render the site, the HTML and other files should show up in the docs directory. At this point, you can push your repository to GitHub, adjust the GitHub Pages settings as you did in Chapter 9, and see the URL at which your Quarto website will live.\nAs an alternative to GitHub Pages, Quarto has a free service called Quarto Pub that makes it easy to get your materials online. If you’re not a GitHub user, this is a great way to publish your work. To see how it works, you’ll publish the website you just made to it. Click the Terminal tab on the bottom-left pane of RStudio. At the prompt, enter quarto publish. This should bring up a list of ways you can publish your website, as shown in Figure 10.10.\n\n\n\n\n\n\n\nFigure 10.10: The list of providers to publish your Quarto website\n\n\n\n\nPress Enter to select Quarto Pub. You’ll then be asked to authorize RStudio to publish to Quarto Pub. Enter Y to do so, which should take you to https://quartopub.com. Sign up for an account (or sign in if you already have one). You should see a screen indicating that you have successfully signed in and authorized RStudio to connect with Quarto Pub. From there, you can return to RStudio, which should prompt you to select a name for your website. The easiest option is to use your project’s name. Once you enter the name, Quarto Pub should publish the site and take you to it, as shown in Figure 10.11.\n\n\n\n\n\n\n\nFigure 10.11: The website published on Quarto Pub\n\n\n\n\nWhen you make updates to your site, you can republish it to Quarto Pub using the same steps. Quarto Pub is probably the easiest way to publish HTML files made with Quarto.\nSummary\nAs you’ve seen in this chapter, you can do everything you did in R Markdown using Quarto, without loading any external packages. In addition, Quarto’s different output formats use a more consistent syntax. For example, because you can make new slides in Quarto by adding firstor second-level headers, the Quarto documents you use to create reports should translate easily to presentations.\nYou’re probably wondering at this point whether you should use R Markdown or Quarto. It’s a good question, and one many in the R community are thinking about. R Markdown isn’t going away, so if you already use it, you don’t need to switch. If you’re new to R, however, you may be a good candidate for Quarto, as its future features may not be backported to R Markdown.\nUltimately, the differences between R Markdown and Quarto are relatively small, and the impact of switching between tools should be minor. Both R Markdown and Quarto can help you become more efficient, avoid manual errors, and share results in a wide variety of formats.\nAdditional Resources\nConsult the following resources to learn the fundamentals of Quarto:\n\nAndrew Bray, Rebecca Barter, Silvia Canelón, Christophe Dervieu, Devin Pastor, and Tatsu Shigeta, “From R Markdown to Quarto,” workshop materials from rstudio::conf 2022, Washington, DC, July 25–26, 2022, https://rstudio-conf-2022.github.io/rmd-to-quarto/.\nTom Mock, “Getting Started with Quarto,” online course, accessed December 1, 2023, https://jthomasmock.github.io/quarto-in-two-hours/.\n\n\n\n\n      \n         9  Websites\n                \n  \n  \n      \n        11  Automatically Accesing Online Data",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#making-websites",
    "href": "quarto.html#making-websites",
    "title": "10  Quarto",
    "section": "Making Websites",
    "text": "Making Websites\nQuarto can make websites without requiring the use of an external package like distill. To create a Quarto website, go to File &gt; New Project. Select New Directory, then Quarto Website. You’ll be prompted to choose a directory in which to place your project. Keep the default engine (Knitr), check Create a Git Repository (which should show up only if you’ve already installed Git), and leave everything else unchecked.\nClick Create Project, which should create a series of files: index.qmd, about.qmd, _quarto.yml, and styles.css. These files resemble those created by the distill package. The .qmd files are where you’ll add content, the _quarto.yml file is where you’ll set options for the entire website, and the styles.css file is where you’ll add CSS to customize the website’s appearance.\nBuilding the Website\nYou’ll start by modifying the .qmd files. Open the home page file (index.qmd), delete the default content after the YAML, and replace it with the content from the website you made in Chapter 9. Remove the layout = \"l-page\" element, which you used to widen the layout. I’ll discuss how to change the page’s layout in Quarto later in this section.\nTo render a Quarto website, look for the Build tab in the top right of RStudio and click Render Website. The rendered website should now appear in the Viewer pane on the bottom-right pane of RStudio. If you navigate to the Files pane on the same panel, you should also see that a _site folder has been created to hold the content of the rendered site. Try opening the index.html file in your web browser. You should see the website in Figure 10.5.\n\n\n\n\n\n\n\nFigure 10.5: The Quarto website with warnings and messages\n\n\n\n\nAs you can see, the web page includes many warnings and messages that you don’t want to show. In R Markdown, you removed these in the setup code chunk; in Quarto, you can do so in the YAML. Add the following code to the index.qmd YAML to remove all code, warnings, and messages from the output:\n\nexecute: \n  echo: false\n  warning: false\n  message: false\n\nNote, however, that these options will make changes to only one file. Next, you’ll see how to set these options for the entire website.\nSetting Options\nWhen using distill, you modified the _site.yml file to make changes to all files in the website. In Quarto, you use the _quarto.yml file for the same purpose. If you open it, you should see three sections:\n\nproject:\n  type: website\n\nwebsite:\n  title: \"covid-website-quarto\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\n\nThe top section sets the project type (in this case, a website). The middle section defines the website’s title and determines the options for its navigation bar. The bottom section modifies the site’s appearance.\nYou’ll start from the bottom. To remove code, warnings, and messages for every page in the website, add the portion of the YAML you wrote earlier to the _quarto.yml file. The bottom section should now look like this:\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\nexecute: \n  echo: false\n  warning: false\n  message: false\n\nIf you build the website again, you should now see just the content, as in Figure 10.6.\n\n\n\n\n\n\n\nFigure 10.6: The website with warnings and messages removed\n\n\n\n\nIn this section of the _quarto.yml file, you can add any options you would otherwise place in a single .qmd file to apply them across all the pages of your website.\nChanging the Website’s Appearance\nThe format section of the _quarto.yml file determines the appearance of rendered files. By default, Quarto applies a theme called cosmo, but there are many themes available. (You can see the full list at https://quarto.org/docs/output-formats/html-themes.html.) To see how a different theme affects the output, make the following change:\n\nformat:\n  html:\n    theme: minty\n    css: styles.css\n    toc: true\n\nThe minty theme changes the website’s fonts and updates the color scheme to gray and light green.\nIn addition to using prebuilt themes, you can customize your website with CSS. The css: styles.css section in the _quarto.yml file indicates that Quarto will use any CSS in the styles.css file when rendering. Try adding the following CSS to styles.css to make first-level headers red and 50 pixels large:\n\nh1 {\n  color: red;\n  font-size: 50px;\n}\n\nThe re-rendered index.html now has large red headings (shown in Figure 10.7).\n\n\n\n\n\n\n\nFigure 10.7: The website with custom CSS applied\n\n\n\n\nAn alternative approach to customizing your website is to use Sass variables in a .scss file, as you did in your presentation. For example, create a file called styles.scss and add a line like this one to make the body background bright yellow:\n\n/*-- scss:defaults --*/\n$body-bg: yellow;\n\nTo get Quarto to use the styles.scss file, adjust the theme line as follows:\n\nformat:\n  html:\n    theme: [minty, styles.scss]\n    css: styles.css\n    toc: true\n\nThis syntax tells Quarto to use the minty theme, then make additional tweaks based on the styles.scss file. If you render the website again, you should see the bright yellow background throughout (Figure 10.8).\n\n\n\n\n\n\n\nFigure 10.8: The website with custom CSS applied through styles.scss\n\n\n\n\nNote that when you add a .scss file, the tweaks made in styles.css no longer apply. If you wanted to use those, you’d need to add them to the styles.scss file.\nThe line toc: true creates a table of contents on the right side of the web pages (which you can see in Figure 10.5 and Figure 10.7). You can remove the table of contents by changing true to false. Add any further options, such as figure height, to the bottom section of the _quarto.yml file.\nAdjusting the Title and Navigation Bar\nThe middle section of the _quarto.yml file sets the website’s title and navigation. Change the title and the text for the About page link as follows:\n\nwebsite:\n  title: \"Quarto COVID Website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: about.qmd\n        text: About this Website\n\nChanging the title requires adjusting the title line. The navbar section functions nearly identically to how it does with distill. The href line lists the files the navigation bar should link to. The optional text line specifies the text that should show up for that link. Figure 10.9 shows these changes applied to the website.\n\n\n\n\n\n\n\nFigure 10.9: The changes to the navigation bar\n\n\n\n\nThe title on the home page is still covid-website-quarto, but you could change this in the index.qmd file.\nCreating Wider Layouts\nWhen you created a website with distill, you used the line layout = \"l-page\" to widen the map on the web page. You can accomplish the same result with Quarto by using the ::: syntax to add HTML &lt;div&gt; tags:\n\n:::{.column-screen-inset}\n```{r}\n#| out-width: 100%\n# Make map\n\nmost_recent &lt;- us_states %&gt;%\n  left_join(covid_data, by = \"state\") %&gt;%\n  slice_max(\n    order_by = date,\n    n = 1\n  )\n\nmost_recent %&gt;%\n  ggplot(aes(fill = deaths_avg_per_100k)) +\n  geom_sf() +\n  scale_fill_viridis_c(option = \"rocket\") +\n  labs(fill = \"Deaths per\\n100,000 people\") +\n  theme_void()\n```\n:::\n\nThis code adds :::{.column-screen-inset} to the beginning of the mapmaking code chunk and ::: to the end of it. This code chunk now also includes the line #| out-width: 100% to specify that the map should take up all of the available width. Without this line, the map would take up only a portion of the window. There are a number of different output widths you can use; see the full list at https://quarto.org/docs/authoring/article-layout.html.\nHosting Your Website on GitHub Pages and Quarto Pub\nYou can host your Quarto website using GitHub Pages, just as you did with your distill website. Recall that GitHub Pages requires you to save the website’s files in the docs folder. Change the _quarto.yml file so that the site outputs to this folder:\n\nproject:\n  type: website\n  output-dir: docs\n\nNow, when you render the site, the HTML and other files should show up in the docs directory. At this point, you can push your repository to GitHub, adjust the GitHub Pages settings as you did in Chapter 9, and see the URL at which your Quarto website will live.\nAs an alternative to GitHub Pages, Quarto has a free service called Quarto Pub that makes it easy to get your materials online. If you’re not a GitHub user, this is a great way to publish your work. To see how it works, you’ll publish the website you just made to it. Click the Terminal tab on the bottom-left pane of RStudio. At the prompt, enter quarto publish. This should bring up a list of ways you can publish your website, as shown in Figure 10.10.\n\n\n\n\n\n\n\nFigure 10.10: The list of providers to publish your Quarto website\n\n\n\n\nPress Enter to select Quarto Pub. You’ll then be asked to authorize RStudio to publish to Quarto Pub. Enter Y to do so, which should take you to https://quartopub.com. Sign up for an account (or sign in if you already have one). You should see a screen indicating that you have successfully signed in and authorized RStudio to connect with Quarto Pub. From there, you can return to RStudio, which should prompt you to select a name for your website. The easiest option is to use your project’s name. Once you enter the name, Quarto Pub should publish the site and take you to it, as shown in Figure 10.11.\n\n\n\n\n\n\n\nFigure 10.11: The website published on Quarto Pub\n\n\n\n\nWhen you make updates to your site, you can republish it to Quarto Pub using the same steps. Quarto Pub is probably the easiest way to publish HTML files made with Quarto.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#summary",
    "href": "quarto.html#summary",
    "title": "10  Quarto",
    "section": "Summary",
    "text": "Summary\nAs you’ve seen in this chapter, you can do everything you did in R Markdown using Quarto, without loading any external packages. In addition, Quarto’s different output formats use a more consistent syntax. For example, because you can make new slides in Quarto by adding firstor second-level headers, the Quarto documents you use to create reports should translate easily to presentations.\nYou’re probably wondering at this point whether you should use R Markdown or Quarto. It’s a good question, and one many in the R community are thinking about. R Markdown isn’t going away, so if you already use it, you don’t need to switch. If you’re new to R, however, you may be a good candidate for Quarto, as its future features may not be backported to R Markdown.\nUltimately, the differences between R Markdown and Quarto are relatively small, and the impact of switching between tools should be minor. Both R Markdown and Quarto can help you become more efficient, avoid manual errors, and share results in a wide variety of formats.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#additional-resources",
    "href": "quarto.html#additional-resources",
    "title": "10  Quarto",
    "section": "Additional Resources",
    "text": "Additional Resources\nConsult the following resources to learn the fundamentals of Quarto:\n\nAndrew Bray, Rebecca Barter, Silvia Canelón, Christophe Dervieu, Devin Pastor, and Tatsu Shigeta, “From R Markdown to Quarto,” workshop materials from rstudio::conf 2022, Washington, DC, July 25–26, 2022, https://rstudio-conf-2022.github.io/rmd-to-quarto/.\nTom Mock, “Getting Started with Quarto,” online course, accessed December 1, 2023, https://jthomasmock.github.io/quarto-in-two-hours/.",
    "crumbs": [
      "Part II: Reports, Presentations, and Websites",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "accessing-data.html",
    "href": "accessing-data.html",
    "title": "11  Automatically Accesing Online Data",
    "section": "",
    "text": "Importing Data from Google Sheets with googlesheets4\nBy using the googlesheets4 package to access data directly from Google Sheets, you avoid having to manually download data, copy it into your project, and adjust your code so it imports that new data every time you want to update a report. This package lets you write code that automatically fetches new data directly from Google Sheets. Whenever you need to update your report, you can simply run your code to refresh the data. In addition, if you work with Google Forms, you can pipe your data into Google Sheets, completely automating the workflow from data collection to data import.\nUsing the googlesheets4 package can help you manage complex datasets that update frequently. For example, in her role at the Primary Care Research Institute at the University of Buffalo, Meghan Harris used it for a research project about people affected by opioid use disorder. The data came from a variety of surveys, all of which fed into a jumble of Google Sheets. Using googlesheets4, Harris was able to collect all of her data in one place and use R to put it to use. Data that had once been largely unused because accessing it was so complicated could now inform research on opioid use disorder.\nThis section demonstrates how the googlesheets4 package works using a fake dataset about video game preferences that Harris created to replace her opioid survey data (which, for obvious reasons, is confidential).",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatically Accesing Online Data</span>"
    ]
  },
  {
    "objectID": "accessing-data.html#importing-data-from-google-sheets-with-googlesheets4",
    "href": "accessing-data.html#importing-data-from-google-sheets-with-googlesheets4",
    "title": "11  Automatically Accesing Online Data",
    "section": "",
    "text": "Connecting to Google\nTo begin, install the googlesheets4 package by running install.packages (\"googlesheets4\"). Next, connect to your Google account by running the gs4_auth() function in the console. If you have more than one Google account, select the account that has access to the Google Sheet you want to work with.\nOnce you do so, a screen should appear. Check the box next to See, Edit, Create, and Delete All Your Google Sheets Spreadsheets. This will ensure that R can access data from your Google Sheets account. Click Continue, and you should see the message “Authentication complete. Please close this page and return to R.” The googlesheets4 package will now save your credentials so that you can use them in the future without having to reauthenticate.\nReading Data from a Sheet\nNow that you’ve connected R to your Google account, you can import the fake data that Harris created about video game preferences (access it at https://data.rfortherestofus.com/google-sheet). Figure 11.1 shows what it looks like in Google Sheets.\n\n\n\n\n\n\n\nFigure 11.1: The video game data in Google Sheets\n\n\n\n\nThe googlesheets4 package has a function called read_sheet() that allows you to pull in data directly from a Google Sheet. Import the data by passing the spreadsheet’s URL to the function like so:\n\nlibrary(googlesheets4)\n\nsurvey_data_raw &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1AR0_RcFBg8wdiY4Cj-k8vRypp_txh27MyZuiRdqScog/edit?usp=sharing\")\n\nTake a look at the survey_data_raw object to confirm that the data was imported. Using the glimpse() function from the dplyr package makes it easier to read:\n\nlibrary(tidyverse)\n\nsurvey_data_raw %&gt;%\n  glimpse()\n\nThe glimpse() function, which creates one output row per variable, shows that you’ve successfully imported the data directly from Google Sheets:\n\n\nRows: 5\nColumns: 5\n$ Timestamp                         &lt;dttm&gt; 2022-05-16 15:20:50, 2022-05-16 15:…\n$ `How old are you?`                &lt;chr&gt; \"25-34\", \"45-54\", \"Under 18\", \"Over …\n$ `Do you like to play video games` &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\"\n$ `What kind of games do you like?` &lt;chr&gt; \"Sandbox, Role-Playing (RPG), Simula…\n$ `What's your favorite game?`      &lt;chr&gt; \"It's hard to choose. House Flipper …\n\n\nOnce you have the data in R, you can use the same workflow you’ve been using to create reports with R Markdown.\nUsing the Data in R Markdown\nThe following code is taken from an R Markdown report that Harris made to summarize the video games data. You can see the YAML, the setup code chunk, a code chunk that loads packages, and the code to import data from Google Sheets:\n\n---\ntitle: \"Video Game Survey\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo = FALSE,\n  warning = FALSE,\n  message = FALSE\n)\n```\n\n```{r}\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(googlesheets4)\nlibrary(gt)\n```\n\n```{r}\n# Import data from Google Sheets\nsurvey_data_raw &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1AR0_RcFBg8wdiY4Cj-k8vRypp_txh27MyZuiRdqScog/edit?usp=sharing\")\n```\n\nThis R Markdown document resembles those discussed in previous chapters, except for the way you import the data. Because you’re bringing it in directly from Google Sheets, there’s no risk of, say, accidentally reading in the wrong CSV. Automating this step reduces the risk of error.\nThe next code chunk cleans the survey_data_raw object, saving the result as survey_data_clean:\n\n```{r}\n# Clean data\nsurvey_data_clean &lt;- survey_data_raw %&gt;%\n  clean_names() %&gt;%\n  mutate(participant_id = as.character(row_number())) %&gt;%\n  rename(\n    age = how_old_are_you,\n    like_games = do_you_like_to_play_video_games,\n    game_types = what_kind_of_games_do_you_like,\n    favorite_game = whats_your_favorite_game\n  ) %&gt;%\n  relocate(participant_id, .before = age) %&gt;%\n  mutate(age = factor(age, levels = c(\"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"Over 65\")))\n```\n\nHere, the clean_names() function from the janitor package makes the variable names easier to work with. Defining a participant_id variable using the row_number() function then adds a consecutively increasing number to each row, and the as.character() function makes the number a character. Next, the code changes several variable names with the rename() function. The mutate() function then transforms the age variable into a data structure known as a factor, which ensures that age will show up in the right order in your chart. Finally, the relocate() function positions participant_id before the age variable.\nNow you can use the glimpse() function again to view your updated survey_data_clean data frame, which looks like this:\n\n\nRows: 5\nColumns: 6\n$ timestamp      &lt;dttm&gt; 2022-05-16 15:20:50, 2022-05-16 15:21:28, 2022-05-16 1…\n$ participant_id &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\"\n$ age            &lt;fct&gt; 25-34, 45-54, Under 18, Over 65, Under 18\n$ like_games     &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\"\n$ game_types     &lt;chr&gt; \"Sandbox, Role-Playing (RPG), Simulation and sports, Pu…\n$ favorite_game  &lt;chr&gt; \"It's hard to choose. House Flipper and Parkitect are m…\n\n\nThe rest of the report uses this data to highlight various statistics:\n\n# Respondent Demographics\n\n```{r}\n# Calculate number of respondents\nnumber_of_respondents &lt;- nrow(survey_data_clean)\n```\n\nWe received responses from `r number_of_respondents` respondents. Their ages are below.\n\n```{r}\nsurvey_data_clean %&gt;%\n  select(participant_id, age) %&gt;%\n  gt() %&gt;%\n  cols_label(\n    participant_id = \"Participant ID\",\n    age = \"Age\"\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels()\n  ) %&gt;%\n  cols_align(\n    align = \"left\",\n    columns = everything()\n  ) %&gt;%\n  cols_width(\n    participant_id ~ px(200),\n    age ~ px(700)\n  )\n```\n\n# Video Games\n\nWe asked if respondents liked video games. Their responses are below.\n\n```{r}\nsurvey_data_clean %&gt;%\n  count(like_games) %&gt;%\n  ggplot(aes(\n    x = like_games,\n    y = n,\n    fill = like_games\n  )) +\n  geom_col() +\n  scale_fill_manual(values = c(\n    \"No\" = \"#6cabdd\",\n    \"Yes\" = \"#ff7400\"\n  )) +\n  labs(\n    title = \"How Many People Like Video Games?\",\n    x = NULL,\n    y = \"Number of Participants\"\n  ) +\n  theme_minimal(base_size = 16) +\n  theme(\n    legend.position = \"none\",\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    axis.title.y = element_blank(),\n    plot.title = element_text(\n      face = \"bold\",\n      hjust = 0.5\n    )\n  )\n```\n\nThese sections calculate the number of survey respondents, then put this value in the text using inline R code; create a table that breaks down the respondents by age group; and generate a graph displaying how many respondents like video games. Figure 11.2 shows the resulting report.\n\n\n\n\n\n\n\nFigure 11.2: The rendered video game report\n\n\n\n\nYou can rerun the code at any point to fetch updated data. The survey had five responses today, but if you run it again tomorrow and it has additional responses, they will be included in the import. If you used Google Forms to run your survey and saved the results to a Google Sheet, you could produce this up-to-date report simply by clicking the Knit button in RStudio.\nImporting Only Certain Columns\nIn the previous sections, you read the data of the entire Google Sheet, but you also have the option to import only a section of a sheet. For example, the survey data includes a timestamp column. This variable is added automatically whenever someone submits a Google Form that pipes data into a Google Sheet, but you don’t use it in your analysis, so you could get rid of it.\nTo do so, use the range argument in the read_sheet() function when importing the data like so:\n\nread_sheet(\n  \"https://docs.google.com/spreadsheets/d/1AR0_RcFBg8wdiY4Cj-k8vRypp_txh27MyZuiRdqScog/edit?usp=sharing\",\n  range = \"Sheet1!B:E\"\n) %&gt;%\n  glimpse()\n\nThis argument lets you specify a range of data to import. It uses the same syntax you may have used to select columns in Google Sheets. In this example, range = \"Sheet1!B:E\" imports columns B through E (but not A, which contains the timestamp). Adding glimpse() and then running this code produces output without the timestamp variable:\n\n\nRows: 5\nColumns: 4\n$ `How old are you?`                &lt;chr&gt; \"25-34\", \"45-54\", \"Under 18\", \"Over …\n$ `Do you like to play video games` &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\"\n$ `What kind of games do you like?` &lt;chr&gt; \"Sandbox, Role-Playing (RPG), Simula…\n$ `What's your favorite game?`      &lt;chr&gt; \"It's hard to choose. House Flipper …\n\n\nThere are a number of other useful functions in the googlesheets4 package. For example, if you ever need to write your output back to a Google Sheet, the write_sheet() function is there to help. To explore other functions in the package, check out its documentation website at https://googlesheets4.tidyverse.org/index.html.\nNow we’ll turn our attention to another R package that allows you to automatically fetch data, this time from the US Census Bureau.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatically Accesing Online Data</span>"
    ]
  },
  {
    "objectID": "accessing-data.html#accessing-census-data-with-tidycensus",
    "href": "accessing-data.html#accessing-census-data-with-tidycensus",
    "title": "11  Automatically Accesing Online Data",
    "section": "Accessing Census Data with tidycensus",
    "text": "Accessing Census Data with tidycensus\nIf you’ve ever worked with data from the US Census Bureau, you know what a hassle it can be. Usually, the process involves visiting the Census Bureau website, searching for the data you need, downloading it, and then analyzing it in your tool of choice. This pointing and clicking gets very tedious after a while.\nKyle Walker, a geographer at Texas Christian University, and Matt Herman (creator of the Westchester COVID-19 website discussed in Chapter 9) developed the tidycensus package to automate the process of importing Census Bureau data into R. With tidycensus, you can write just a few lines of code to get data about, say, the median income in all counties in the United States.\nIn this section, I’ll show you how the tidycensus package works using examples from two datasets to which it provides access: the Decennial Census, administered every 10 years, and the annual American Community Survey. I’ll also show you how to use the data from these two sources to perform additional analysis and make maps by accessing geospatial and demographic data simultaneously.\nConnecting to the Census Bureau with an API Key\nBegin by installing tidycensus using install.packages(\"tidycensus\"). To use tidycensus, you must get an application programming interface (API) key from the Census Bureau. API keys are like passwords that online services use to determine whether you’re authorized to access data.\nTo obtain this key, which is free, go to https://api.census.gov/data/key_signup.html and enter your details. Once you receive the key by email, you need to put it in a place where tidycensus can find it. The census_api_key() function does this for you, so after loading the tidycensus package, run the function as follows, replacing 123456789 with your actual API key:\n\nlibrary(tidycensus)\n\ncensus_api_key(\"123456789\", install = TRUE)\n\nThe install = TRUE argument saves your API key in your .Renviron file, which is designed for storing confidential information. The package will look for your API key there in the future so that you don’t have to reenter it every time you use the package.\nNow you can use tidycensus to access Census Bureau datasets. While the Decennial Census and the American Community Survey are the most common, Chapter 2 of Kyle Walker’s book Analyzing US Census Data: Methods, Maps, and Models in R discusses others you can access.\nWorking with Decennial Census Data\nThe tidycensus packages includes several functions dedicated to specific Census Bureau datasets, such as get_decennial() for Decennial Census data. To access data from the 2020 Decennial Census about the Asian population in each state, use the get_decennial() function with three arguments as follows:\n\nget_decennial(\n  geography = \"state\",\n  variables = \"P1_006N\",\n  year = 2020\n)\n\nSetting the geography argument to “state” tells get_decennial() to access data at the state level. In addition to the 50 states, it will return data for the District of Columbia and Puerto Rico. The variables argument specifies the variable or variables you want to access. Here, P1_006N is the variable name for the total Asian population. I’ll discuss how to identify other variables you may want to use in the next section. Finally, year specifies the year for which you want to access data — in this case, 2020.\nRunning this code returns the following:\n\n\n# A tibble: 52 × 4\n   GEOID NAME                 variable   value\n   &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;      &lt;dbl&gt;\n 1 42    Pennsylvania         P1_006N   510501\n 2 06    California           P1_006N  6085947\n 3 54    West Virginia        P1_006N    15109\n 4 49    Utah                 P1_006N    80438\n 5 36    New York             P1_006N  1933127\n 6 11    District of Columbia P1_006N    33545\n 7 02    Alaska               P1_006N    44032\n 8 12    Florida              P1_006N   643682\n 9 45    South Carolina       P1_006N    90466\n10 38    North Dakota         P1_006N    13213\n# ℹ 42 more rows\n\n\nThe resulting data frame has four variables. GEOID is the geographic identifier assigned to the state by the Census Bureau. Each state has a geographic identifier, as do all counties, census tracts, and other geographies. NAME is the name of each state, and variable is the name of the variable you passed to the get_decennial() function. Finally, value is the numeric value for the state and variable in each row. In this case, it represents the total Asian population in each state.\nIdentifying Census Variable Values\nYou’ve just seen how to retrieve the total number of Asian residents of each state, but say you want to calculate that number instead as a percentage of all the state’s residents. To do that, first you need to retrieve the variable for the state’s total population.\nThe tidycensus package has a function called load_variables() that shows all of the variables from a Decennial Census. Run it with the year argument set to 2020 and dataset set to pl as follows:\n\nload_variables(\n  year = 2020,\n  dataset = \"pl\"\n)\n\nRunning this code pulls data from so-called redistricting summary data files (which Public Law 94-171 requires the Census Bureau to produce every 10 years) and returns the name, label (description), and concept (category) of all available variables:\n\n\n# A tibble: 301 × 3\n   name    label                                                         concept\n   &lt;chr&gt;   &lt;chr&gt;                                                         &lt;chr&gt;  \n 1 H1_001N \" !!Total:\"                                                   OCCUPA…\n 2 H1_002N \" !!Total:!!Occupied\"                                         OCCUPA…\n 3 H1_003N \" !!Total:!!Vacant\"                                           OCCUPA…\n 4 P1_001N \" !!Total:\"                                                   RACE   \n 5 P1_002N \" !!Total:!!Population of one race:\"                          RACE   \n 6 P1_003N \" !!Total:!!Population of one race:!!White alone\"             RACE   \n 7 P1_004N \" !!Total:!!Population of one race:!!Black or African Americ… RACE   \n 8 P1_005N \" !!Total:!!Population of one race:!!American Indian and Ala… RACE   \n 9 P1_006N \" !!Total:!!Population of one race:!!Asian alone\"             RACE   \n10 P1_007N \" !!Total:!!Population of one race:!!Native Hawaiian and Oth… RACE   \n# ℹ 291 more rows\n\n\nBy looking at this list, you can see that the variable P1_001N returns the total population.\nUsing Multiple Census Variables\nNow that you know which variables you need, you can use the get_decennial() function again with two variables at once:\n\nget_decennial(\n  geography = \"state\",\n  variables = c(\"P1_001N\", \"P1_006N\"),\n  year = 2020\n) %&gt;%\n  arrange(NAME)\n\nAdding arrange(NAME) after get_decennial() sorts the results by state name, allowing you to easily see that the output includes both variables for each state:\n\n\n# A tibble: 104 × 4\n   GEOID NAME       variable    value\n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n 1 01    Alabama    P1_001N   5024279\n 2 01    Alabama    P1_006N     76660\n 3 02    Alaska     P1_001N    733391\n 4 02    Alaska     P1_006N     44032\n 5 04    Arizona    P1_001N   7151502\n 6 04    Arizona    P1_006N    257430\n 7 05    Arkansas   P1_001N   3011524\n 8 05    Arkansas   P1_006N     51839\n 9 06    California P1_001N  39538223\n10 06    California P1_006N   6085947\n# ℹ 94 more rows\n\n\nWhen you’re working with multiple census variables like this, you might have trouble remembering what names like P1_001N and P1_006N mean. Fortunately, you can adjust the code in the call to get_decennial() to give these variables more meaningful names using the following syntax:\n\nget_decennial(\n  geography = \"state\",\n  variables = c(\n    total_population = \"P1_001N\",\n    asian_population = \"P1_006N\"\n  ),\n  year = 2020\n) %&gt;%\n  arrange(NAME)\n\nWithin the variables argument, this code specifies the new names for the variables, followed by the equal sign and the original variable names. The c() function allows you to rename multiple variables at one time.\nNow it’s much easier to see which variables you’re working with:\n\n\n# A tibble: 104 × 4\n   GEOID NAME       variable            value\n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;               &lt;dbl&gt;\n 1 01    Alabama    total_population  5024279\n 2 01    Alabama    asian_population    76660\n 3 02    Alaska     total_population   733391\n 4 02    Alaska     asian_population    44032\n 5 04    Arizona    total_population  7151502\n 6 04    Arizona    asian_population   257430\n 7 05    Arkansas   total_population  3011524\n 8 05    Arkansas   asian_population    51839\n 9 06    California total_population 39538223\n10 06    California asian_population  6085947\n# ℹ 94 more rows\n\n\nInstead of P1_001N and P1_006N, the variables appear as total_population and asian_population. Much better!\nAnalyzing Census Data\nNow you have the data you need to calculate the Asian population in each state as a percentage of the total. There are just a few functions to add to the code from the previous section:\n\nget_decennial(\n  geography = \"state\",\n  variables = c(\n    total_population = \"P1_001N\",\n    asian_population = \"P1_006N\"\n  ),\n  year = 2020\n) %&gt;%\n  arrange(NAME) %&gt;%\n  group_by(NAME) %&gt;%\n  mutate(pct = value / sum(value)) %&gt;%\n  ungroup() %&gt;%\n  filter(variable == \"asian_population\")\n\nThe group_by(NAME) function creates one group for each state because you want to calculate the Asian population percentage in each state (not for the entire United States). Then mutate() calculates each percentage, taking the value in each row and dividing it by the total_population and asian_population rows for each state. The ungroup() function removes the state-level grouping, and filter() shows only the Asian population percentage.\nWhen you run this code, you should see both the total Asian population and the Asian population as a percentage of the total population in each state:\n\n\n# A tibble: 52 × 5\n   GEOID NAME                 variable           value    pct\n   &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n 1 01    Alabama              asian_population   76660 0.0150\n 2 02    Alaska               asian_population   44032 0.0566\n 3 04    Arizona              asian_population  257430 0.0347\n 4 05    Arkansas             asian_population   51839 0.0169\n 5 06    California           asian_population 6085947 0.133 \n 6 08    Colorado             asian_population  199827 0.0335\n 7 09    Connecticut          asian_population  172455 0.0456\n 8 10    Delaware             asian_population   42699 0.0413\n 9 11    District of Columbia asian_population   33545 0.0464\n10 12    Florida              asian_population  643682 0.0290\n# ℹ 42 more rows\n\n\nThis is a reasonable way to calculate the Asian population as a percentage of the total population in each state — but it’s not the only way.\nUsing a Summary Variable\nKyle Walker knew that calculating summaries like you’ve just done would be a common use case for tidycensus. To calculate, say, the Asian population as a percentage of the whole, you need to have a numerator (the Asian population) and denominator (the total population). So, to simplify things, Walker included the summary_var argument, which can be used within get_decennial() to import the total population as a separate variable. Instead of putting P1_001N (total population) in the variables argument and renaming it, you can assign it to the summary_var argument as follows:\n\nget_decennial(\n  geography = \"state\",\n  variables = c(asian_population = \"P1_006N\"),\n  summary_var = \"P1_001N\",\n  year = 2020\n) %&gt;%\n  arrange(NAME)\n\nThis returns a nearly identical data frame to what you just got, except that the total population is now a separate variable, rather than additional rows for each state:\n\n\n# A tibble: 52 × 5\n   GEOID NAME                 variable           value summary_value\n   &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 01    Alabama              asian_population   76660       5024279\n 2 02    Alaska               asian_population   44032        733391\n 3 04    Arizona              asian_population  257430       7151502\n 4 05    Arkansas             asian_population   51839       3011524\n 5 06    California           asian_population 6085947      39538223\n 6 08    Colorado             asian_population  199827       5773714\n 7 09    Connecticut          asian_population  172455       3605944\n 8 10    Delaware             asian_population   42699        989948\n 9 11    District of Columbia asian_population   33545        689545\n10 12    Florida              asian_population  643682      21538187\n# ℹ 42 more rows\n\n\nWith the data in this new format, now you can calculate the Asian population as a percentage of the whole by dividing the value variable by the summary_value variable. Then you drop the summary_value variable because you no longer need it after doing this calculation:\n\nget_decennial(\n  geography = \"state\",\n  variables = c(asian_population = \"P1_006N\"),\n  summary_var = \"P1_001N\",\n  year = 2020\n) %&gt;%\n  arrange(NAME) %&gt;%\n  mutate(pct = value / summary_value) %&gt;%\n  select(-summary_value)\n\nThe resulting output is identical to the output of the previous section:\n\n\n# A tibble: 52 × 5\n   GEOID NAME                 variable           value    pct\n   &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n 1 01    Alabama              asian_population   76660 0.0153\n 2 02    Alaska               asian_population   44032 0.0600\n 3 04    Arizona              asian_population  257430 0.0360\n 4 05    Arkansas             asian_population   51839 0.0172\n 5 06    California           asian_population 6085947 0.154 \n 6 08    Colorado             asian_population  199827 0.0346\n 7 09    Connecticut          asian_population  172455 0.0478\n 8 10    Delaware             asian_population   42699 0.0431\n 9 11    District of Columbia asian_population   33545 0.0486\n10 12    Florida              asian_population  643682 0.0299\n# ℹ 42 more rows\n\n\nHow you choose to calculate summary statistics is up to you; tidycensus makes it easy to do either way.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatically Accesing Online Data</span>"
    ]
  },
  {
    "objectID": "accessing-data.html#visualizing-american-community-survey-data",
    "href": "accessing-data.html#visualizing-american-community-survey-data",
    "title": "11  Automatically Accesing Online Data",
    "section": "Visualizing American Community Survey Data",
    "text": "Visualizing American Community Survey Data\nOnce you’ve accessed data using the tidycensus package, you can do whatever you want with it. In this section, you’ll practice analyzing and visualizing survey data using the American Community Survey. This survey, which is conducted every year, differs from the Decennial Census in two major ways: it is given to a sample of people rather than the entire population, and it includes a wider range of questions.\nDespite these differences, you can access data from the American Community Survey nearly identically to how you access Decennial Census data. Instead of get_decennial(), you use the get_acs() function, but the arguments you pass to it are the same:\n\nget_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2020\n)\n\nThis code uses the B01002_001 variable to get median age data from 2020 for each state. Here’s what the output looks like:\n\n\n# A tibble: 52 × 5\n   GEOID NAME                 variable   estimate   moe\n   &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n 1 01    Alabama              B01002_001     39.2   0.1\n 2 02    Alaska               B01002_001     34.6   0.2\n 3 04    Arizona              B01002_001     37.9   0.2\n 4 05    Arkansas             B01002_001     38.3   0.2\n 5 06    California           B01002_001     36.7   0.1\n 6 08    Colorado             B01002_001     36.9   0.1\n 7 09    Connecticut          B01002_001     41.1   0.2\n 8 10    Delaware             B01002_001     41     0.2\n 9 11    District of Columbia B01002_001     34.1   0.1\n10 12    Florida              B01002_001     42.2   0.2\n# ℹ 42 more rows\n\n\nYou should notice two differences in the output from get_acs() compared to that from get_decennial(). First, instead of the value column, get_acs() produces a column called estimate. Second, it adds a column called moe, for the margin of error. These changes are the result of American Community Survey being given only to a sample of the population, since extrapolating values from that sample to produce an estimate for the population as a whole introduces a margin of error.\nIn the state-level data, the margins of error are relatively low, but in smaller geographies, they tend to be higher. Cases in which your margins of error are high relative to your estimates indicate a greater level of uncertainty about how well the data represents the population as a whole, so you should interpret such results with caution.\nMaking Charts\nTo pipe your data on median age into ggplot to create a bar chart, add the following lines:\n\nget_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2020\n) %&gt;%\n  ggplot(aes(\n    x = estimate,\n    y = NAME\n  )) +\n  geom_col()\n\nAfter importing the data with the get_acs() function, the ggplot() function pipes it directly into ggplot. States (which use the variable NAME) will go on the y-axis, and median age (estimate) will go on the x-axis. A simple geom_col() creates the bar chart shown in Figure 11.3.\n\n\n\n\n\n\n\nFigure 11.3: A bar chart generated using data acquired with the get_asc() function\n\n\n\n\nThis chart is nothing special, but the fact that it takes just six lines of code to create most definitely is!\nMaking Population Maps with the geometry Argument\nIn addition to co-creating tidycensus, Kyle Walker created the tigris package for working with geospatial data. As a result, these packages are tightly integrated. Within the get_acs() function, you can set the geometry argument to TRUE to receive both demographic data from the Census Bureau and geospatial data from tigris:\n\nget_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2020,\n  geometry = TRUE\n)\n\nIn the resulting data, you can see that it has the metadata and geometry column of the simple features objects that you saw in Chapter 4:\n\n\nSimple feature collection with 52 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1467 ymin: 17.88328 xmax: 179.7785 ymax: 71.38782\nGeodetic CRS:  NAD83\nFirst 10 features:\n   GEOID         NAME   variable estimate moe                       geometry\n1     35   New Mexico B01002_001     38.1 0.1 MULTIPOLYGON (((-109.0502 3...\n2     46 South Dakota B01002_001     37.2 0.2 MULTIPOLYGON (((-104.0579 4...\n3     06   California B01002_001     36.7 0.1 MULTIPOLYGON (((-118.6044 3...\n4     21     Kentucky B01002_001     39.0 0.1 MULTIPOLYGON (((-89.41728 3...\n5     01      Alabama B01002_001     39.2 0.1 MULTIPOLYGON (((-88.05338 3...\n6     13      Georgia B01002_001     36.9 0.1 MULTIPOLYGON (((-81.27939 3...\n7     05     Arkansas B01002_001     38.3 0.2 MULTIPOLYGON (((-94.61792 3...\n8     42 Pennsylvania B01002_001     40.9 0.2 MULTIPOLYGON (((-80.51989 4...\n9     29     Missouri B01002_001     38.7 0.2 MULTIPOLYGON (((-95.77355 4...\n10    08     Colorado B01002_001     36.9 0.1 MULTIPOLYGON (((-109.0603 3...\n\n\nThe geometry type is MULTIPOLYGON, which you learned about in Chapter 4. To pipe this data into ggplot to make a map, add the following code:\n\nget_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2020,\n  geometry = TRUE\n) %&gt;%\n  ggplot(aes(fill = estimate)) +\n  geom_sf() +\n  scale_fill_viridis_c()\n\nAfter importing the data with get_acs() and piping it into the ggplot() function, this code sets the estimate variable to use for the fill aesthetic property; that is, the fill color of each state will vary depending on the median age of its residents. Then geom_sf() draws the map, and the scale_fill_viridis_c() function gives it a colorblind-friendly palette.\nThe resulting map, shown in Figure 11.4, is less than ideal because the Aleutian Islands in Alaska cross the 180-degree line of longitude, or the International Date Line. As a result, most of Alaska appears on one side of the map and a small part appears on the other side. What’s more, both Hawaii and Puerto Rico are hard to see.\n\n\n\n\n\n\n\nFigure 11.4: A hard-to-read map showing median age by state\n\n\n\n\nTo fix these problems, load the tigris package, then use the shift_geometry() To fix these problems, load the tigris package, then use the shift _geometry() function to move Alaska, Hawaii, and Puerto Rico into places where they’ll be more easily visible:\n\nlibrary(tigris)\n\nget_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2020,\n  geometry = TRUE\n) %&gt;%\n  shift_geometry(preserve_area = FALSE) %&gt;%\n  ggplot(aes(fill = estimate)) +\n  geom_sf() +\n  scale_fill_viridis_c()\n\nSetting the preserve_area argument to FALSE shrinks the giant state of Alaska and makes Hawaii and Puerto Rico larger. Although the state sizes in the map won’t be precise, the map will be easier to read, as you can see in Figure 11.5.\n\n\n\n\n\n\n\nFigure 11.5: An easier-to-read map tweaked using tigris functions\n\n\n\n\nNow try making the same map for all 3,000 counties by changing the geography argument to “county”. Other geographies include region, tract (for census tracts), place (for census-designated places, more commonly known as towns and cities), and congressional district. There are also many more arguments in both the get_decennial() and get_acs() functions; I’ve shown you only a few of the most common. If you want to learn more, Walker’s book Analyzing US Census Data: Methods, Maps, and Models in R is a great resource.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatically Accesing Online Data</span>"
    ]
  },
  {
    "objectID": "accessing-data.html#summary",
    "href": "accessing-data.html#summary",
    "title": "11  Automatically Accesing Online Data",
    "section": "Summary",
    "text": "Summary\nThis chapter explored two packages that use APIs to access data directly from its source. The googlesheets4 package lets you import data from a Google Sheet. It’s particularly useful when you’re working with survey data, as it makes it easy to update your reports when new results come in. If you don’t work with Google Sheets, you could use similar packages to fetch data from Excel365 (Microsoft365R), Qualtrics (qualtRics), Survey Monkey (svmkrR), and other sources.\nIf you work with US Census Bureau data, the tidycensus package is a huge time-saver. Rather than having to manually download data from the Census Bureau website, you can use tidycensus to write R code that accesses the data automatically, making it ready for analysis and reporting. Because of the package’s integration with tigris, you can also easily map this demographic data.\nIf you’re looking for census data from other countries, there are also R packages to bring data from Canada (cancensus), Kenya (rKenyaCensus), Mexico (mxmaps and inegiR), Europe (eurostat), and other regions. Before hitting that download button in your data collection tool to get a CSV file, it’s worth looking for a package that can import that data directly into R.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatically Accesing Online Data</span>"
    ]
  },
  {
    "objectID": "accessing-data.html#additional-resources",
    "href": "accessing-data.html#additional-resources",
    "title": "11  Automatically Accesing Online Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nIsabella Velásquez and Curtis Kephart, “Automated Survey Reporting with googlesheets4, pins, and R Markdown,” Posit, June 15, 2022, https://posit.co/blog/automated-survey-reporting/.\nKyle Walker, Analyzing US Census Data: Methods, Maps, and Models in R (Boca Raton, FL: CRC Press, 2023), https://walker-data.com/census-r/.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Automatically Accesing Online Data</span>"
    ]
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "12  Creating Functions and Packages",
    "section": "",
    "text": "Creating Your Own Functions\nHadley Wickham, developer of the tidyverse set of packages, recommends creating a function once you’ve copied some code three times. Functions have three pieces: a name, a body, and arguments.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating Functions and Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#creating-your-own-functions",
    "href": "packages.html#creating-your-own-functions",
    "title": "12  Creating Functions and Packages",
    "section": "",
    "text": "Writing a Simple Function\nYou’ll begin by writing an example of a relatively simple function. This function, called show_in_excel_penguins(), opens the penguin data from Chapter 7 in Microsoft Excel:\n\nlibrary(tidyverse)\nlibrary(fs)\n\npenguins &lt;- read_csv(\"https://data.rfortherestofus.com/penguins-2007.csv\")\n\nshow_in_excel_penguins &lt;- function() {\n  csv_file &lt;- str_glue(\"{tempfile()}.csv\")\n\n  write_csv(\n    x = penguins,\n    file = csv_file,\n    na = \"\"\n  )\n\n  file_show(path = csv_file)\n}\n\nThis code first loads the tidyverse and fs packages. You’ll use tidyverse to create a filename for the CSV file and save it, and fs to open the CSV file in Excel (or whichever program your computer uses to open CSV files by default).\nNext, the read_csv() function imports the penguin data and names the data frame penguins. Then it creates the new show_in_excel_penguins function, using the assignment operator (&lt;-) and function() to specify that show_in_excel_penguins isn’t a variable name but a function name. The open curly bracket ({) at the end of the line indicates the start of the function body, where the “meat” of the function can be found. In this case, the body does three things:\n\nCreates a location for a CSV file to be saved using the str_glue() function combined with the tempfile() function. This creates a file at a temporary location with the .csv extension and saves it as csv_file.\nWrites penguins to the location set in csv_file. The x argument in write_csv() refers to the data frame to be saved. It also specifies that all NA values should show up as blanks. (By default, they would display the text NA.)\nUses the file_show() function from the fs package to open the temporary CSV file in Excel.\n\nTo use the show_in_excel_penguins() function, highlight the lines that define the function and then press command-enter on macOS or ctrl-enter on Windows. You should now see the function in your global environment, as shown in Figure 12.1.\n\n\n\n\n\n\n\nFigure 12.1: The new function in the global environment\n\n\n\n\nFrom now on, any time you run the code show_in_excel_penguins(), R will open the penguins data frame in Excel.\nAdding Arguments\nYou’re probably thinking that this function doesn’t seem very useful. All it does is open the penguins data frame. Why would you want to keep doing that? A more practical function would let you open any data in Excel so you can use it in a variety of contexts.\nThe show_in_excel() function does just that: it takes any data frame from R, saves it as a CSV file, and opens the CSV file in Excel. Bruno Rodrigues, head of the Department of Statistics and Data Strategy at the Ministry of Higher Education and Research in Luxembourg, wrote show_in_excel() to easily share data with his non-R-user colleagues. Whenever he needed data in a CSV file, he could run this function.\nReplace your show_in_excel_penguins() function definition with this slightly simplified version of the code that Rodrigues used:\n\nshow_in_excel &lt;- function(data) {\n  csv_file &lt;- str_glue(\"{tempfile()}.csv\")\n  write_csv(\n    x = data,\n    file = csv_file,\n    na = \"\"\n  )\n  file_show(path = csv_file)\n}\n\nThis code looks the same as show_in_excel_penguins(), with two exceptions. Notice that the first line now says function(data). Items listed within the parentheses of the function definition are arguments. If you look farther down, you’ll see the second change. Within write_csv(), instead of x = penguins, it now says x = data. This allows you to use the function with any data, not just penguins.\nTo use this function, you simply tell show_in_excel() what data to use, and the function opens the data in Excel. For example, tell it to open the penguins data frame as follows:\n\nshow_in_excel(data = penguins)\n\nHaving created the function with the data argument, now you can run it with any data you want to. This code, for example, imports the COVID case data from Chapter 4 and opens it in Excel:\n\ncovid_data &lt;- read_csv(\"https://data.rfortherestofus.com/us-states-covid-rolling-average.csv\")\n\nshow_in_excel(data = covid_data)\n\nYou can also use show_in_excel() at the end of a pipeline. This code filters the covid_data data frame to include only data from California before opening it in Excel:\n\ncovid_data %&gt;%\n  filter(state == \"California\") %&gt;%\n  show_in_excel()\n\nRodrigues could have copied the code within the show_in_excel() function and rerun it every time he wanted to view his data in Excel. But, by creating a function, he was able to write the code just once and then run it as many times as necessary.\nCreating a Function to Automatically Format Race and Ethnicity Data\nHopefully now you better understand how functions work, so let’s walk through an example function you could use to simplify some of the activities from previous chapters.\nIn Chapter 11, when you used the tidycensus package to automatically import data from the US Census Bureau, you learned that the census data has many variables with nonintuitive names. Say you regularly want to access data about race and ethnicity from the American Community Survey, but you can never remember which variables enable you to do so. To make your task more efficient, you’ll create a get_acs_race_ethnicity() function step-by-step in this section, learning some important concepts about custom functions along the way.\nA first version of the get_acs_race_ethnicity() function might look like this:\n\nlibrary(tidycensus)\n\nget_acs_race_ethnicity &lt;- function() {\n  race_ethnicity_data &lt;-\n    get_acs(\n      geography = \"state\",\n      variables = c(\n        \"White\" = \"B03002_003\",\n        \"Black/African American\" = \"B03002_004\",\n        \"American Indian/Alaska Native\" = \"B03002_005\",\n        \"Asian\" = \"B03002_006\",\n        \"Native Hawaiian/Pacific Islander\" = \"B03002_007\",\n        \"Other race\" = \"B03002_008\",\n        \"Multi-Race\" = \"B03002_009\",\n        \"Hispanic/Latino\" = \"B03002_012\"\n      )\n    )\n\n  race_ethnicity_data\n}\n\nWithin the function body, this code calls the get_acs() function from tidycensus to retrieve population data at the state level. But instead of returning the function’s default output, it updates the hard-to-remember variable names to human-readable names, such as White and Black/African American, and saves them as an object called race_ethnicity_data. The code then uses the race_ethnicity_data object to return that data when the get_acs_race_ethnicity() function is run.\nTo run this function, enter the following:\n\nget_acs_race_ethnicity()\n\nDoing so should return data with easy-to-read race and ethnicity group names:\n\n\n# A tibble: 416 × 5\n   GEOID NAME    variable                         estimate   moe\n   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;                               &lt;dbl&gt; &lt;dbl&gt;\n 1 01    Alabama White                             3247262  2133\n 2 01    Alabama Black/African American            1318388  3585\n 3 01    Alabama American Indian/Alaska Native       14864   868\n 4 01    Alabama Asian                               69099  1113\n 5 01    Alabama Native Hawaiian/Pacific Islander     1557   303\n 6 01    Alabama Other race                          14724  1851\n 7 01    Alabama Multi-Race                         129791  3724\n 8 01    Alabama Hispanic/Latino                    232407   199\n 9 02    Alaska  White                              428802  1173\n10 02    Alaska  Black/African American              22400   935\n# ℹ 406 more rows\n\n\nYou could improve this function in a few ways. You might want the resulting variable names to follow a consistent syntax, for example, so you could use the clean_names() function from the janitor package to format them in snake case (in which all words are lowercase and separated by underscores). However, you might also want to have the option of keeping the original variable names. To accomplish this, add the clean_variable_names argument to the function definition as follows:\n\nget_acs_race_ethnicity &lt;- function(clean_variable_names = FALSE) {\n  race_ethnicity_data &lt;-\n    get_acs(\n      geography = \"state\",\n      variables = c(\n        \"White\" = \"B03002_003\",\n        \"Black/African American\" = \"B03002_004\",\n        \"American Indian/Alaska Native\" = \"B03002_005\",\n        \"Asian\" = \"B03002_006\",\n        \"Native Hawaiian/Pacific Islander\" = \"B03002_007\",\n        \"Other race\" = \"B03002_008\",\n        \"Multi-Race\" = \"B03002_009\",\n        \"Hispanic/Latino\" = \"B03002_012\"\n      )\n    )\n\n  if (clean_variable_names == TRUE) {\n    race_ethnicity_data &lt;- clean_names(race_ethnicity_data)\n  }\n\n  race_ethnicity_data\n}\n\nThis code adds the clean_variable_names argument to get_acs_race _ethnicity() and specifies that its value should be FALSE by default. Then, in the function body, an if statement says that if the argument is TRUE, the variable names should be overwritten by versions formatted in snake case. If the argument is FALSE, the variable names remain unchanged.\nIf you run the function now, nothing should change, because the new argument is set to FALSE by default. Try setting clean_variable_names to TRUE as follows:\n\nget_acs_race_ethnicity(clean_variable_names = TRUE)\n\nThis function call should return data with consistent variable names:\n\n\n# A tibble: 416 × 5\n   geoid name    variable                         estimate   moe\n   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;                               &lt;dbl&gt; &lt;dbl&gt;\n 1 01    Alabama White                             3247262  2133\n 2 01    Alabama Black/African American            1318388  3585\n 3 01    Alabama American Indian/Alaska Native       14864   868\n 4 01    Alabama Asian                               69099  1113\n 5 01    Alabama Native Hawaiian/Pacific Islander     1557   303\n 6 01    Alabama Other race                          14724  1851\n 7 01    Alabama Multi-Race                         129791  3724\n 8 01    Alabama Hispanic/Latino                    232407   199\n 9 02    Alaska  White                              428802  1173\n10 02    Alaska  Black/African American              22400   935\n# ℹ 406 more rows\n\n\nNotice that GEOID and NAME now appear as geoid and name.\nNow that you’ve seen how to add arguments to two separate functions, you’ll learn how to pass arguments from one function to another.\nUsing … to Pass Arguments to Another Function\nThe get_acs_race_ethnicity() function you’ve created retrieves population data at the state level by passing the geography = \"state\" argument to the get_acs() function. But what if you wanted to obtain county-level or census tract data? You could do so using get_acs(), but get_acs_race_ethnicity() isn’t currently written in a way that would allow this. How could you modify the function to make it more flexible?\nYour first idea might be to add a new argument for the level of data to retrieve. You could edit the first two lines of the function as follows to add a my_geography argument and then use it in the get_acs() function like so:\n\nget_acs_race_ethnicity &lt;- function(\n  clean_variable_names = FALSE,\n  my_geography\n) {\n  race_ethnicity_data &lt;-\n    get_acs(\n      geography = my_geography,\n      variables = c(\n        \"White\" = \"B03002_003\",\n        \"Black/African American\" = \"B03002_004\",\n        \"American Indian/Alaska Native\" = \"B03002_005\",\n        \"Asian\" = \"B03002_006\",\n        \"Native Hawaiian/Pacific Islander\" = \"B03002_007\",\n        \"Other race\" = \"B03002_008\",\n        \"Multi-Race\" = \"B03002_009\",\n        \"Hispanic/Latino\" = \"B03002_012\"\n      )\n    )\n\n  if (clean_variable_names == TRUE) {\n    race_ethnicity_data &lt;- clean_names(race_ethnicity_data)\n  }\n\n  race_ethnicity_data\n}\n\nBut what if you also want to select the year for which to retrieve data? Well, you could add an argument for that as well. However, as you saw in Chapter 11, the get_acs() function has many arguments, and repeating them all in your code would quickly become cumbersome.\nThe ... syntax gives you a more efficient option. Placing ... in the get_acs_race_ethnicity() function allows you to automatically pass any of its arguments to get_acs() by including ... in that function as well:\n\nget_acs_race_ethnicity &lt;- function(\n  clean_variable_names = FALSE,\n  ...\n) {\n  race_ethnicity_data &lt;-\n    get_acs(\n      ...,\n      variables = c(\n        \"White\" = \"B03002_003\",\n        \"Black/African American\" = \"B03002_004\",\n        \"American Indian/Alaska Native\" = \"B03002_005\",\n        \"Asian\" = \"B03002_006\",\n        \"Native Hawaiian/Pacific Islander\" = \"B03002_007\",\n        \"Other race\" = \"B03002_008\",\n        \"Multi-Race\" = \"B03002_009\",\n        \"Hispanic/Latino\" = \"B03002_012\"\n      )\n    )\n\n  if (clean_variable_names == TRUE) {\n    race_ethnicity_data &lt;- clean_names(race_ethnicity_data)\n  }\n\n  race_ethnicity_data\n}\n\nTry running your function by passing it the geography argument set to “state”:\n\nget_acs_race_ethnicity(geography = \"state\")\n\nThis should return the following:\n\n\n# A tibble: 416 × 5\n   GEOID NAME    variable                         estimate   moe\n   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;                               &lt;dbl&gt; &lt;dbl&gt;\n 1 01    Alabama White                             3247262  2133\n 2 01    Alabama Black/African American            1318388  3585\n 3 01    Alabama American Indian/Alaska Native       14864   868\n 4 01    Alabama Asian                               69099  1113\n 5 01    Alabama Native Hawaiian/Pacific Islander     1557   303\n 6 01    Alabama Other race                          14724  1851\n 7 01    Alabama Multi-Race                         129791  3724\n 8 01    Alabama Hispanic/Latino                    232407   199\n 9 02    Alaska  White                              428802  1173\n10 02    Alaska  Black/African American              22400   935\n# ℹ 406 more rows\n\n\nYou’ll see that the GEOID and NAME variables are uppercase because the clean_variable_names argument is set to FALSE by default, and we didn’t change it when using the get_acs_race_ethnicity() function.\nAlternatively, you could change the value of the argument to get data by county:\n\nget_acs_race_ethnicity(geography = \"county\")\n\nYou could also run the function with the geometry = TRUE argument to return geospatial data alongside demographic data:\n\nget_acs_race_ethnicity(\n  geography = \"county\",\n  geometry = TRUE\n)\n\nThe function should return data like the following:\n\n\nSimple feature collection with 416 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1467 ymin: 17.88328 xmax: 179.7785 ymax: 71.38782\nGeodetic CRS:  NAD83\nFirst 10 features:\n   GEOID         NAME                         variable estimate  moe\n1     35   New Mexico                            White   752424 1849\n2     35   New Mexico           Black/African American    37996 1116\n3     35   New Mexico    American Indian/Alaska Native   178608 1339\n4     35   New Mexico                            Asian    32214  868\n5     35   New Mexico Native Hawaiian/Pacific Islander     1117  190\n6     35   New Mexico                       Other race     7680  967\n7     35   New Mexico                       Multi-Race    50798 2565\n8     35   New Mexico                  Hispanic/Latino  1051626   NA\n9     46 South Dakota                            White   718056  978\n10    46 South Dakota           Black/African American    19172  805\n                         geometry\n1  MULTIPOLYGON (((-109.0502 3...\n2  MULTIPOLYGON (((-109.0502 3...\n3  MULTIPOLYGON (((-109.0502 3...\n4  MULTIPOLYGON (((-109.0502 3...\n5  MULTIPOLYGON (((-109.0502 3...\n6  MULTIPOLYGON (((-109.0502 3...\n7  MULTIPOLYGON (((-109.0502 3...\n8  MULTIPOLYGON (((-109.0502 3...\n9  MULTIPOLYGON (((-104.0579 4...\n10 MULTIPOLYGON (((-104.0579 4...\n\n\nThe ... syntax allows you to create your own function and pass arguments from it to another function without repeating all of that function’s arguments in your own code. This approach gives you flexibility while keeping your code concise.\nNow let’s look at how to put your custom functions into a package.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating Functions and Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#creating-a-package",
    "href": "packages.html#creating-a-package",
    "title": "12  Creating Functions and Packages",
    "section": "Creating a Package",
    "text": "Creating a Package\nPackages bundle your functions so you can use them in multiple projects. If you find yourself copying functions from one project to another, or from a functions.R file into each new project, that’s a good indication that you should make a package.\nWhile you can run the functions from a functions.R file in your own environment, this code might not work on someone else’s computer. Other users may not have the necessary packages installed, or they may be confused about how your functions’ arguments work and not know where to go for help. Putting your functions in a package makes them more likely to work for everyone, as they include the necessary dependencies as well as built-in documentation to help others use the functions on their own.\nStarting the Package\nTo create a package in RStudio, go to File &gt; New Project &gt; New Directory. Select R Package from the list of options and give your package a name. In Figure 12.2, I’ve called mine dk. Also decide where you want your package to live on your computer. You can leave everything else as is.\n\n\n\n\n\n\n\nFigure 12.2: The RStudio menu for creating your own package\n\n\n\n\nRStudio will now create and open the package. It should already contain a few files, including hello.R, which has a prebuilt function called hello() that, when run, prints the text Hello, world! in the console. You’ll get rid of this and a few other default files so you can start with a clean slate. Delete hello.R, NAMESPACE, and hello.Rd in the man directory.\nAdding Functions with use_r()\nAll of the functions in a package should go in separate files in the R folder. To add these files to the package automatically and test that they work correctly, you’ll use the usethis and devtools packages. Install them using install.packages() like so:\n\ninstall.packages(\"usethis\")\ninstall.packages(\"devtools\")\n\nTo add a function to the package, run the use_r() function from the usethis package in the console:\n\nusethis::use_r(\"acs\")\n\nThe package::function() syntax allows you to use a function without loading the associated package. The use_r() function should create a file in the R directory with the argument name you provide — in this case, the file is called acs.R. The name itself doesn’t really matter, but it’s a good practice to choose something that gives an indication of the functions the file contains. Now you can open the file and add code to it. Copy the get_acs_race _ethnicity() function to the package.\nChecking our Package with devtools\nYou need to change the get_acs_race_ethnicity() function in a few ways to make it work in a package. The easiest way to figure out what changes you need to make is to use built-in tools to check that your package is built correctly. Run the function devtools::check() in the console to perform what is known as an R CMD check, a command that runs under the hood to ensure others can install your package on their system. Running R CMD check on the dk package outputs this long message:\n── R CMD check results ─────────────── dk 0.1.0 ────\nDuration: 4s\n\n❯ checking DESCRIPTION meta-information ... WARNING\nNon-standard license specification:\nWhat license is it under?\nStandardizable: FALSE\n\n❯ checking for missing documentation entries ... WARNING\nUndocumented code objects:\n‘get_acs_race_ethnicity’\nAll user-level objects in a package should have documentation entries.\nSee chapter ‘Writing R documentation files’ in the ‘Writing R\nExtensions’ manual.\n\n❯ checking R code for possible problems ... NOTE\nget_acs_race_ethnicity: no visible global function definition for\n‘get_acs’\nget_acs_race_ethnicity: no visible global function definition for\n‘clean_names’\nUndefined global functions or variables:\nclean_names get_acs\n\n0 errors ✔ | 2 warnings ✖ | 1 note ✖\nThe last part is the most important, so let’s review the output from bottom to top. The line 0 errors ✔ | 2 warnings ✖ | 1 note ✖ highlights three levels of issues identified in the package. Errors are the most severe, as they mean others won’t be able to install your package, while warnings and notes may cause problems for others. It’s best practice to eliminate all errors, warnings, and notes.\nWe’ll start by addressing the note. To help you understand what R CMD check is saying here, I need to explain a bit about how packages work. When you install a package using the install.packages() function, it often takes a while. That’s because the package you’re telling R to install likely uses functions from other packages. To access these functions, R must install these packages (known as dependencies) for you; after all, it would be a pain if you had to manually install a whole set of dependencies every time you installed a new package. But to make sure that the appropriate packages are installed for any user of the dk package, you still have to make a few changes.\nR CMD check is saying this package includes several “undefined global functions or variables” and “no visible global function definition” for various functions. This is because you’re trying to use functions from the tidycensus and janitor packages, but you haven’t specified where these functions come from. I can run this code in my environment because I have tidycensus and janitor installed, but you can’t assume the same of everyone.\nAdding Dependency Packages\nTo ensure the package’s code will work, you need to install tidycensus and janitor for users when they install the dk package. To do this, run the use_package() function from the usethis package in the console, first specifying \"tidycensus\" for the package argument:\n\nusethis::use_package(package = \"tidycensus\")\n\nYou should get the following message:\n✔ Setting active project to '/Users/davidkeyes/Documents/Work/R Without Statistics/dk'\n✔ Adding 'tidycensus' to Imports field in DESCRIPTION\n• Refer to functions with `tidycensus::fun()`\nThe Setting active project... line indicates that you’re working in the dk project. The second line indicates that the DESCRIPTION file has been edited. This file provides metadata about the package you’re developing.\nNext, add the janitor package the same way you added tidyverse\n\nusethis::use_package(package = \"janitor\")\n\nwhich should give you the following output:\n✔ Adding 'janitor' to Imports field in DESCRIPTION\n• Refer to functions with `janitor::fun()`\nIf you open the DESCRIPTION file in the root directory of your project, you should see the following:\nPackage: dk\nType: Package\nTitle: What the Package Does (Title Case)\nVersion: 0.1.0\nAuthor: Who wrote it\nMaintainer: The package maintainer &lt;yourself@somewhere.net&gt;\nDescription: More about what it does (maybe more than one line)\n    Use four spaces when indenting paragraphs within the Description.\nLicense: What license is it under?\nEncoding: UTF-8\nLazyData: true\nImports: \n    janitor,\n    tidycensus\nThe Imports section at the bottom of the file indicates that when a user installs the dk package, the tidycensus and janitor packages will also be imported.\nReferring to Functions Correctly\nThe output from running usethis::use_package(package = \"janitor\") also included the line Refer to functions with tidycensus::fun() (where fun() stands for function name). This tells you that in order to use functions from other packages in the dk package, you need to specify both the package name and the function name to ensure that the correct function is used at all times. On rare occasions, you’ll find functions with identical names used across multiple packages, and this syntax avoids ambiguity. Remember this line from the R CMD check?\nUndefined global functions or variables:\nclean_names get_acs\nIt appeared because you were using functions without saying what package they came from. The clean_names() function comes from the janitor package, and get_acs() comes from tidycensus, so you will need to add these package names before each function:\n\nget_acs_race_ethnicity &lt;- function(\n  clean_variable_names = FALSE,\n  ...\n) {\n  race_ethnicity_data &lt;- tidycensus::get_acs(\n    ...,\n    variables = c(\n      \"White\" = \"B03002_003\",\n      \"Black/African American\" = \"B03002_004\",\n      \"American Indian/Alaska Native\" = \"B03002_005\",\n      \"Asian\" = \"B03002_006\",\n      \"Native Hawaiian/Pacific Islander\" = \"B03002_007\",\n      \"Other race\" = \"B03002_008\",\n      \"Multi-Race\" = \"B03002_009\",\n      \"Hispanic/Latino\" = \"B03002_012\"\n    )\n  )\n\n  if (clean_variable_names == TRUE) {\n    race_ethnicity_data &lt;- janitor::clean_names(race_ethnicity_data)\n  }\n\n  race_ethnicity_data\n}\n\nNow you can run devtools::check() again, and you should see that the notes have gone away:\n❯ checking DESCRIPTION meta-information ... WARNING\nNon-standard license specification:\nWhat license is it under?\nStandardizable: FALSE\n\n❯ checking for missing documentation entries ... WARNING\nUndocumented code objects:\n‘get_acs_race_ethnicity’\nAll user-level objects in a package should have documentation entries.\nSee chapter ‘Writing R documentation files’ in the ‘Writing R\nExtensions’ manual.\n\n0 errors ✔ | 2 warnings ✖ | 0 notes ✔\nHowever, there are still two warnings to deal with. You’ll do that next.\nCreating Documentation with Roxygen\nThe checking for missing documentation entries warning indicates that you need to document your get_acs_race_ethnicity() function. One of the benefits of creating a package is that you can add documentation to help others use your code. In the same way that users can enter ?get_acs() and see documentation about that function, you want them to be able to enter ?get_acs _race_ethnicity() to learn how your function works.\nTo create documentation for get_acs_race_ethnicity(), you’ll use Roxygen, a documentation tool that uses a package called roxygen2. To get started, place your cursor anywhere in your function. Then, in RStudio go to Code &gt; Insert Roxygen Skeleton. This should add the following text before the get_acs_race_ethnicity() function:\n#' Title\n#'\n#' @param clean_variable_names \n#' @param ... \n#'\n#' @return\n#' @export\n#'\n#' @examples\nThis text is the documentation’s skeleton. Each line starts with the special characters #', which indicate that you’re working with Roxygen. Now you can edit the text to create your documentation. Begin by replacing Title with a sentence that describes the function:\n#' Access race and ethnicity data from the American Community Survey\nNext, turn your attention to the lines beginning with @param. Roxygen automatically creates one of these lines for each function argument, but it’s up to you to fill them in with a description. Begin by describing what the clean_variable_names argument does. Next, specify that the ... will pass additional arguments to the tidycensus::get_acs() function:\n#' @param clean_variable_names Should variable names be cleaned (i.e. snake case)\n#' @param ... Other arguments passed to tidycensus::get_acs()\nThe @return line should tell the user what the get_acs_race_ethnicity() function returns. In this case, it returns data, which you document as follows:\n#' @return A tibble with five variables: GEOID, NAME, variable, estimate, and moe\nAfter @return is @export. You don’t need to change anything here. Most functions in a package are known as exported functions, meaning they’re available to users of the package. In contrast, internal functions, which are used only by the package developers, don’t have @export in the Roxygen skeleton.\nThe last section is @examples. This is where you can give examples of code that users can run to learn how the function works. Doing this introduces some complexity and isn’t required, so you can skip it here and delete the line with @examples on it.\n\n\n\n\n\n\nIf you want to learn more about adding examples to your documentation, the second edition of Hadley Wickham and Jenny Bryan’s book R Packages is a great resource.\n\n\n\nNow that you’ve added documentation with Roxygen, run devtools:: document() in the console. This should create a get_acs_race_ethnicity.Rd documentation file in the man directory using the very specific format that R packages require. You’re welcome to look at it, but you can’t change it; it’s read-only.\nRunning the function should also create a NAMESPACE file, which lists the functions that your package makes available to users. It should look like this:\n# Generated by roxygen2: do not edit by hand\n\nexport(get_acs_race_ethnicity)\nYour get_acs_race_ethnicity() function is now almost ready for users.\nAdding a License and Metadata\nRun devtools::check() again to see if you’ve fixed the issues that led to the warnings. The warning about missing documentation should no longer be there. However, you do still get one warning:\n❯ checking DESCRIPTION meta-information ... WARNING\nNon-standard license specification:\nWhat license is it under?\nStandardizable: FALSE\n\n0 errors ✔ | 1 warning ✖ | 0 notes ✔\nThis warning reminds you that you have not given your package a license. If you plan to make your package publicly available, choosing a license is important because it tells other people what they can and cannot do with your code. For information about how to choose the right license for your package, see https://choosealicense.com.\nIn this example, you’ll use the MIT license, which allows users to do essentially whatever they want with your code, by running usethis::use_mit_license(). The usethis package has similar functions for other common licenses. You should get the following output:\n✔ Setting active project to '/Users/davidkeyes/Documents/Work/R Without Statistics/dk'\n✔ Setting License field in DESCRIPTION to 'MIT + file LICENSE'\n✔ Writing 'LICENSE'\n✔ Writing 'LICENSE.md'\n✔ Adding '^LICENSE\\\\.md$' to '.Rbuildignore'\nThe use_mit_license() function handles a lot of the tedious parts of adding a license to your package. Most importantly for our purposes, it specifies the license in the DESCRIPTION file. If you open it, you should see this confirmation that you’ve added the MIT license:\nLicense: MIT + file LICENSE\nIn addition to the license, the DESCRIPTION file contains metadata about the package. You can make a few changes to identify its title and add an author, a maintainer, and a description. The final DESCRIPTION file might look something like this\nPackage: dk\nType: Package\nTitle: David Keyes's Personal Package\nVersion: 0.1.0\nAuthor: David Keyes\nMaintainer: David Keyes &lt;david@rfortherestofus.com&gt;\n    Description: A package with functions that David Keyes may find \n    useful.\nLicense: MIT + file LICENSE\nEncoding: UTF-8\nLazyData: true\nImports: \n    janitor,\n    tidycensus\nHaving made these changes, run devtools::check() one more time to make sure everything is in order:\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\nThis is exactly what you want to see!\nAdding Additional Functions\nYou’ve now got a package with one working function in it. If you wanted to add more functions, you would follow the same procedure:\n\nCreate a new .R file with usethis::use_r() or copy another function to the existing .R file.\nDevelop your function using the package::function() syntax to refer to functions from other packages.\nAdd any dependency packages with use_package().\nAdd documentation for your function.\nRun devtools::check() to make sure you did everything correctly.\n\nYour package can contain a single function, like dk, or as many functions as you want.\nInstalling the Package\nNow you’re ready to install and use the new package. When you’re developing your own package, installing it for your own use is relatively straightforward. Simply run devtools::install(), and the package will be ready for you to use in any project.\nOf course, if you’re developing a package, you’re likely doing it not just for yourself but for others as well. The most common way to make your package available to others is with the code-sharing website GitHub. The details of how to put your code on GitHub are beyond what I can cover here, but the book Happy Git and GitHub for the useR by Jenny Bryan (self-published at https://happygitwithr.com) is a great place to start.\nI’ve pushed the dk package to GitHub, and you can find it at https://github.com/dgkeyes/dk. If you’d like to install it, first make sure you have the remotes package installed, then run the code remotes::install_github(\"dgkeyes/dk\") in the console.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating Functions and Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#summary",
    "href": "packages.html#summary",
    "title": "12  Creating Functions and Packages",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you saw that packages are useful because they let you bundle several elements needed to reliably run your code: a set of functions, instructions to automatically install dependency packages, and code documentation.\nCreating your own R package is especially beneficial when you’re working for an organization, as packages can allow advanced R users to help colleagues with less experience. When Travis Gerke and Garrick Aden-Buie provided researchers at the Moffitt Cancer Center with a package that contained functions for easily accessing their databases, the researchers began to use R more creatively.\nIf you create a package, you can also guide people to use R in the way you think is best. Packages are a way to ensure that others follow best practices (without even being aware they are doing so). They make it easy to reuse functions across projects, help others, and adhere to a consistent style.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating Functions and Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#additional-resources",
    "href": "packages.html#additional-resources",
    "title": "12  Creating Functions and Packages",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nMalcolm Barrett, “Package Development with R,” online course, accessed December 2, 2023, https://rfortherestofus.com/courses/package-development.\nHadley Wickham and Jennifer Bryan, R Packages, 2nd ed. (Sebastopol, CA: O’Reilly Media, 2023), https://r-pkgs.org.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating Functions and Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#wrapping-up",
    "href": "packages.html#wrapping-up",
    "title": "12  Creating Functions and Packages",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nR was invented in 1993 as a tool for statistics, and in the years since, it has been used for plenty of statistical analysis. But over the last three decades, R has also become a tool that can do much more than statistics.\nAs you’ve seen in this book, R is great for making visualizations. You can use it to create high-quality graphics and maps, make your own theme to keep your visuals consistent and on-brand, and generate tables that look good and communicate well. Using R Markdown or Quarto, you can create reports, presentations, and websites. And best of all, these documents are all reproducible, meaning that updating them is as easy as rerunning your code. Finally, you’ve seen that R can help you automate how you access data, as well as assist you in collaborating with others through the functions and packages you create.\nIf R was new to you when you started this book, I hope you now feel inspired to use it. If you’re an experienced R user, I hope this book has shown you some ways to use R that you hadn’t previously considered. No matter your background, my hope is that you now understand how to use R like a pro. Because it isn’t just a tool for statisticians — R is a tool for the rest of us too.",
    "crumbs": [
      "Part III: Automation and Collaboration",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Creating Functions and Packages</span>"
    ]
  }
]